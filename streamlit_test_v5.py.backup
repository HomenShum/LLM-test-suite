# streamlit_app.py
# ============================================================
# Structured classification + evaluation suite (Enhanced Version)
# - Uses your file context, env ingestion, and async style
# - Providers: OpenRouter + OpenAI
# - Tests:
#   1) CSV classify + F1/Latency (two models) + Error Analysis
#   2) Add third model + weighted pick by (per-class F1 * row confidence)
#   3) Mistral as Judge over 3 models + Judge evaluation
#   4) Quantitative context pruning + action decision test
# ============================================================

import os, asyncio, json, time, re
import pandas as pd
import streamlit as st
import httpx, nest_asyncio
from typing import List, Optional, Tuple, Dict, Any, Set
from pydantic import BaseModel, ValidationError, Field
from openai import AsyncOpenAI
from google import genai
from google.genai import types
from pathlib import Path

from dotenv import load_dotenv
from collections import Counter, defaultdict

# --- NEW IMPORTS for orchestrator infrastructure ---
from dataclasses import dataclass, field
from enum import Enum
import hashlib

# --- LEAF AGENT SCAFFOLD IMPORTS ---
# Only import what we directly use in streamlit_test_v5.py
# (Other classes like LeafAgent, SubTask, etc. are used internally by these)
from leaf_agent_scaffold import (
    SupervisorAgent,
    AgentType,
    WebResearchAgent,
    CodeExecutorAgent,
    ContentGeneratorAgent,
    ValidatorAgent,
    TaskPlanner,
    ResultSynthesizer,
    PolicyUpdater  # For self-correcting research pipeline
)

# --- STATEFUL COMPONENTS IMPORTS ---
from utils.dashboard_logger import DashboardLogger
from utils.stateful_components import (
    MemoryManager,
    SecurityAuditAgent,
    SelfCorrectionManager
)

# --- NEW IMPORTS for enhanced testing ---
from sklearn.metrics import classification_report, confusion_matrix

# --- PLOTLY IMPORTS for interactive visualizations ---
import plotly.graph_objects as go
from plotly.subplots import make_subplots

import warnings

# --- REFACTORED IMPORTS: Extracted modules ---
from config.scenarios import (
    PI_AGENT_GOAL_PROMPT,
    FOLDING_POLICY_BLOCK,
    CYBERSECURITY_GOAL_PROMPT,
    THREAT_POLICY_BLOCK,
    SMOKE_TEST_SCENARIOS,
    SUGGESTED_PROMPTS,
    DEFAULT_DATASET_PROMPTS,
    SKELETON_COLUMNS,
    ROW_LIMIT_OPTIONS,
    CANON_MAP as _CANON_MAP,
    TEST_FLOWS,
    JUDGE_SCHEMA,
    JUDGE_INSTRUCTIONS
)

from utils.visualizations import (
    render_test_flow_diagram,
    render_kpi_metrics,
    render_cost_dashboard,
    visualize_dataset_composition,
    render_model_comparison_chart
)

from utils.gantt_charts import (
    render_agent_gantt_chart,
    render_test5_gantt_chart
)

from utils.ui_components import (
    ModelSelector,
    ConfigDisplay,
    TestResultTabs
)

# --- NEW PHASE 2 IMPORTS: Additional extracted modules ---
from core.models import (
    Classification,
    ClassificationWithConf,
    SyntheticDataItem,
    ToolCallSequenceItem,
    PruningDataItem,
    TestSummaryAndRefinement,
    FactualConstraint,
    ValidationResultArtifact,
    convert_validation_to_artifact
)

from core.pricing import (
    fetch_openrouter_pricing,
    _to_openrouter_model_id,
    _to_native_model_id,
    _get_provider_from_model_id,
    custom_openrouter_price_lookup,
    custom_gemini_price_lookup,
    get_all_available_models
)

from utils.model_discovery import (
    fetch_openrouter_models_for_ui,
    fetch_openai_models,
    get_third_model_display_name,
    _normalize_ollama_root,
    OPENROUTER_MODEL,
    OPENAI_MODEL,
    THIRD_MODEL_KIND,
    THIRD_MODEL
)

from utils.data_helpers import (
    ensure_dataset_directory,
    save_dataset_to_file,
    save_results_df,
    load_classification_dataset,
    load_tool_sequence_dataset,
    load_context_pruning_dataset,
    _load_df_from_path,
    auto_generate_default_datasets,
    check_and_generate_datasets,
    _allowed_labels,
    _subset_for_run,
    _style_selected_rows,
    _normalize_label,
    DATASET_DIR,
    CLASSIFICATION_DATASET_PATH,
    TOOL_SEQUENCE_DATASET_PATH,
    CONTEXT_PRUNING_DATASET_PATH
)

from utils.helpers import (
    _retry,
    enhance_prompt_with_user_input,
    capture_run_config,
    display_run_config,
    _non_empty
)

from utils.execution_tracker import (
    ExecutionEvent,
    ExecutionTracker
)

from core.api_clients import (
    classify_with_openai,
    classify_with_gemini,
    classify_with_ollama,
    classify_with_openrouter,
    openai_structured_json,
    openrouter_json,
    ollama_json,
    generate_text_async
)

# --- PLOTLY CONFIG (to avoid deprecation warnings) ---
PLOTLY_CONFIG = {
    "displaylogo": False,
    "responsive": True,
    "scrollZoom": True
}

# ============================================================
# DEMONSTRATION SCENARIO CONFIGURATIONS
# ============================================================
# NOTE: Scenario configurations moved to config/scenarios.py
# Imported above as: PI_AGENT_GOAL_PROMPT, FOLDING_POLICY_BLOCK, etc.

async def run_live_smoke_test(scenario_key: str):
    """
    Executes a single-turn, critical-path smoke test for the selected scenario,
    with full logging to ExecutionTracker for immediate dashboard visibility.

    Args:
        scenario_key: Key from SMOKE_TEST_SCENARIOS dict

    Returns:
        Dict with success status, final answer, metadata, and code output
    """
    config = SMOKE_TEST_SCENARIOS[scenario_key]
    goal = config['goal']
    agents_to_use = config['agents']
    policy = config['policy']
    mode = config['mode']

    test_data = None
    if mode == 'inference':
        test_data = [{"query": "Test query for smoke test", "expected_sequence": []}]

    # Use a unique test name for the smoke test that the dashboard can filter on
    SMOKE_TEST_NAME = "Smoke Test Run"

    # Get tracker (don't reset to preserve other test data)
    tracker = st.session_state.get('execution_tracker')
    if not tracker:
        tracker = ExecutionTracker()
        st.session_state['execution_tracker'] = tracker

    # Temporarily configure specialized memory for the run
    if 'demo_memory_policy' in st.session_state:
        del st.session_state['demo_memory_policy']
    if policy:
        st.session_state['demo_memory_policy'] = policy
        st.session_state['demo_scenario'] = scenario_key.split('.')[1].strip()

    # Initialize Orchestrator and force load policies
    orchestrator = UnifiedOrchestrator(
        goal=goal,
        test_data=test_data,
        budget=Budget(mode="turns", max_turns=1),
        mode=mode,
        coordination_pattern='leaf_scaffold',
    )

    # Force initialize to load policies
    orchestrator._initialize_stateful_components()

    # Log Orchestrator start
    orch_id = f"smoke_{hash(scenario_key)}"
    tracker.emit(SMOKE_TEST_NAME, "start", orch_id, f"Smoke Test: {scenario_key}", "orchestrator", goal=goal[:80])

    # Execution - Single turn of the Leaf Agent Scaffold
    llm_client = GeminiLLMClient(GEMINI_API_KEY)
    leaf_agents = []
    agent_map = {
        "web_researcher": WebResearchAgent,
        "code_executor": CodeExecutorAgent,
        "validator": ValidatorAgent,
        "content_generator": ContentGeneratorAgent
    }

    # Map roles for clearer logging
    role_map = {
        "web_researcher": {"PI": "Vision/Perception Agent", "Cyber": "Threat Intel Agent"},
        "code_executor": {"PI": "Motor Control Agent", "Cyber": "Risk Score Agent"},
        "validator": {"PI": "Perception Feedback Agent", "Cyber": "Validator Agent"},
        "content_generator": {"PI": "Task Summary Agent", "Cyber": "Report Generator Agent"},
    }
    demo_type = "PI" if "PI Agent" in scenario_key else ("Cyber" if "Cybersecurity" in scenario_key else "General")

    for agent_type_str in agents_to_use:
        agent_class = agent_map.get(agent_type_str)
        if agent_class:
            agent = agent_class(llm_client)
            # Override name for demo clarity
            agent.name = role_map.get(agent_type_str, {}).get(demo_type, agent_class.__name__)
            leaf_agents.append(agent)

    supervisor = SupervisorAgent(leaf_agents, memory_manager=orchestrator.memory_manager)
    supervisor.task_planner = GeminiTaskPlanner(
        available_agents=[a.agent_type for a in leaf_agents],
        llm_client=llm_client
    )
    supervisor.result_synthesizer = GeminiResultSynthesizer(llm_client)

    # Turn Execution
    try:
        # Step 1: Planning
        planning_id = f"{orch_id}_plan"
        tracker.emit(SMOKE_TEST_NAME, "start", planning_id, "Task Planner", "sub_agent", parent_id=orch_id)
        sub_tasks = await supervisor.task_planner.decompose(goal, mode)
        tracker.emit(SMOKE_TEST_NAME, "complete", planning_id, "Task Planner", "sub_agent", parent_id=orch_id, tasks_count=len(sub_tasks))

        # Step 2: Sequential Execution of subtasks
        results = []
        for i, task in enumerate(sub_tasks):
            task_id = f"{orch_id}_task_{i}"
            agent_name = next((a.name for a in leaf_agents if a.agent_type == task.agent_type), task.agent_type.value)

            tracker.emit(SMOKE_TEST_NAME, "start", task_id, agent_name, "leaf_agent", parent_id=orch_id, description=task.description[:50])
            result = await supervisor.execute_single_task(task)
            results.append(result)

            status_key = "success" if result.success else "failure"
            tracker.emit(SMOKE_TEST_NAME, "complete", task_id, agent_name, "leaf_agent", parent_id=orch_id, success=result.success, output_size=len(result.output) if result.output else 0, status=status_key)

        # Step 3: Synthesis
        synthesis_id = f"{orch_id}_synth"
        tracker.emit(SMOKE_TEST_NAME, "start", synthesis_id, "Result Synthesizer", "sub_agent", parent_id=orch_id)
        final_result = await supervisor.result_synthesizer.synthesize(goal, results)
        tracker.emit(SMOKE_TEST_NAME, "complete", synthesis_id, "Result Synthesizer", "sub_agent", parent_id=orch_id, answer_size=len(final_result.answer))

        # Log Orchestrator completion
        tracker.emit(SMOKE_TEST_NAME, "complete", orch_id, f"Smoke Test: {scenario_key}", "orchestrator", final_score=1.0)

        return {
            "success": True,
            "final_answer": final_result.answer,
            "metadata": final_result.metadata,
            "code_output": next((r.output for r in results if r.agent_name in ["Motor Control Agent", "Risk Score Agent", "Code Executor Agent"] and r.output), "No specific code output.")
        }
    except Exception as e:
        # Log Orchestrator error
        tracker.emit(SMOKE_TEST_NAME, "error", orch_id, f"Smoke Test: {scenario_key}", "orchestrator", error=str(e))
        return {
            "success": False,
            "error": str(e),
            "metadata": orchestrator.memory_manager.get_snapshot(99) if orchestrator.memory_manager else {}
        }

# ============================================================
# HELPER FUNCTIONS FOR USER-INPUT-AWARE PROMPTS
# ============================================================

def enhance_prompt_with_user_input(base_prompt: str, user_input: str, context: Dict[str, Any]) -> str:
    """
    Enhances a generated prompt by incorporating user's existing input.

    Args:
        base_prompt: The generated template prompt
        user_input: What the user has already typed
        context: Additional context (dataset info, model selection, etc.)

    Returns:
        Enhanced prompt that preserves user intent while adding suggestions
    """
    if not user_input or len(user_input.strip()) < 10:
        return base_prompt

    # Extract key entities from user input
    user_lower = user_input.lower()

    # Check what user has already specified
    has_dataset_ref = any(word in user_lower for word in ['dataset', 'examples', 'test data', 'queries'])
    has_goal = any(word in user_lower for word in ['goal', 'objective', 'target', 'achieve'])
    has_approach = any(word in user_lower for word in ['approach', 'strategy', 'method', 'using'])

    # Build enhanced prompt
    sections = []

    # Preserve user's input as the primary goal
    sections.append(f"USER'S OBJECTIVE:\n{user_input.strip()}\n")

    # Add dataset context if not mentioned
    if not has_dataset_ref and context.get('dataset_size'):
        sections.append(f"DATASET CONTEXT:\n- {context['dataset_size']} examples available\n- Columns: {', '.join(context.get('columns', []))}\n")

    # Add approach suggestions if not mentioned
    if not has_approach and context.get('suggested_approach'):
        sections.append(f"SUGGESTED APPROACH:\n{context['suggested_approach']}\n")

    # Add success criteria if not mentioned
    if not has_goal and context.get('success_criteria'):
        sections.append(f"SUCCESS CRITERIA:\n{context['success_criteria']}\n")

    return '\n'.join(sections)


# ============================================================
# UNIVERSAL EXECUTION TRACKING SYSTEM
# ============================================================

@dataclass
# ExecutionEvent and ExecutionTracker moved to utils/execution_tracker.py


# --- COST TRACKER IMPORTS ---
from cost_tracker import CostTracker, combined_price_lookup, register_all_extractors


# --- TARGETED SUPPRESSION FOR PLOTLY/STREAMLIT KEYWORD ARGUMENTS ---
# Suppress the specific deprecation message and any Plotly deprecations that bubble up via Streamlit logs
warnings.filterwarnings(
    "ignore",
    message=r".*keyword arguments have been deprecated.*Use `config` instead.*",
)
# Also suppress general Plotly deprecation/future warnings that Streamlit may surface
warnings.filterwarnings("ignore", category=DeprecationWarning, module=r"^plotly(\.|$)")
warnings.filterwarnings("ignore", category=FutureWarning, module=r"^plotly(\.|$)")

load_dotenv()  # load env vars
nest_asyncio.apply()  # patch loop for Streamlit


# ---------- Config / env ----------
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", "")
# --- FIX: Use latest GPT-5 series as default (OpenRouter format) ---
OPENAI_MODEL = st.secrets.get("OPENAI_MODEL", "openai/gpt-5-mini")

# --- NEW: Dataset Directory Configuration ---
DATASET_DIR = st.secrets.get("DATASET_DIR", "test_dataset")
CLASSIFICATION_DATASET_PATH = os.path.join(DATASET_DIR, "classification_dataset.csv")
TOOL_SEQUENCE_DATASET_PATH = os.path.join(DATASET_DIR, "tool_sequence_dataset.csv")
CONTEXT_PRUNING_DATASET_PATH = os.path.join(DATASET_DIR, "context_pruning_dataset.csv")
# -------------------------------------------

OLLAMA_BASE_URL = st.secrets.get("OLLAMA_BASE_URL", "http://localhost:11434/api/generate")
OLLAMA_MODEL = st.secrets.get("OLLAMA_MODEL", "mistral-small:24b-instruct-2501-q4_K_M")

# Optional third model (env-driven)
THIRD_KIND = st.secrets.get("THIRD_KIND", "None")
THIRD_MODEL = st.secrets.get("THIRD_MODEL", "mistralai/mistral-small-3.2-24b-instruct" if THIRD_KIND == "OpenRouter" else "gpt-5-mini")

GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "")
# --- FIX: Use latest Gemini 2.5 series as default (OpenRouter format) ---
GEMINI_MODEL = st.secrets.get("GEMINI_MODEL", "google/gemini-2.5-flash")

# OpenRouter config
OPENROUTER_API_KEY = st.secrets.get("OPENROUTER_API_KEY", "")
OPENROUTER_MODEL = st.secrets.get("OPENROUTER_MODEL", "mistralai/mistral-small-3.2-24b-instruct")
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"
# Default provider toggles (sidebar deprecated; per-test controls set overrides)
use_ollama = False
use_openai = True
use_ollama_local = False


# --- NEW: API Routing Configuration ---
# Set to "openrouter" to route all calls through OpenRouter (unified pricing, simpler)
# Set to "native" to use native APIs (OpenAI SDK, Google Genai SDK) for advanced features
API_ROUTING_MODE = st.secrets.get("API_ROUTING_MODE", "openrouter").lower()  # "openrouter" or "native"

# --- PATCH 22/23: OpenRouter Model Fetching with Metadata ---
OPENROUTER_MODELS_URL = "https://openrouter.ai/api/v1/models"
# A default dictionary in case the API call fails or key is missing
DEFAULT_OPENROUTER_MODEL_METADATA = {
    "mistralai/mistral-small-3.2-24b-instruct": {'context': '131,072', 'input_cost': '$0.06', 'output_cost': '$0.18'},
    "deepseek/deepseek-v3.1-terminus": {'context': '128,000', 'input_cost': '$0.14', 'output_cost': '$0.28'},
}


# --- PATCH 25: Ollama Local Model Definition ---
OLLAMA_MODEL_METADATA = {
    "mistral:latest": {'context': '4,096', 'local_info': 'General Purpose'},
    "llama3:8b": {'context': '8,192', 'local_info': 'Next-Gen Llama'},
    "mistral-small:24b-instruct-2501-q4_K_M": {'context': '32,768', 'local_info': 'Mistral Small Quantized'},
    "Custom...": {'context': 'N/A', 'local_info': 'User Defined'}
}
# -----------------------------------------------

# --- Initialize cost tracker ---
if "cost_tracker" not in st.session_state:
    st.session_state.cost_tracker = CostTracker()
    register_all_extractors()

# --- PATCH 30: Dynamic OpenRouter Pricing Cache (TTL 30 days) ---
# Pricing functions moved to core/pricing.py

# Import pricing cache for backward compatibility
from core.pricing import fetch_openrouter_pricing

OPENROUTER_PRICING_CACHE = fetch_openrouter_pricing()
GEMINI_PRICING_CACHE = OPENROUTER_PRICING_CACHE  # Gemini pricing is in the same cache
OPENAI_PRICING_CACHE = OPENROUTER_PRICING_CACHE  # OpenAI pricing is in the same cache

# Skip to Pydantic models (pricing functions removed)
# All pricing functions (_load_pricing_from_disk, _save_pricing_to_disk, fetch_openrouter_pricing,
# _to_openrouter_model_id, _to_native_model_id, _get_provider_from_model_id,
# _fetch_models_from_openrouter, fetch_gemini_models_from_linkup, _get_default_gemini_models,
# _parse_gemini_models_from_linkup, custom_gemini_price_lookup, fetch_openai_models_from_linkup,
# _get_default_openai_models, _parse_openai_models_from_linkup, get_all_available_models,
# custom_openrouter_price_lookup, _normalize_ollama_root) moved to core/pricing.py and utils/model_discovery.py

# ---------- Pydantic structured outputs ----------
# Pydantic models moved to core/models.py
# Pricing functions removed (lines 478-1086) - see core/pricing.py and utils/model_discovery.py

# ---------- Pydantic structured outputs ----------
# Pydantic models moved to core/models.py
# (Removed ~600 lines of pricing functions - see core/pricing.py)

# All pricing and model discovery functions removed (~550 lines)
# See core/pricing.py and utils/model_discovery.py for:
# - _to_openrouter_model_id, _to_native_model_id, _get_provider_from_model_id
# - _fetch_models_from_openrouter, fetch_gemini_models_from_linkup
# - _get_default_gemini_models, _parse_gemini_models_from_linkup
# - custom_gemini_price_lookup, fetch_openai_models_from_linkup
# - _get_default_openai_models, _parse_openai_models_from_linkup
# - get_all_available_models, custom_openrouter_price_lookup
# - fetch_openrouter_models_for_ui, fetch_openai_models
# - get_third_model_display_name, _normalize_ollama_root

# ---------- Pydantic structured outputs ----------
# Pydantic models moved to core/models.py
# (Removed ~515 lines of pricing/model discovery functions)

# ---------- Pydantic structured outputs ----------
# Pydantic models moved to core/models.py
    # Fetch live pricing from OpenRouter
    gemini_models = _fetch_models_from_openrouter(provider_filter="google")

    if gemini_models:
        return gemini_models

    # Hard fallback if OpenRouter fetch fails (using actual OpenRouter pricing)
    return {
        "google/gemini-2.5-flash": {
            'context': '1,048,576',
            'input_cost': '$0.300',
            'output_cost': '$2.500',
            'input_per_mtok_usd': 0.300,
            'output_per_mtok_usd': 2.500
        },
        "google/gemini-2.5-flash-lite": {
            'context': '1,048,576',
            'input_cost': '$0.100',
            'output_cost': '$0.400',
            'input_per_mtok_usd': 0.100,
            'output_per_mtok_usd': 0.400
        },
        "google/gemini-2.5-pro": {
            'context': '1,048,576',
            'input_cost': '$1.250',
            'output_cost': '$10.000',
            'input_per_mtok_usd': 1.250,
            'output_per_mtok_usd': 10.000
        },
    }

def _parse_gemini_models_from_linkup(data: dict) -> Dict[str, Dict[str, Any]]:
    """
    Parse Gemini model information from Linkup API response.
    Extracts model names, context windows, and pricing.
    """
    import re

    models = {}

    # Get text content from response
    text_content = []
    answer = data.get("answer", "")
    if answer:
        text_content.append(str(answer))

    sources = data.get("sources", [])
    for source in sources:
        content = source.get("content", "") or source.get("snippet", "")
        if content:
            text_content.append(str(content))

    full_text = "\n\n".join(text_content)

    # Regex patterns to find Gemini models
    # Looking for patterns like: gemini-2.5-flash, gemini-1.5-pro, etc.
    model_pattern = re.compile(r'gemini-[\d\.]+-(?:flash|pro|ultra)?', re.IGNORECASE)
    model_names = set(model_pattern.findall(full_text.lower()))

    # Context window patterns
    context_pattern = re.compile(r'(\d+(?:,\d+)*)\s*(?:tokens?|token\s+context|context\s+window)', re.IGNORECASE)

    # Pricing patterns
    price_pattern = re.compile(r'\$?([\d.]+)\s*(?:per|/)\s*(?:million|1M|M)\s*(?:tokens?|input|output)', re.IGNORECASE)

    for model_name in model_names:
        # Try to find context and pricing for this specific model
        # Look for text near the model name
        model_idx = full_text.lower().find(model_name)
        if model_idx != -1:
            # Extract window around model mention
            window_start = max(0, model_idx - 300)
            window_end = min(len(full_text), model_idx + 300)
            window_text = full_text[window_start:window_end]

            # Extract context
            context_match = context_pattern.search(window_text)
            context = context_match.group(1).replace(',', '') if context_match else 'N/A'

            # Extract pricing (simplified - may need refinement)
            prices = price_pattern.findall(window_text)
            input_cost = float(prices[0]) if len(prices) > 0 else 0.0
            output_cost = float(prices[1]) if len(prices) > 1 else 0.0

            # Determine if free (experimental models)
            is_free = 'exp' in model_name or 'free' in model_name or 'free' in window_text.lower()

            if is_free:
                input_cost = 0.0
                output_cost = 0.0

            models[model_name] = {
                'context': f"{int(context):,}" if context != 'N/A' else context,
                'input_cost': 'Free' if input_cost == 0 else f"${input_cost:.3f}",
                'output_cost': 'Free' if output_cost == 0 else f"${output_cost:.2f}",
                'input_per_mtok_usd': input_cost,
                'output_per_mtok_usd': output_cost
            }

    return models if models else _get_default_gemini_models()

# Fetch Gemini 2.5 models from OpenRouter (via default function)
GEMINI_MODELS_FULL = _get_default_gemini_models()

# Extract pricing cache
DEFAULT_GEMINI_PRICING = {
    model_id: {
        "input_per_mtok_usd": meta.get('input_per_mtok_usd', 0.0),
        "output_per_mtok_usd": meta.get('output_per_mtok_usd', 0.0)
    }
    for model_id, meta in GEMINI_MODELS_FULL.items()
}
# -------------------------------------------------------------

# Use the discovered models for pricing cache
GEMINI_PRICING_CACHE = DEFAULT_GEMINI_PRICING.copy()

# Use the discovered models for UI metadata
GEMINI_MODEL_METADATA = {
    model_id: {
        'context': meta.get('context', 'N/A'),
        'input_cost': meta.get('input_cost', 'N/A'),
        'output_cost': meta.get('output_cost', 'N/A')
    }
    for model_id, meta in GEMINI_MODELS_FULL.items()
}

def custom_gemini_price_lookup(provider: str, model: str) -> Optional[Dict[str, float]]:
    """Custom price lookup for Gemini models using 30-day cached pricing."""
    if provider == "Google":
        # Use the 30-day cached pricing data
        return GEMINI_PRICING_CACHE.get(model)
    return None
# ---------------------------------------------------------------

# --- PATCH 31: Dynamic OpenAI Model Discovery via Linkup API ---
@st.cache_data(ttl=60 * 60 * 24 * 30)  # Cache for 30 days
def fetch_openai_models_from_linkup() -> Dict[str, Dict[str, Any]]:
    """
    Fetch latest OpenAI models using Linkup API to search OpenAI's documentation.
    Returns dict mapping model_id to metadata (context, pricing, etc.)
    """
    try:
        import requests
        linkup_api_key = st.secrets.get("LINKUP_API_KEY", "")

        if not linkup_api_key:
            st.warning("⚠️ LINKUP_API_KEY not set. Using default OpenAI models.")
            return _get_default_openai_models()

        # Search for latest OpenAI models
        headers = {
            "Authorization": f"Bearer {linkup_api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "q": "OpenAI API models 2025 GPT-4o GPT-4 GPT-3.5 available models pricing context window tokens",
            "depth": "standard",
            "outputType": "sourcedAnswer",
            "includeImages": False,
            "includeDomains": ["openai.com", "platform.openai.com"],
            "includeInlineCitations": False,
            "includeSources": True
        }

        response = requests.post("https://api.linkup.so/v1/search", headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Extract model information from response
        models = _parse_openai_models_from_linkup(data)

        if models:
            st.success(f"✅ Discovered {len(models)} OpenAI models via Linkup API (cached for 30 days).")
            return models
        else:
            st.info("ℹ️ No models found via Linkup. Using defaults.")
            return _get_default_openai_models()

    except Exception as e:
        st.warning(f"⚠️ Linkup model discovery failed: {e}. Using defaults.")
        return _get_default_openai_models()

def _get_default_openai_models() -> Dict[str, Dict[str, Any]]:
    """Fallback default GPT-5 models - fetches live pricing from OpenRouter."""
    # Fetch live pricing from OpenRouter
    openai_models = _fetch_models_from_openrouter(provider_filter="openai")

    if openai_models:
        return openai_models

    # Hard fallback if OpenRouter fetch fails (using actual OpenRouter pricing)
    return {
        "openai/gpt-5": {
            'context': '400,000',
            'input_cost': '$1.250',
            'output_cost': '$10.000',
            'input_per_mtok_usd': 1.250,
            'output_per_mtok_usd': 10.000
        },
        "openai/gpt-5-mini": {
            'context': '400,000',
            'input_cost': '$0.250',
            'output_cost': '$2.000',
            'input_per_mtok_usd': 0.250,
            'output_per_mtok_usd': 2.000
        },
        "openai/gpt-5-nano": {
            'context': '400,000',
            'input_cost': '$0.050',
            'output_cost': '$0.400',
            'input_per_mtok_usd': 0.050,
            'output_per_mtok_usd': 0.400
        },
    }

def _parse_openai_models_from_linkup(data: dict) -> Dict[str, Dict[str, Any]]:
    """
    Parse OpenAI model information from Linkup API response.
    Extracts model names, context windows, and pricing.
    """
    import re

    models = {}

    # Get text content from response
    text_content = []
    answer = data.get("answer", "")
    if answer:
        text_content.append(str(answer))

    sources = data.get("sources", [])
    for source in sources:
        content = source.get("content", "") or source.get("snippet", "")
        if content:
            text_content.append(str(content))

    full_text = "\n\n".join(text_content)

    # Regex patterns to find OpenAI models
    # Looking for patterns like: gpt-4o, gpt-4-turbo, gpt-3.5-turbo, etc.
    model_pattern = re.compile(r'gpt-(?:4o|4|3\.5)(?:-turbo|-mini)?', re.IGNORECASE)
    model_names = set(model_pattern.findall(full_text.lower()))

    # Context window patterns
    context_pattern = re.compile(r'(\d+(?:,\d+)*)\s*(?:tokens?|token\s+context|context\s+window)', re.IGNORECASE)

    # Pricing patterns
    price_pattern = re.compile(r'\$?([\d.]+)\s*(?:per|/)\s*(?:million|1M|M)\s*(?:tokens?|input|output)', re.IGNORECASE)

    for model_name in model_names:
        # Try to find context and pricing for this specific model
        model_idx = full_text.lower().find(model_name)
        if model_idx != -1:
            # Extract window around model mention
            window_start = max(0, model_idx - 300)
            window_end = min(len(full_text), model_idx + 300)
            window_text = full_text[window_start:window_end]

            # Extract context
            context_match = context_pattern.search(window_text)
            context = context_match.group(1).replace(',', '') if context_match else 'N/A'

            # Extract pricing
            prices = price_pattern.findall(window_text)
            input_cost = float(prices[0]) if len(prices) > 0 else 0.0
            output_cost = float(prices[1]) if len(prices) > 1 else 0.0

            models[model_name] = {
                'context': f"{int(context):,}" if context != 'N/A' else context,
                'input_cost': f"${input_cost:.2f}" if input_cost > 0 else 'N/A',
                'output_cost': f"${output_cost:.2f}" if output_cost > 0 else 'N/A',
                'input_per_mtok_usd': input_cost,
                'output_per_mtok_usd': output_cost
            }

    return models if models else _get_default_openai_models()

# Fetch GPT-5 models from OpenRouter (via default function)
OPENAI_MODELS_FULL = _get_default_openai_models()

# Extract pricing cache
DEFAULT_OPENAI_PRICING = {
    model_id: {
        "input_per_mtok_usd": meta.get('input_per_mtok_usd', 0.0),
        "output_per_mtok_usd": meta.get('output_per_mtok_usd', 0.0)
    }
    for model_id, meta in OPENAI_MODELS_FULL.items()
}
# -------------------------------------------------------------

# Use the discovered models for pricing cache
OPENAI_PRICING_CACHE = DEFAULT_OPENAI_PRICING.copy()

# Use the discovered models for UI metadata
OPENAI_MODEL_METADATA = {
    model_id: {
        'context': meta.get('context', 'N/A'),
        'input_cost': meta.get('input_cost', 'N/A'),
        'output_cost': meta.get('output_cost', 'N/A')
    }
    for model_id, meta in OPENAI_MODELS_FULL.items()
}
# ---------------------------------------------------------------

# --- NEW: Get all available models for UI selection ---
def get_all_available_models() -> List[str]:
    """
    Get all available models from Gemini, OpenAI, and OpenRouter.
    Returns a list of model IDs in OpenRouter format.
    """
    all_models = []

    # Add Gemini models
    gemini_models = list(GEMINI_MODELS_FULL.keys())
    all_models.extend(gemini_models)

    # Add OpenAI models
    openai_models = list(OPENAI_MODELS_FULL.keys())
    all_models.extend(openai_models)

    # Add OpenRouter model (if different from above)
    if OPENROUTER_MODEL not in all_models:
        all_models.append(OPENROUTER_MODEL)

    # Add some common OpenRouter models
    common_openrouter = [
        "mistralai/mistral-small-3.2-24b-instruct",
        "anthropic/claude-3.5-sonnet",
        "meta-llama/llama-3.1-70b-instruct",
        "deepseek/deepseek-v3.1-terminus",
    ]

    for model in common_openrouter:
        if model not in all_models:
            all_models.append(model)

    return sorted(all_models)

# Get available models
AVAILABLE_MODELS = get_all_available_models()

# --- PATCH 30: Unified Custom Price Lookup Function ---
def custom_openrouter_price_lookup(provider: str, model: str) -> Optional[Dict[str, float]]:
    """Unified custom price lookup that uses 30-day cached pricing for all providers."""
    if provider == "OpenRouter":
        # Use the 30-day cached OpenRouter pricing
        return OPENROUTER_PRICING_CACHE.get(model)
    elif provider == "Google":
        # Use the 30-day cached Gemini pricing
        return custom_gemini_price_lookup(provider, model)
    elif provider == "OpenAI":
        # Use the 30-day cached OpenAI pricing
        return OPENAI_PRICING_CACHE.get(model)
    # Fallback to combined lookup for other providers
    return combined_price_lookup(provider, model)
# -------------------------------------------------------------------


# Helper to normalize Ollama base URL to server root
def _normalize_ollama_root(url: str) -> str:
    u = (url or "").rstrip("/")
    if u.endswith("/api/generate"):
        u = u[: -len("/api/generate")]
    return u or "http://localhost:11434"

# ---------- Pydantic structured outputs ----------
class Classification(BaseModel):
    classification_result: str
    rationale: str

class ClassificationWithConf(Classification):
    confidence: Optional[float] = Field(default=None, ge=0.0, le=1.0)

# --- PATCH 2: NEW SCHEMAS FOR DATA GENERATION AND SUMMARY ---
class SyntheticDataItem(BaseModel):
    query: str = Field(description="The user query or input text.")
    classification: str = Field(description="The expected classification label.") # Used for Classification data type

class ToolCallSequenceItem(BaseModel):
    query: str = Field(description="The user query requiring a specific tool sequence.")
    # We use classification for compatibility with the final DF structure, but its meaning changes
    expected_sequence: List[str] = Field(description="The sequential list of tool names or actions expected.")

# --- PATCH 15: Pruning Data Schema Definition ---
class PruningDataItem(BaseModel):
    instruction: str = Field(description="System instruction/persona given to the LLM.")
    summary: str = Field(description="Summary of the conversation history so far.")
    user_msgs: str = Field(description="Pipe-separated (||) list of previous user messages.")
    agent_resps: str = Field(description="Pipe-separated (||) list of previous agent responses.")
    tool_logs: str = Field(description="Pipe-separated (||) list of previous tool call logs/results.")
    new_question: str = Field(description="The latest user query.")
    expected_action: str = Field(description="The ground truth next action (general_answer, kb_lookup, tool_call).")
    expected_kept_keys: str = Field(description="Comma-separated list of context keys expected to be kept (e.g., instruction, summary).")
# -----------------------------------------------------------

class TestSummaryAndRefinement(BaseModel):
    findings_summary: str = Field(description="A concise summary of the test results and model performance.")
    key_suggestions: List[str] = Field(description="Actionable suggestions for prompt, architecture, or tool improvement.")
    suggested_improvement_code: Optional[str] = Field(default=None, description="The refined Python code, agent framework, or prompt string based on suggestions, ready for the next test iteration.")
    suggested_improvement_prompt_reasoning: Optional[str] = Field(default=None, description="Detailed reasoning for the suggested code or prompt improvement.")

# --- PATCH 31: Pydantic Schemas for Self-Correcting Research Pipeline ---
class FactualConstraint(BaseModel):
    """Schema for individual policy constraints extracted from validation failures."""
    constraint_id: str = Field(description="Unique identifier for the constraint (e.g., 'NO_PHD_CLAIM')")
    constraint_text: str = Field(description="The explicit rule/fact to enforce")
    priority: float = Field(default=0.5, ge=0.0, le=1.0, description="Importance score (0.0-1.0)")
    severity: str = Field(default="MEDIUM", description="Severity level: CRITICAL, HIGH, MEDIUM, LOW")

class ValidationResultArtifact(BaseModel):
    """Schema for ValidatorAgent output with policy update information."""
    confidence_score: float = Field(ge=0.0, le=1.0, description="Overall validation confidence (0.0-1.0)")
    final_verdict: str = Field(description="Verification status: 'Verified', 'Hallucination Detected', or 'Inconclusive'")
    policy_updates: List[FactualConstraint] = Field(default_factory=list, description="New constraints to add to memory")
    report_summary: str = Field(description="Cleaned summary for ContentGenerator")
    red_flags: Optional[List[Dict[str, Any]]] = Field(default_factory=list, description="Detected hallucination risks")

def convert_validation_to_artifact(validation_result: Dict[str, Any]) -> ValidationResultArtifact:
    """
    Convert ValidatorAgent's validation_result to ValidationResultArtifact schema.

    This enables type-safe policy updates while maintaining backward compatibility
    with the existing validation_result format.
    """
    # Map verdict to final_verdict
    verdict = validation_result.get("verdict", "UNKNOWN")
    if verdict == "VERIFIED":
        final_verdict = "Verified"
    elif verdict == "FAILED":
        final_verdict = "Hallucination Detected"
    else:
        final_verdict = "Inconclusive"

    # Convert policy_suggestions to FactualConstraint objects
    policy_updates = []
    for idx, suggestion in enumerate(validation_result.get("policy_suggestions", [])):
        # Extract constraint ID from suggestion text (e.g., "CONSTRAINT: No PhD claims" -> "NO_PHD_CLAIMS")
        constraint_id = f"CONSTRAINT_{idx + 1}"
        if ":" in suggestion:
            # Try to extract meaningful ID from constraint text
            constraint_text = suggestion.split(":", 1)[1].strip()
            # Generate ID from first few words
            words = constraint_text.upper().replace(",", "").replace(".", "").split()[:3]
            constraint_id = "_".join(words)
        else:
            constraint_text = suggestion

        # Determine severity based on hallucination risk
        red_flags = validation_result.get("red_flags", [])
        max_risk = max([rf.get("hallucination_risk", 0.0) for rf in red_flags], default=0.0)

        if max_risk >= 0.8:
            severity = "CRITICAL"
            priority = 0.9
        elif max_risk >= 0.6:
            severity = "HIGH"
            priority = 0.7
        elif max_risk >= 0.4:
            severity = "MEDIUM"
            priority = 0.5
        else:
            severity = "LOW"
            priority = 0.3

        policy_updates.append(FactualConstraint(
            constraint_id=constraint_id,
            constraint_text=constraint_text,
            priority=priority,
            severity=severity
        ))

    return ValidationResultArtifact(
        confidence_score=validation_result.get("confidence_score", 0.5),
        final_verdict=final_verdict,
        policy_updates=policy_updates,
        report_summary=validation_result.get("summary", "No summary available"),
        red_flags=validation_result.get("red_flags", [])
    )
# -----------------------------------------------------------

@st.cache_data(ttl=3600)  # Cache for 1 hour (UI list refreshes more frequently than pricing)
def fetch_openrouter_models_for_ui() -> Dict[str, Dict[str, Any]]:
    """Fetches models from OpenRouter that support structured outputs, with metadata for UI display."""
    if not OPENROUTER_API_KEY:
        st.warning("OPENROUTER_API_KEY not set. Using default model list.")
        return DEFAULT_OPENROUTER_MODEL_METADATA

    params = {
        "supported_parameters": "structured_outputs",
        "order": "newest",
    }
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }

    try:
        with httpx.Client(timeout=10) as client:
            response = client.get(OPENROUTER_MODELS_URL, headers=headers, params=params)
            response.raise_for_status()
            data = response.json()

            model_metadata = {}
            for m in data.get('data', []):
                model_id = m['id']

                # Extract context length
                context = m.get('context_length', 'N/A')

                # PATCH 30: Use the 30-day cached pricing data if available
                cached_pricing = OPENROUTER_PRICING_CACHE.get(model_id, {})
                if cached_pricing:
                    # Use cached pricing (already in per-million-tokens format)
                    input_cost = cached_pricing.get('input_per_mtok_usd', 0.0)
                    output_cost = cached_pricing.get('output_per_mtok_usd', 0.0)
                else:
                    # Fallback to API response pricing
                    pricing = m.get('pricing', {})
                    input_cost_raw = pricing.get('prompt', 0.0)
                    output_cost_raw = pricing.get('completion', 0.0)

                    try:
                        input_cost = float(input_cost_raw) * 1_000_000 if input_cost_raw else 0.0
                        output_cost = float(output_cost_raw) * 1_000_000 if output_cost_raw else 0.0
                    except (ValueError, TypeError):
                        input_cost = 0.0
                        output_cost = 0.0

                # Format context - handle both int and string types
                if isinstance(context, int):
                    context_str = f"{context:,}"
                elif isinstance(context, str):
                    context_str = context
                else:
                    context_str = str(context)

                # Format costs - ensure they are numeric before comparison
                try:
                    input_cost_num = float(input_cost) if not isinstance(input_cost, (int, float)) else input_cost
                    output_cost_num = float(output_cost) if not isinstance(output_cost, (int, float)) else output_cost
                except (ValueError, TypeError):
                    input_cost_num = 0.0
                    output_cost_num = 0.0

                model_metadata[model_id] = {
                    'context': context_str,
                    'input_cost': f"${input_cost_num:.2f}" if input_cost_num > 0 else "Free",
                    'output_cost': f"${output_cost_num:.2f}" if output_cost_num > 0 else "Free",
                }

            # Limit to 50 for a manageable dropdown
            limited_model_metadata = {k: v for k, v in list(model_metadata.items())[:50]}

            return limited_model_metadata

    except Exception as e:
        st.error(f"Failed to fetch models from OpenRouter: {e}. Using default list.")
        return DEFAULT_OPENROUTER_MODEL_METADATA

# Call the function once to populate the list used in the sidebar
OPENROUTER_MODEL_METADATA = fetch_openrouter_models_for_ui()
# -------------------------------------------------------------

# --- PATCH 31: Use Linkup-discovered OpenAI models ---
# Use the models discovered via Linkup API
@st.cache_data(ttl=3600)  # Cache for 1 hour (UI refresh)
def fetch_openai_models() -> Dict[str, Dict[str, Any]]:
    """Returns OpenAI models discovered via Linkup API (cached for 30 days)."""
    # Simply return the Linkup-discovered models
    return OPENAI_MODEL_METADATA.copy()

# Populate the OpenAI model metadata from Linkup-discovered models
_openai_model_ids = list(OPENAI_MODEL_METADATA.keys())
# -------------------------------------------------------------

SKELETON_COLUMNS = [
    "query", "classification",
    # openrouter mistral
    "classification_result_openrouter_mistral", "classification_result_openrouter_mistral_rationale", "classification_result_openrouter_mistral_confidence", "latency_openrouter_mistral",
    # openai (gpt-5-mini)
    "classification_result_openai", "classification_result_openai_rationale", "classification_result_openai_confidence", "latency_openai",
    # gemini
    "classification_result_gemini", "classification_result_gemini_rationale", "classification_result_gemini_confidence", "latency_gemini",
    # third model (optional)
    "classification_result_third", "classification_result_third_rationale", "classification_result_third_confidence", "latency_third",
    # ensemble / judge
    "weighted_pick_model", "weighted_pick_label",
    "judge_choice_model", "judge_choice_label", "judge_rationale",
]

# --- PATCH 19: Suggested Prompts for Data Generation Tab ---
SUGGESTED_PROMPTS = {
    "Classification": [
        "A classification dataset for identifying sentiment in social media posts, with labels: positive, negative, neutral.",
        "Generate queries asking for technical support for a banking application, categorized by: login_issue, transaction_error, account_settings, general_help.",
        "Create short news headlines classified into five topics: sports, finance, politics, tech, weather."
    ],
    "Tool/Agent Sequence": [
        "Queries that require a sequence of internal tools like: crm_lookup -> schedule_callback, or check_inventory -> order_part.",
        "Generate customer support questions requiring tool sequences for flight booking: check_availability -> select_seat -> confirm_payment.",
        "Queries needing a single tool call followed by a database query: user_auth -> internal_db_read."
    ],
    "Context Pruning": [
        "Conversational turns where the new question only requires the `tool_logs` and `new_question` to take the action `kb_lookup` (summary is irrelevant).",
        "Scenarios where the `instruction` and `summary` are critical to deciding the `tool_call` action.",
        "Generate examples where the `new_question` is a simple follow-up, and the action should be `general_answer`, keeping only `summary`."
    ]
}

# --- NEW: Default Dataset Generation Prompts ---
DEFAULT_DATASET_PROMPTS = {
    "Classification": """Generate a diverse classification dataset for an IT support chatbot with three categories:
1. 'general_chat' - casual greetings, small talk, non-technical questions
2. 'kb_lookup' - questions about IT concepts, tools, best practices, knowledge base queries
3. 'tool' - requests that require calling specific tools or APIs (e.g., database queries, system lookups, device information)

Create varied, realistic user queries that cover common IT support scenarios. Include questions about network devices, incidents, IP addresses, locations, and general IT knowledge.""",

    "Tool/Agent Sequence": """Generate queries that require multi-step tool sequences for an IT operations agent:
- Single tool calls: check_device, get_incident, lookup_ip
- Two-step sequences: authenticate_user -> query_database, check_inventory -> create_order
- Three-step sequences: validate_request -> fetch_data -> update_system

Include realistic IT operations scenarios like device management, incident tracking, and network operations.""",

    "Context Pruning": """Generate context pruning test cases for a conversational AI agent with three action types:
1. 'general_answer' - Simple questions that only need conversation summary
2. 'kb_lookup' - Knowledge base queries that need instruction and tool logs
3. 'tool_call' - Actions requiring full context (instruction, summary, tool logs)

Create realistic multi-turn conversations with varying context requirements. Include banking, IT support, and customer service scenarios."""
}
# -------------------------------------------------------------

# Rate limit (max concurrent calls)
CONCURRENCY_LIMIT = int(st.secrets.get("CONCURRENCY_LIMIT", "1000"))
_rate_limiter = asyncio.Semaphore(CONCURRENCY_LIMIT)


# ---------- Row-limit helpers ----------
from typing import Optional as _OptInt
ROW_LIMIT_OPTIONS = {"First 5": 5, "First 25": 25, "First 100": 100, "All": None}

# --- PATCH 13: Helper for Dynamic Third Model Name ---
def get_third_model_display_name() -> str:
    """Dynamically returns the name of the configured third model."""
    if THIRD_KIND == "None" or not THIRD_MODEL:
        return "Third Model (N/A)"

    model_name = THIRD_MODEL
    # Try to shorten OpenRouter model names for display
    if THIRD_KIND == "OpenRouter":
        model_name = model_name.split('/')[-1]

    # Cap length for readability
    if len(model_name) > 30:
        model_name = model_name[:27] + "..."

    return f"{THIRD_KIND} ({model_name})"
# ---------------------------------------------------

def _subset_for_run(df: pd.DataFrame, n: _OptInt[int]) -> pd.DataFrame:
    try:
        if n is None or not len(df) or (isinstance(n, int) and n >= len(df)):
            return df
        if isinstance(n, int) and n > 0:
            return df.head(n)
        return df
    except Exception:
        return df

def _style_selected_rows(df: pd.DataFrame, n: _OptInt[int]):
    try:
        # Truncate long rationale-like columns for display safety
        display_df = df.copy()
        rationale_cols = [c for c in display_df.columns if c.endswith("_rationale") or c == "judge_rationale"]
        for col in rationale_cols:
            try:
                display_df[col] = display_df[col].astype(str).str.slice(0, 150) + "..."
            except Exception:
                pass

        if n is None:
            selected = set(display_df.index)
        else:
            selected = set(display_df.index[: max(0, int(n))])
        def _hl(row):
            return ["background-color: #1E4594"] * len(row) if row.name in selected else [""] * len(row)
        return display_df.style.apply(lambda r: _hl(r), axis=1)
    except Exception:
        return df

# ---------- Helpers for saving and explaining ----------
def save_results_df(df: pd.DataFrame, test_name: str, row_limit: Optional[int], is_pruning_test: bool = False):
    """Saves the results DataFrame to the test_output directory."""
    try:
        output_dir = "test_output"
        os.makedirs(output_dir, exist_ok=True)
        timestamp = time.strftime("%Y%m%d-%H%M%S")

        if is_pruning_test:
            # For Test 4, the context is the length of the testset df
            limit_str = f"{len(df)}_rows"
            test_slug = "test_4_pruning"
        else:
            limit_str = f"{row_limit}_rows" if row_limit is not None else "all_rows"
            test_slug = test_name.lower().replace(" ", "_").replace(":", "").replace(",", "")

        filename = f"{output_dir}/{test_slug}_{limit_str}_{timestamp}.csv"
        df.to_csv(filename, index=False)
        st.success(f"Results saved to `{filename}`")
    except Exception as e:
        st.warning(f"Failed to save results: {e}")

# ---------- UI ----------
st.set_page_config(page_title="Classification + Eval (OpenRouter Mistral + OpenAI)", layout="wide")
st.title("🧩 Enhanced Classification + Evaluation Suite")
st.caption(f"Dataset directory: `{DATASET_DIR}` — Datasets auto-generated and persisted.")

# --- API Routing Configuration UI ---
with st.sidebar:
    st.header("⚙️ API Configuration")

    # Global API routing mode selector
    api_mode = st.radio(
        "API Routing Mode",
        options=["openrouter", "native"],
        index=0 if API_ROUTING_MODE == "openrouter" else 1,
        help="""
        **OpenRouter Mode**: Routes all API calls through OpenRouter
        - ✅ Unified pricing from one source
        - ✅ Consistent model IDs
        - ✅ Accurate cost tracking
        - ✅ Access to latest models

        **Native Mode**: Uses native provider APIs (OpenAI SDK, Google Genai SDK)
        - ✅ Access to advanced features (file analysis, vision, etc.)
        - ✅ Direct provider integration
        - ⚠️ Requires separate API keys for each provider
        """
    )

    # Update global routing mode
    globals()['API_ROUTING_MODE'] = api_mode

    st.divider()

    # Display current model configuration
    st.subheader("📋 Current Models")

    if api_mode == "openrouter":
        st.info("**Using OpenRouter for all calls**")
        st.code(f"OpenAI: {OPENAI_MODEL}\nGemini: {GEMINI_MODEL}\nOpenRouter: {OPENROUTER_MODEL}", language="text")
    else:
        st.info("**Using Native APIs**")
        st.code(f"OpenAI: {_to_native_model_id(OPENAI_MODEL)}\nGemini: {_to_native_model_id(GEMINI_MODEL)}", language="text")

    st.divider()

# ---------- NEW: Dataset Generation and Persistence Helpers ----------

def ensure_dataset_directory():
    """Ensure the test_dataset directory exists."""
    os.makedirs(DATASET_DIR, exist_ok=True)

def save_dataset_to_file(df: pd.DataFrame, dataset_type: str, model_used: Optional[str] = None, routing_mode: Optional[str] = None):
    """Save a dataset to the appropriate file based on type and write a .meta.json with model/routing."""
    ensure_dataset_directory()

    if dataset_type == "Classification":
        path = CLASSIFICATION_DATASET_PATH
    elif dataset_type == "Tool/Agent Sequence":
        path = TOOL_SEQUENCE_DATASET_PATH
    elif dataset_type == "Context Pruning":
        path = CONTEXT_PRUNING_DATASET_PATH
    else:
        st.warning(f"Unknown dataset type: {dataset_type}")
        return

    try:
        df.to_csv(path, index=False)
        # Write sidecar metadata for traceability
        meta_path = path.replace(".csv", ".meta.json")
        meta = {
            "model": model_used,
            "routing_mode": routing_mode or API_ROUTING_MODE,
            "when": pd.Timestamp.utcnow().isoformat(),
            "dataset_type": dataset_type,
        }
        # Attach cost tracker snapshot if available
        try:
            ct = st.session_state.cost_tracker
            meta["cost_tracker_totals"] = dict(ct.totals)
            meta["cost_tracker_calls"] = len(ct.by_call)
        except Exception:
            pass
        try:
            with open(meta_path, "w", encoding="utf-8") as f:
                json.dump(meta, f, indent=2)
        except Exception:
            pass

        if model_used:
            st.success(f"✅ Saved {dataset_type} dataset to `{path}` using {model_used} [{routing_mode or API_ROUTING_MODE}]")
        else:
            st.success(f"✅ Saved {dataset_type} dataset to `{path}`")
    except Exception as e:
        st.error(f"Failed to save {dataset_type} dataset: {e}")

def load_classification_dataset() -> pd.DataFrame:
    """Load classification dataset from file, or return empty DataFrame."""
    try:
        if os.path.exists(CLASSIFICATION_DATASET_PATH):
            df = pd.read_csv(CLASSIFICATION_DATASET_PATH)
            cols_map = {c.lower(): c for c in df.columns}
            q, c = cols_map.get("query"), cols_map.get("classification")
            if q and c:
                df2 = df[[q, c]].rename(columns={q: "query", c: "classification"})
            else:
                df2 = pd.DataFrame(columns=["query", "classification"])

            # Add skeleton columns
            for col in SKELETON_COLUMNS:
                if col not in df2.columns:
                    df2[col] = None

            # Backward compatibility
            backcompat_map = {
                "classification_result_ollama": "classification_result_openrouter_mistral",
                "classification_result_ollama_rationale": "classification_result_openrouter_mistral_rationale"
            }
            for old_col, new_col in backcompat_map.items():
                if old_col in df.columns and new_col in df2.columns:
                    try:
                        df2[new_col] = df[old_col]
                    except Exception:
                        pass

            return df2[SKELETON_COLUMNS]
        else:
            return pd.DataFrame(columns=SKELETON_COLUMNS)
    except Exception as e:
        st.warning(f"Could not load classification dataset: {e}")
        return pd.DataFrame(columns=SKELETON_COLUMNS)

def load_tool_sequence_dataset() -> pd.DataFrame:
    """Load tool/agent sequence dataset from file."""
    try:
        if os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
            return pd.read_csv(TOOL_SEQUENCE_DATASET_PATH)
        else:
            return pd.DataFrame(columns=["query", "expected_sequence"])
    except Exception as e:
        st.warning(f"Could not load tool sequence dataset: {e}")
        return pd.DataFrame(columns=["query", "expected_sequence"])

# ---------- Label Normalization Helpers ----------
_CANON_MAP = {
    # general
    "general": "general_answer", "general answer": "general_answer",
    "general_answer": "general_answer",

    # knowledge lookups
    "kb": "kb_lookup", "kb lookup": "kb_lookup", "kb_lookup": "kb_lookup",
    "knowledge": "kb_lookup", "knowledge_lookup": "kb_lookup",

    # tool calls
    "tool": "tool_call", "tool call": "tool_call", "tool_call": "tool_call", "toolcall": "tool_call",
}

def _normalize_label(s: Optional[str]) -> str:
    if not s:
        return ""
    s = str(s).strip().lower()
    s = re.sub(r"\s+", " ", s)
    s = s.replace("-", " ").replace("_", " ")
    canon = _CANON_MAP.get(s)
    if canon:
        return canon
    return s.replace(" ", "_")

def load_context_pruning_dataset() -> pd.DataFrame:
    """Load context pruning dataset from file."""
    try:
        if os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
            df = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH)
            # Optional hard fail on legacy labels
            VALID_ACTIONS = {"general_answer", "kb_lookup", "tool_call"}
            if "expected_action" in df.columns:
                invalid = set(df["expected_action"].dropna().map(_normalize_label)) - VALID_ACTIONS
                if invalid:
                    raise ValueError(f"Invalid action labels found: {sorted(invalid)}")
            return df
        else:
            required_cols = list(PruningDataItem.model_fields.keys())
            return pd.DataFrame(columns=required_cols)
    except Exception as e:
        st.warning(f"Could not load context pruning dataset: {e}")
        required_cols = list(PruningDataItem.model_fields.keys())
        return pd.DataFrame(columns=required_cols)

# ---------- Data loading ----------
def _load_df_from_path() -> pd.DataFrame:
    """Load classification dataset (backward compatibility wrapper)."""
    return load_classification_dataset()

# --- NEW: Auto-generate datasets on first run if they don't exist ---
async def auto_generate_default_datasets():
    """Generate default datasets if they don't exist in the test_dataset directory."""
    ensure_dataset_directory()

    datasets_to_generate = []

    # Check which datasets are missing
    if not os.path.exists(CLASSIFICATION_DATASET_PATH):
        datasets_to_generate.append(("Classification", CLASSIFICATION_DATASET_PATH, 100))
    if not os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
        datasets_to_generate.append(("Tool/Agent Sequence", TOOL_SEQUENCE_DATASET_PATH, 50))
    if not os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
        datasets_to_generate.append(("Context Pruning", CONTEXT_PRUNING_DATASET_PATH, 50))

    if not datasets_to_generate:
        return  # All datasets exist

    st.info(f"🔄 Generating {len(datasets_to_generate)} missing dataset(s)... This may take a moment.")

    # Use a simple, fast model for default generation
    default_gen_model = "openai/gpt-5-mini"

    for data_type, path, size in datasets_to_generate:
        try:
            with st.spinner(f"Generating {data_type} dataset ({size} items)..."):
                prompt = DEFAULT_DATASET_PROMPTS[data_type]
                df = await generate_synthetic_data(prompt, size, data_type, default_gen_model)

                if not df.empty:
                    df.to_csv(path, index=False)
                    st.success(f"✅ Generated and saved {data_type} dataset to `{path}`")
                else:
                    st.warning(f"⚠️ Failed to generate {data_type} dataset")
        except Exception as e:
            st.error(f"Error generating {data_type} dataset: {e}")

def check_and_generate_datasets():
    """Check if datasets exist, and generate them if needed (synchronous wrapper)."""
    ensure_dataset_directory()

    # Check if any datasets are missing
    missing = []
    if not os.path.exists(CLASSIFICATION_DATASET_PATH):
        missing.append("Classification")
    if not os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
        missing.append("Tool/Agent Sequence")
    if not os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
        missing.append("Context Pruning")

    if missing:
        st.warning(f"⚠️ Missing datasets: {', '.join(missing)}. Use the 'Preparation' tab to generate them.")

# Initialize session state
if "df" not in st.session_state:
    st.session_state.df = _load_df_from_path()

# --- Initialize Universal Execution Tracker ---
if 'execution_tracker' not in st.session_state:
    st.session_state['execution_tracker'] = ExecutionTracker()

# --- PATCH 16: Initialize Pruning DF in session state ---
if "agent_df" not in st.session_state:
    st.session_state.agent_df = load_tool_sequence_dataset()
if "pruning_df" not in st.session_state:
    st.session_state.pruning_df = load_context_pruning_dataset()
# --------------------------------------------------------

# Check for missing datasets on startup
if "datasets_checked" not in st.session_state:
    check_and_generate_datasets()
    st.session_state.datasets_checked = True

# ---------- Helpers ----------
def _allowed_labels(df: pd.DataFrame) -> List[str]:
    try:
        if "classification" in df.columns:
            return sorted({str(x) for x in df["classification"].dropna().unique().tolist()})
    except Exception:
        pass
    return []

async def _retry(op, attempts: int = 3, base_delay: float = 0.5):
    last = None
    for i in range(attempts):
        try:
            return await op()
        except Exception as e:
            last = e
            await asyncio.sleep(base_delay * (2 ** i))
    raise last

# ============================================================
# DASHBOARD VISUALIZATION HELPER FUNCTIONS
# ============================================================
# NOTE: Visualization functions moved to utils/visualizations.py and utils/gantt_charts.py
# Imported above as: render_test_flow_diagram, render_kpi_metrics, etc.

# render_kpi_metrics moved to utils/visualizations.py
# render_cost_dashboard moved to utils/visualizations.py

# visualize_dataset_composition moved to utils/visualizations.py

def render_model_comparison_chart(df: pd.DataFrame, model_cols: List[str], model_names: List[str] = None):
    """
    Renders an interactive model comparison chart with F1 scores (bars) and latency (line on secondary axis).

    Args:
        df: Results dataframe
        model_cols: List of model column prefixes (e.g., ["openrouter_mistral", "openai", "third"])
        model_names: Optional list of display names for models
    """
    if df.empty or "classification" not in df.columns:
        return

    if model_names is None:
        model_names = model_cols

    # Calculate F1 scores and latencies for each model
    y_true = df["classification"].fillna("").map(_normalize_label).tolist()

    f1_scores = []
    latencies = []
    valid_models = []
    valid_names = []

    for i, model_prefix in enumerate(model_cols):
        result_col = f"classification_result_{model_prefix}"
        latency_col = f"latency_{model_prefix}"

        if result_col in df.columns:
            y_pred = df[result_col].fillna("").map(_normalize_label).tolist()
            valid_indices = [idx for idx, (t, p) in enumerate(zip(y_true, y_pred)) if t and p]

            if valid_indices:
                y_true_f = [y_true[idx] for idx in valid_indices]
                y_pred_f = [y_pred[idx] for idx in valid_indices]

                # Calculate F1 score
                report = classification_report(y_true_f, y_pred_f, output_dict=True, zero_division=0)
                f1 = report.get('macro avg', {}).get('f1-score', 0.0)
                f1_scores.append(f1)

                # Calculate average latency
                if latency_col in df.columns and df[latency_col].notna().any():
                    avg_latency = df[latency_col].mean()
                    latencies.append(avg_latency)
                else:
                    latencies.append(0.0)

                valid_models.append(model_prefix)
                valid_names.append(model_names[i] if i < len(model_names) else model_prefix)

    if not valid_models:
        return

    # Create figure with secondary y-axis
    fig = make_subplots(specs=[[{"secondary_y": True}]])

    # Add F1 score bars
    fig.add_trace(
        go.Bar(
            name="F1 Score",
            x=valid_names,
            y=f1_scores,
            text=[f"{f1:.3f}" for f1 in f1_scores],
            textposition='auto',
            marker_color='#1f77b4',
            hovertemplate='<b>%{x}</b><br>F1 Score: %{y:.4f}<extra></extra>'
        ),
        secondary_y=False
    )

    # Add latency line
    fig.add_trace(
        go.Scatter(
            name="Avg Latency",
            x=valid_names,
            y=latencies,
            mode='lines+markers',
            line=dict(color='#ff7f0e', width=2),
            marker=dict(size=8),
            hovertemplate='<b>%{x}</b><br>Latency: %{y:.2f}s<extra></extra>'
        ),
        secondary_y=True
    )

    # Update axes
    fig.update_xaxes(title_text="Model")
    fig.update_yaxes(title_text="F1 Score", secondary_y=False, range=[0, 1])
    fig.update_yaxes(title_text="Latency (seconds)", secondary_y=True)

    fig.update_layout(
        title="Model Performance Comparison",
        height=400,
        hovermode='x unified',
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )

    st.plotly_chart(fig, use_container_width=True, config=PLOTLY_CONFIG)

def render_organized_results(df: pd.DataFrame, test_type: str = "classification", model_cols: List[str] = None, model_names: List[str] = None):
    """
    Renders results in organized subtabs: Summary, Performance, Errors, Raw Data.

    Args:
        df: Results dataframe
        test_type: Type of test ("classification", "pruning", "agent")
        model_cols: List of model column prefixes for classification tests
        model_names: Optional list of display names for models
    """
    if df.empty:
        st.warning("No results to display.")
        return

    # Create subtabs
    result_tabs = st.tabs(["📊 Summary", "🎯 Performance", "❌ Errors", "💾 Raw Data"])

    # Tab 1: Summary
    with result_tabs[0]:
        st.subheader("Summary Dashboard")

        if test_type == "classification":
            # KPI metrics
            render_kpi_metrics(df, test_type="classification", model_cols=model_cols)

            st.divider()

            # Model comparison chart
            if model_cols:
                render_model_comparison_chart(df, model_cols, model_names)

        elif test_type == "pruning":
            render_kpi_metrics(df, test_type="pruning")

    # Tab 2: Performance
    with result_tabs[1]:
        st.subheader("Detailed Performance Reports")

        if test_type == "classification" and "classification" in df.columns:
            y_true = df["classification"].tolist()

            # Generate reports for each model side-by-side
            if model_cols:
                cols = st.columns(len(model_cols))

                for i, (col, model_prefix) in enumerate(zip(cols, model_cols)):
                    result_col = f"classification_result_{model_prefix}"

                    if result_col in df.columns:
                        with col:
                            model_name = model_names[i] if model_names and i < len(model_names) else model_prefix
                            y_pred = df[result_col].tolist()

                            # Generate classification report
                            y_true_norm = [_normalize_label(s) for s in y_true]
                            y_pred_norm = [_normalize_label(s) for s in y_pred]

                            valid_indices = [idx for idx, (t, p) in enumerate(zip(y_true_norm, y_pred_norm)) if t and p]

                            if valid_indices:
                                y_true_f = [y_true_norm[idx] for idx in valid_indices]
                                y_pred_f = [y_pred_norm[idx] for idx in valid_indices]

                                report_dict = classification_report(y_true_f, y_pred_f, output_dict=True, zero_division=0)

                                st.markdown(f"**{model_name}**")
                                st.dataframe(pd.DataFrame(report_dict).transpose().style.format("{:.2f}"), use_container_width=True)

    # Tab 3: Errors
    with result_tabs[2]:
        st.subheader("Error Analysis")

        if test_type == "classification" and "classification" in df.columns and model_cols:
            # Create error analysis dataframe
            df_analysis = df.copy()
            df_analysis['ground_truth'] = df_analysis['classification'].map(_normalize_label)

            # Add correctness columns for each model
            for model_prefix in model_cols:
                result_col = f"classification_result_{model_prefix}"
                if result_col in df_analysis.columns:
                    df_analysis[f'{model_prefix}_norm'] = df_analysis[result_col].map(_normalize_label)
                    df_analysis[f'{model_prefix}_correct'] = df_analysis[f'{model_prefix}_norm'] == df_analysis['ground_truth']

            # Filter options
            filter_options = ["All Errors"]

            # Add model-specific filters
            for model_prefix in model_cols:
                if f'{model_prefix}_correct' in df_analysis.columns:
                    filter_options.append(f"{model_prefix.replace('_', ' ').title()} Errors Only")

            # Add comparison filters if multiple models
            if len(model_cols) >= 2:
                filter_options.extend([
                    "All Models Incorrect",
                    "All Models Correct",
                    "Model Disagreements"
                ])

            filter_choice = st.selectbox("Filter errors:", filter_options)

            # Apply filter
            if filter_choice == "All Errors":
                # Show all rows where at least one model is incorrect
                error_mask = False
                for model_prefix in model_cols:
                    if f'{model_prefix}_correct' in df_analysis.columns:
                        error_mask = error_mask | ~df_analysis[f'{model_prefix}_correct']
                view_df = df_analysis[error_mask] if isinstance(error_mask, pd.Series) else df_analysis

            elif "Errors Only" in filter_choice:
                # Show errors for specific model
                model_prefix = filter_choice.split(" Errors Only")[0].lower().replace(" ", "_")
                if f'{model_prefix}_correct' in df_analysis.columns:
                    view_df = df_analysis[~df_analysis[f'{model_prefix}_correct']]
                else:
                    view_df = df_analysis

            elif filter_choice == "All Models Incorrect":
                # Show rows where all models are incorrect
                all_incorrect = True
                for model_prefix in model_cols:
                    if f'{model_prefix}_correct' in df_analysis.columns:
                        all_incorrect = all_incorrect & ~df_analysis[f'{model_prefix}_correct']
                view_df = df_analysis[all_incorrect] if isinstance(all_incorrect, pd.Series) else df_analysis

            elif filter_choice == "All Models Correct":
                # Show rows where all models are correct
                all_correct = True
                for model_prefix in model_cols:
                    if f'{model_prefix}_correct' in df_analysis.columns:
                        all_correct = all_correct & df_analysis[f'{model_prefix}_correct']
                view_df = df_analysis[all_correct] if isinstance(all_correct, pd.Series) else df_analysis

            elif filter_choice == "Model Disagreements":
                # Show rows where models disagree
                if len(model_cols) >= 2:
                    # At least one correct and at least one incorrect
                    any_correct = False
                    any_incorrect = False
                    for model_prefix in model_cols:
                        if f'{model_prefix}_correct' in df_analysis.columns:
                            any_correct = any_correct | df_analysis[f'{model_prefix}_correct']
                            any_incorrect = any_incorrect | ~df_analysis[f'{model_prefix}_correct']
                    view_df = df_analysis[any_correct & any_incorrect]
                else:
                    view_df = df_analysis
            else:
                view_df = df_analysis

            # Display filtered results
            display_cols = ["query", "ground_truth"]
            for model_prefix in model_cols:
                if f'{model_prefix}_norm' in view_df.columns:
                    display_cols.append(f'{model_prefix}_norm')
                if f'{model_prefix}_correct' in view_df.columns:
                    display_cols.append(f'{model_prefix}_correct')

            existing_cols = [col for col in display_cols if col in view_df.columns]

            st.markdown(f"**Showing {len(view_df)} of {len(df_analysis)} rows**")
            st.dataframe(view_df[existing_cols], use_container_width=True)

    # Tab 4: Raw Data
    with result_tabs[3]:
        st.subheader("Complete Results Data")

        # Display truncated dataframe to avoid MemoryError with long rationales
        display_df = df.copy()
        rationale_cols = [c for c in display_df.columns if c.endswith("_rationale") or c == "judge_rationale"]
        for col in rationale_cols:
            if col in display_df.columns:
                display_df[col] = display_df[col].astype(str).str.slice(0, 150) + "..."
        st.dataframe(display_df, use_container_width=True)

        # Download options
        st.markdown("### Download Options")
        col1, col2 = st.columns(2)

        with col1:
            csv_bytes = df.to_csv(index=False).encode("utf-8")
            st.download_button(
                "⬇️ Download as CSV",
                data=csv_bytes,
                file_name=f"{test_type}_results.csv",
                mime="text/csv",
                use_container_width=True
            )

        with col2:
            json_str = df.to_json(orient='records', indent=2)
            st.download_button(
                "⬇️ Download as JSON",
                data=json_str,
                file_name=f"{test_type}_results.json",
                mime="application/json",
                use_container_width=True
            )

def render_progress_replay(test_name: str = "classification"):
    """
    Renders an animated replay of the processing progression after async operations complete.
    Uses progress metadata collected during execution.

    Args:
        test_name: Name of the test to retrieve metadata for
    """
    if 'last_progress_metadata' not in st.session_state:
        return

    metadata = st.session_state.last_progress_metadata.get(test_name)
    if not metadata or not metadata.get('batch_timestamps'):
        return

    st.subheader("📈 Processing Progress Replay")

    # Create progress visualization
    timestamps = metadata['batch_timestamps']
    cumulative_counts = metadata['cumulative_counts']
    batch_latencies = metadata['batch_latencies']
    success_rates = metadata['success_rates']

    # Create animated progress chart
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=("Cumulative Items Processed", "Batch Performance"),
        specs=[[{"secondary_y": False}], [{"secondary_y": True}]],
        vertical_spacing=0.15
    )

    # Top chart: Cumulative progress
    fig.add_trace(
        go.Scatter(
            x=timestamps,
            y=cumulative_counts,
            mode='lines+markers',
            name='Items Processed',
            line=dict(color='#1f77b4', width=3),
            marker=dict(size=8),
            fill='tozeroy',
            hovertemplate='Time: %{x:.1f}s<br>Items: %{y}<extra></extra>'
        ),
        row=1, col=1
    )

    # Bottom chart: Batch latency (bars)
    batch_numbers = list(range(1, len(batch_latencies) + 1))
    fig.add_trace(
        go.Bar(
            x=batch_numbers,
            y=batch_latencies,
            name='Batch Latency',
            marker_color='#2ca02c',
            hovertemplate='Batch %{x}<br>Latency: %{y:.2f}s<extra></extra>'
        ),
        row=2, col=1,
        secondary_y=False
    )

    # Bottom chart: Success rate (line)
    fig.add_trace(
        go.Scatter(
            x=batch_numbers,
            y=[sr * 100 for sr in success_rates],
            mode='lines+markers',
            name='Success Rate',
            line=dict(color='#ff7f0e', width=2),
            marker=dict(size=6),
            yaxis='y2',
            hovertemplate='Batch %{x}<br>Success: %{y:.1f}%<extra></extra>'
        ),
        row=2, col=1,
        secondary_y=True
    )

    # Update axes
    fig.update_xaxes(title_text="Time (seconds)", row=1, col=1)
    fig.update_yaxes(title_text="Items Processed", row=1, col=1)

    fig.update_xaxes(title_text="Batch Number", row=2, col=1)
    fig.update_yaxes(title_text="Latency (seconds)", row=2, col=1, secondary_y=False)
    fig.update_yaxes(title_text="Success Rate (%)", row=2, col=1, secondary_y=True, range=[0, 105])

    fig.update_layout(
        height=600,
        showlegend=True,
        hovermode='x unified',
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )

    st.plotly_chart(fig, use_container_width=True, config=PLOTLY_CONFIG)

    # Summary statistics
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        total_time = timestamps[-1] if timestamps else 0
        st.metric("Total Time", f"{total_time:.1f}s")

    with col2:
        total_items = cumulative_counts[-1] if cumulative_counts else 0
        st.metric("Total Items", total_items)

    with col3:
        avg_latency = sum(batch_latencies) / len(batch_latencies) if batch_latencies else 0
        st.metric("Avg Batch Latency", f"{avg_latency:.2f}s")

    with col4:
        avg_success = sum(success_rates) / len(success_rates) * 100 if success_rates else 0
        st.metric("Avg Success Rate", f"{avg_success:.1f}%")

    # Export progress data
    progress_export = pd.DataFrame({
        'Timestamp_Seconds': timestamps,
        'Items_Processed': cumulative_counts,
        'Batch_Number': range(1, len(batch_latencies) + 1),
        'Batch_Latency_Seconds': batch_latencies,
        'Success_Rate': success_rates
    })

    csv = progress_export.to_csv(index=False).encode('utf-8')
    st.download_button(
        "⬇️ Download Progress Data",
        data=csv,
        file_name=f"{test_name}_progress.csv",
        mime="text/csv",
        use_container_width=True
    )

def render_agent_gantt_chart(test_name: str = "classification"):
    """Renders an interactive Gantt chart for agent/batch execution timeline."""
    if 'last_progress_metadata' not in st.session_state:
        st.info("No execution data yet. Run a test to see the timeline.")
        return

    metadata = st.session_state.last_progress_metadata.get(test_name)
    if not metadata or not metadata.get('batch_timestamps'):
        st.info(f"No timeline data for {test_name}. Run the test first.")
        return

    # Prepare Gantt chart data
    gantt_data = []

    # Add batches as tasks
    for i, (end_time, duration, size, success_rate) in enumerate(zip(
        metadata.get('batch_timestamps', []),
        metadata.get('batch_latencies', []),
        metadata.get('batch_sizes', []),
        metadata.get('success_rates', [])
    )):
        start_time = end_time - duration
        gantt_data.append({
            'Task': f'Batch {i+1}',
            'Start': start_time,
            'Finish': end_time,
            'Resource': 'Async Worker',
            'Description': f'{size} items, {success_rate*100:.0f}% success',
            'Progress': success_rate * 100
        })

    if not gantt_data:
        return

    # Create Gantt chart using Plotly
    df_gantt = pd.DataFrame(gantt_data)

    fig = go.Figure()

    # Color scale based on success rate
    colors = ['#EF4444' if p < 50 else '#F59E0B' if p < 80 else '#10B981'
              for p in df_gantt['Progress']]

    for idx, row in df_gantt.iterrows():
        fig.add_trace(go.Bar(
            y=[row['Task']],
            x=[row['Finish'] - row['Start']],
            base=row['Start'],
            orientation='h',
            name=row['Task'],
            marker=dict(color=colors[idx]),
            text=row['Description'],
            textposition='inside',
            hovertemplate=(
                f"<b>{row['Task']}</b><br>" +
                f"Start: {row['Start']:.1f}s<br>" +
                f"Duration: {row['Finish']-row['Start']:.2f}s<br>" +
                f"{row['Description']}<br>" +
                "<extra></extra>"
            ),
            showlegend=False
        ))

    fig.update_layout(
        title="Batch Execution Timeline (Gantt Chart)",
        xaxis_title="Time (seconds)",
        yaxis_title="Batch",
        height=400,
        showlegend=False,
        hovermode='closest',
        plot_bgcolor='#F7F7FB',
        paper_bgcolor='white'
    )

    st.plotly_chart(fig, use_container_width=True, config=PLOTLY_CONFIG)


def render_test5_gantt_chart():
    """Renders Gantt chart for Test 5 orchestrator execution with enhanced colors."""
    if 'orchestrator_events' not in st.session_state or not st.session_state['orchestrator_events']:
        st.info("No execution data yet. Run Test 5 to see the timeline.")
        return

    events = st.session_state['orchestrator_events']

    # Build timeline data from events
    agents = {}  # agent_id -> {start, end, name, type, parent, status, data}

    for event in events:
        agent_id = event['agent_id']

        if agent_id not in agents:
            agents[agent_id] = {
                "id": agent_id,
                "name": event['data'].get('name', agent_id),
                "type": event['data'].get('type', 'task'),
                "parent": event['data'].get('parent'),
                "start": event['timestamp'],
                "end": None,
                "status": "running",
                "data": event['data'].copy()
            }

        if event['type'] == "complete":
            agents[agent_id]['end'] = event['timestamp']
            agents[agent_id]['status'] = "complete"
            agents[agent_id]['data'].update(event['data'])
        elif event['type'] == "error":
            agents[agent_id]['end'] = event['timestamp']
            agents[agent_id]['status'] = "error"
            agents[agent_id]['data'].update(event['data'])

    # Enhanced color scheme for different agent types
    AGENT_COLORS = {
        'orchestrator': '#6366F1',      # Indigo
        'main_agent': '#16A34A',        # Green
        'web_researcher': '#0EA5E9',    # Sky Blue
        'code_executor': '#F59E0B',     # Amber
        'knowledge_retriever': '#8B5CF6', # Purple
        'content_generator': '#EC4899',  # Pink
        'validator': '#14B8A6',         # Teal
        'editor': '#EF4444',            # Red
        'sub_agent': '#64748B',         # Slate
        'task': '#94A3B8'               # Light Slate
    }

    # Create Gantt chart data
    gantt_data = []

    if not agents:
        st.info("No agent data available.")
        return

    max_time = max([a.get('end', a['start']) for a in agents.values()])

    # Add orchestrator (overall execution)
    gantt_data.append({
        'Task': 'Orchestrator',
        'Start': 0,
        'Finish': max_time,
        'Resource': 'Test 5',
        'Description': f"{len([a for a in agents.values() if a['type'] == 'main_agent'])} turns executed",
        'Progress': 100 if all(a['status'] == 'complete' for a in agents.values() if a['type'] == 'main_agent') else 75,
        'Status': 'complete'
    })

    # Add turns (main agents) and their sub-tasks
    for agent in sorted(agents.values(), key=lambda x: x['start']):
        if agent['type'] == 'main_agent':
            duration = (agent.get('end', max_time) - agent['start'])
            accuracy = agent['data'].get('accuracy', 0) * 100

            gantt_data.append({
                'Task': agent['name'],
                'Start': agent['start'],
                'Finish': agent.get('end', max_time),
                'Resource': 'Turn',
                'Description': f"Accuracy: {accuracy:.1f}%",
                'Progress': accuracy,
                'Status': agent['status']
            })

            # Add sub-agents (code gen, evaluation)
            sub_agents = [a for a in agents.values() if a.get('parent') == agent['id']]
            for sub in sorted(sub_agents, key=lambda x: x['start']):
                sub_duration = (sub.get('end', max_time) - sub['start'])
                gantt_data.append({
                    'Task': f"  └─ {sub['name']}",  # Indent for hierarchy
                    'Start': sub['start'],
                    'Finish': sub.get('end', max_time),
                    'Resource': 'Sub-task',
                    'Description': f"{sub_duration:.2f}s",
                    'Progress': 100 if sub['status'] == 'complete' else 50,
                    'Status': sub['status']
                })

    if not gantt_data:
        st.info("No timeline data available.")
        return

    # Render using Plotly with enhanced colors
    df_gantt = pd.DataFrame(gantt_data)

    # Determine colors based on agent type and status
    def get_agent_color(task_name, status):
        """Get color based on agent type and status."""
        task_lower = task_name.lower()

        # Determine base color from agent type
        if 'orchestrator' in task_lower:
            base_color = AGENT_COLORS['orchestrator']
        elif 'web' in task_lower or 'research' in task_lower:
            base_color = AGENT_COLORS['web_researcher']
        elif 'code' in task_lower or 'executor' in task_lower:
            base_color = AGENT_COLORS['code_executor']
        elif 'knowledge' in task_lower or 'retriev' in task_lower:
            base_color = AGENT_COLORS['knowledge_retriever']
        elif 'content' in task_lower or 'generat' in task_lower:
            base_color = AGENT_COLORS['content_generator']
        elif 'validat' in task_lower:
            base_color = AGENT_COLORS['validator']
        elif 'editor' in task_lower or 'edit' in task_lower:
            base_color = AGENT_COLORS['editor']
        elif 'turn' in task_lower or 'main' in task_lower:
            base_color = AGENT_COLORS['main_agent']
        else:
            base_color = AGENT_COLORS['task']

        # Adjust opacity based on status
        if status == 'error':
            return '#EF4444'  # Red for errors
        elif status == 'running':
            return base_color + '99'  # Semi-transparent for running
        else:
            return base_color  # Full color for complete

    colors = [get_agent_color(row['Task'], row['Status']) for _, row in df_gantt.iterrows()]

    fig = go.Figure()

    for idx, row in df_gantt.iterrows():
        fig.add_trace(go.Bar(
            y=[row['Task']],
            x=[row['Finish'] - row['Start']],
            base=row['Start'],
            orientation='h',
            name=row['Task'],
            marker=dict(
                color=colors[idx],
                line=dict(color='rgba(0,0,0,0.1)', width=1)
            ),
            text=row['Description'],
            textposition='inside',
            textfont=dict(color='white', size=10),
            hovertemplate=(
                f"<b>{row['Task']}</b><br>" +
                f"Start: {row['Start']:.1f}s<br>" +
                f"Duration: {row['Finish']-row['Start']:.2f}s<br>" +
                f"Status: {row['Status']}<br>" +
                f"{row['Description']}<br>" +
                "<extra></extra>"
            ),
            showlegend=False
        ))

    fig.update_layout(
        title="Test 5: Orchestrator Execution Timeline",
        xaxis_title="Time (seconds)",
        yaxis_title="Agent / Task",
        height=max(400, len(gantt_data) * 40),
        showlegend=False,
        hovermode='closest',
        plot_bgcolor='#F7F7FB',
        paper_bgcolor='white'
    )

    st.plotly_chart(fig, width='content', config=PLOTLY_CONFIG)

    # Summary metrics
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        total_turns = len([a for a in agents.values() if a['type'] == 'main_agent'])
        st.metric("Total Turns", total_turns)

    with col2:
        completed = len([a for a in agents.values() if a['status'] == 'complete'])
        st.metric("Completed Tasks", completed)

    with col3:
        total_time = max_time if agents else 0
        st.metric("Total Time", f"{total_time:.1f}s")

    with col4:
        best_acc = max([a['data'].get('accuracy', 0) for a in agents.values() if 'accuracy' in a['data']], default=0) * 100
        st.metric("Best Accuracy", f"{best_acc:.1f}%")


def render_universal_gantt_chart(test_name: str = "Test 1"):
    """
    Universal Gantt chart renderer for ALL tests using ExecutionTracker events.

    Args:
        test_name: Name of the test to visualize (e.g., "Test 1", "Test 2", "Classification Test")
    """
    if 'execution_events' not in st.session_state or not st.session_state['execution_events']:
        st.info(f"No execution data yet. Run {test_name} to see the timeline.")
        return

    # Filter events for this test
    all_events = st.session_state['execution_events']
    test_events = [e for e in all_events if e.get('test_name') == test_name]

    if not test_events:
        st.info(f"No execution data for {test_name}. Run the test first.")
        return

    # Build timeline data from events
    agents = {}  # agent_id -> {start, end, name, type, parent, status, data}

    for event in test_events:
        agent_id = event['agent_id']

        if agent_id not in agents:
            agents[agent_id] = {
                "id": agent_id,
                "name": event['agent_name'],
                "type": event['agent_type'],
                "parent": event.get('parent_id'),
                "start": event['timestamp'],
                "end": None,
                "status": event['status'],
                "data": event['metadata'].copy()
            }

        if event['event_type'] == "complete":
            agents[agent_id]['end'] = event['timestamp']
            agents[agent_id]['status'] = "complete"
            agents[agent_id]['data'].update(event['metadata'])
        elif event['event_type'] == "error":
            agents[agent_id]['end'] = event['timestamp']
            agents[agent_id]['status'] = "error"
            agents[agent_id]['data'].update(event['metadata'])
        elif event['event_type'] == "progress":
            agents[agent_id]['progress'] = event['progress']
            agents[agent_id]['data'].update(event['metadata'])

    if not agents:
        st.info("No agent data available.")
        return

    max_time = max([a.get('end', a['start']) for a in agents.values()])

    # Create Gantt chart data
    gantt_data = []

    # Add orchestrator (overall execution)
    orchestrator_agents = [a for a in agents.values() if a['type'] == 'orchestrator']
    if orchestrator_agents:
        orch = orchestrator_agents[0]
        gantt_data.append({
            'Task': test_name,
            'Start': 0,
            'Finish': max_time,
            'Resource': 'Orchestrator',
            'Description': f"{len([a for a in agents.values() if a['type'] in ['main_agent', 'batch']])} tasks executed",
            'Progress': 100 if all(a['status'] == 'complete' for a in agents.values()) else 75,
            'Status': 'complete' if all(a['status'] == 'complete' for a in agents.values()) else 'running'
        })

    # Add main agents and batches
    for agent in sorted(agents.values(), key=lambda x: x['start']):
        if agent['type'] in ['main_agent', 'batch']:
            duration = (agent.get('end', max_time) - agent['start'])

            # Extract metrics from metadata
            success_rate = agent['data'].get('success_rate', 0) * 100
            items_processed = agent['data'].get('items_processed', 0)

            gantt_data.append({
                'Task': agent['name'],
                'Start': agent['start'],
                'Finish': agent.get('end', max_time),
                'Resource': agent['type'].replace('_', ' ').title(),
                'Description': f"{items_processed} items, {success_rate:.0f}% success" if items_processed else f"{duration:.2f}s",
                'Progress': 100 if agent['status'] == 'complete' else 50,
                'Status': agent['status']
            })

            # Add sub-agents (code gen, evaluation, etc.)
            sub_agents = [a for a in agents.values() if a.get('parent') == agent['id']]
            for sub in sorted(sub_agents, key=lambda x: x['start']):
                sub_duration = (sub.get('end', max_time) - sub['start'])
                gantt_data.append({
                    'Task': f"  └─ {sub['name']}",  # Indent for hierarchy
                    'Start': sub['start'],
                    'Finish': sub.get('end', max_time),
                    'Resource': 'Sub-task',
                    'Description': f"{sub_duration:.2f}s",
                    'Progress': 100 if sub['status'] == 'complete' else 50,
                    'Status': sub['status']
                })

    if not gantt_data:
        st.info("No timeline data available.")
        return

    # Render using Plotly
    df_gantt = pd.DataFrame(gantt_data)

    # Color scale based on status
    color_map = {
        'complete': '#10B981',  # Green
        'running': '#F59E0B',   # Yellow
        'error': '#EF4444'      # Red
    }
    colors = [color_map.get(row['Status'], '#6B7280') for _, row in df_gantt.iterrows()]

    fig = go.Figure()

    for idx, row in df_gantt.iterrows():
        fig.add_trace(go.Bar(
            y=[row['Task']],
            x=[row['Finish'] - row['Start']],
            base=row['Start'],
            orientation='h',
            name=row['Task'],
            marker=dict(color=colors[idx]),
            text=row['Description'],
            textposition='inside',
            hovertemplate=(
                f"<b>{row['Task']}</b><br>" +
                f"Start: {row['Start']:.1f}s<br>" +
                f"Duration: {row['Finish']-row['Start']:.2f}s<br>" +
                f"{row['Description']}<br>" +
                "<extra></extra>"
            ),
            showlegend=False
        ))

    fig.update_layout(
        title=f"{test_name}: Execution Timeline",
        xaxis_title="Time (seconds)",
        yaxis_title="Agent / Task",
        height=max(400, len(gantt_data) * 40),
        showlegend=False,
        hovermode='closest',
        plot_bgcolor='#F7F7FB',
        paper_bgcolor='white'
    )

    st.plotly_chart(fig, width='content', config=PLOTLY_CONFIG)

    # Summary metrics
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        total_tasks = len([a for a in agents.values() if a['type'] in ['main_agent', 'batch']])
        st.metric("Total Tasks", total_tasks)

    with col2:
        completed = len([a for a in agents.values() if a['status'] == 'complete'])
        st.metric("Completed", completed)

    with col3:
        total_time = max_time if agents else 0
        st.metric("Total Time", f"{total_time:.1f}s")

    with col4:
        avg_success = sum([a['data'].get('success_rate', 0) for a in agents.values() if 'success_rate' in a['data']]) / max(len([a for a in agents.values() if 'success_rate' in a['data']]), 1) * 100
        st.metric("Avg Success", f"{avg_success:.0f}%")


def generate_gantt_data(test_name: str, tracker: ExecutionTracker) -> pd.DataFrame:
    """Generate Gantt chart data from events."""

    events = tracker.get_test_events(test_name)

    if not events:
        return pd.DataFrame()

    # Group events by agent_id
    agent_timeline = {}

    for event in events:
        if event.agent_id not in agent_timeline:
            agent_timeline[event.agent_id] = {
                'name': event.agent_name,
                'type': event.agent_type,
                'parent': event.parent_id,
                'start': event.timestamp,
                'end': event.timestamp,
                'status': event.status,
                'progress': event.progress,
                'metadata': event.metadata.copy()
            }
        else:
            # Update end time and status
            agent_timeline[event.agent_id]['end'] = max(
                agent_timeline[event.agent_id]['end'],
                event.timestamp
            )
            agent_timeline[event.agent_id]['status'] = event.status
            agent_timeline[event.agent_id]['progress'] = max(
                agent_timeline[event.agent_id]['progress'],
                event.progress
            )
            # Merge metadata
            agent_timeline[event.agent_id]['metadata'].update(event.metadata)

    # Build DataFrame
    rows = []
    for agent_id, data in agent_timeline.items():
        duration = data['end'] - data['start']

        # Indent based on hierarchy
        indent = "  " if data['parent'] else ""

        rows.append({
            'Task': f"{indent}{data['name']}",
            'Start': data['start'],
            'Duration': duration,
            'Status': data['status'],
            'Progress': data['progress'],
            'Type': data['type'],
            'Metadata': data['metadata'],
            'Code': data['metadata'].get('code', '')  # For hover display
        })

    return pd.DataFrame(rows)


def render_task_cards(test_name: str, tracker: ExecutionTracker):
    """Render task cards (similar to HTML prototype) with hover details."""

    st.subheader("Task Cards View")

    events = tracker.get_test_events(test_name)

    if not events:
        st.info("No tasks to display.")
        return

    # Group events into tasks (main agents)
    tasks = {}
    for event in events:
        if event.agent_type in ['orchestrator', 'main_agent', 'batch']:
            if event.agent_id not in tasks:
                tasks[event.agent_id] = {
                    'name': event.agent_name,
                    'type': event.agent_type,
                    'status': event.status,
                    'progress': event.progress,
                    'start_time': event.timestamp,
                    'end_time': event.timestamp,
                    'metadata': event.metadata.copy(),
                    'sub_agents': []
                }
            else:
                tasks[event.agent_id]['end_time'] = max(
                    tasks[event.agent_id]['end_time'],
                    event.timestamp
                )
                tasks[event.agent_id]['status'] = event.status
                tasks[event.agent_id]['progress'] = max(
                    tasks[event.agent_id]['progress'],
                    event.progress
                )
                tasks[event.agent_id]['metadata'].update(event.metadata)
        elif event.agent_type in ['sub_agent']:
            # Find parent task
            parent_id = event.parent_id
            if parent_id and parent_id in tasks:
                tasks[parent_id]['sub_agents'].append({
                    'name': event.agent_name,
                    'status': event.status,
                    'progress': event.progress,
                    'metadata': event.metadata.copy()
                })

    if not tasks:
        st.info("No main tasks to display.")
        return

    # Render cards in grid
    cols = st.columns(3)

    for idx, (task_id, task) in enumerate(tasks.items()):
        with cols[idx % 3]:
            render_single_task_card(task_id, task, tracker)


def render_single_task_card(task_id: str, task: Dict, tracker: ExecutionTracker):
    """Render a single task card with hover details."""

    # Status icon
    status_icon = {
        'complete': '✅',
        'running': '⏳',
        'error': '❌',
        'pending': '⏸️'
    }.get(task['status'], '⏸️')

    duration = task['end_time'] - task['start_time']

    # Card container
    with st.container():
        st.markdown(f"""
        <div style="
            background: white;
            border: 1px solid #E5E7EB;
            border-radius: 12px;
            padding: 16px;
            margin-bottom: 16px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.05);
        ">
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 12px;">
                <strong>{task['name']}</strong>
                <span style="font-size: 20px;">{status_icon}</span>
            </div>
            <div style="margin-bottom: 8px;">
                <small style="color: #6B7280;">Duration: {duration:.1f}s • Progress: {task['progress']:.0f}%</small>
            </div>
        </div>
        """, unsafe_allow_html=True)

        # Expander for details
        with st.expander("View Details"):
            st.markdown(f"**Status:** {task['status']}")
            st.markdown(f"**Type:** {task['type']}")
            st.markdown(f"**Sub-agents:** {len(task['sub_agents'])}")

            if task['sub_agents']:
                st.markdown("**Sub-agent Status:**")
                for sub in task['sub_agents']:
                    sub_status_icon = {
                        'complete': '✅',
                        'running': '⏳',
                        'error': '❌',
                        'pending': '⏸️'
                    }.get(sub['status'], '⏸️')
                    st.markdown(f"- {sub_status_icon} {sub['name']} ({sub['progress']:.0f}%)")

            # Show code if available
            if 'code' in task['metadata'] and task['metadata']['code']:
                st.markdown("**Generated Code:**")
                code_preview = task['metadata']['code'][:500]
                if len(task['metadata']['code']) > 500:
                    code_preview += "\n... (truncated)"
                st.code(code_preview, language='python')

            # Show other metadata
            if task['metadata']:
                st.markdown("**Metadata:**")
                filtered_metadata = {k: v for k, v in task['metadata'].items() if k != 'code'}
                if filtered_metadata:
                    st.json(filtered_metadata)


def render_live_agent_status(test_name: str = "classification"):
    """Renders live status cards for running agents/batches."""
    if 'active_batches' not in st.session_state:
        st.session_state.active_batches = {}

    active = st.session_state.active_batches.get(test_name, {})

    if not active:
        st.info("No active batches. Start a test to see live status.")
        return

    cols = st.columns(3)

    with cols[0]:
        st.metric(
            "Active Batches",
            active.get('count', 0),
            delta=f"+{active.get('recent_completed', 0)} completed"
        )

    with cols[1]:
        st.metric(
            "Items Processed",
            active.get('items_processed', 0),
            delta=f"{active.get('throughput', 0):.1f} items/sec"
        )

    with cols[2]:
        progress = active.get('overall_progress', 0)
        st.metric(
            "Overall Progress",
            f"{progress:.0f}%",
            delta=f"{active.get('eta_seconds', 0):.0f}s remaining"
        )


def render_agent_task_cards():
    """Renders task cards showing status of different test runs."""
    st.subheader("Test Run History")

    # Get test history from session state
    if 'test_history' not in st.session_state:
        st.session_state.test_history = []

    history = st.session_state.test_history[-6:]  # Show last 6 runs

    if not history:
        st.info("No test runs yet. Execute tests to see history here.")
        return

    # Create grid of cards
    cols = st.columns(3)

    for idx, run in enumerate(history):
        with cols[idx % 3]:
            status_color = {
                'complete': '🟢',
                'running': '🟡',
                'error': '🔴',
                'pending': '⚪'
            }.get(run.get('status', 'pending'), '⚪')

            st.markdown(f"""
            <div style="
                background: white;
                border: 1px solid #E5E7EB;
                border-radius: 12px;
                padding: 16px;
                margin-bottom: 12px;
                box-shadow: 0 2px 8px rgba(0,0,0,0.05);
            ">
                <div style="display: flex; align-items: center; gap: 8px; margin-bottom: 8px;">
                    <span style="font-size: 24px;">{run.get('icon', '📊')}</span>
                    <strong>{run.get('name', 'Test')}</strong>
                    <span style="margin-left: auto;">{status_color}</span>
                </div>
                <div style="font-size: 12px; color: #6B7280;">
                    {run.get('description', 'No description')}
                </div>
                <div style="margin-top: 12px; padding-top: 12px; border-top: 1px solid #E5E7EB;">
                    <div style="display: flex; justify-content: space-between; font-size: 11px; color: #9CA3AF;">
                        <span>⏱ {run.get('duration', 0):.1f}s</span>
                        <span>🤖 {run.get('batch_count', 0)} batches</span>
                        <span>💰 ${run.get('cost', 0):.4f}</span>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)


def capture_run_config(test_name: str, overrides: Optional[Dict[str, Any]] = None):
    """Snapshots the current run's configuration into session state.
    If overrides are provided (per-test settings), they take precedence.
    """
    if "last_run_configs" not in st.session_state:
        st.session_state.last_run_configs = {}

    config: Dict[str, Any] = {}
    allowed_labels = _allowed_labels(st.session_state.df)

    # Pull effective settings from overrides if provided
    use_openai_eff = overrides.get('use_openai') if overrides else use_openai
    use_ollama_eff = overrides.get('use_ollama') if overrides else use_ollama
    use_ollama_local_eff = overrides.get('use_ollama_local') if overrides else use_ollama_local
    openai_model_eff = overrides.get('openai_model') if overrides else OPENAI_MODEL
    openrouter_model_eff = overrides.get('openrouter_model') if overrides else OPENROUTER_MODEL
    ollama_model_eff = overrides.get('ollama_model') if overrides else OLLAMA_MODEL
    third_kind_eff = overrides.get('third_kind') if overrides else THIRD_KIND
    third_model_eff = overrides.get('third_model') if overrides else THIRD_MODEL

    # --- Classification Models (Tests 1-3) ---
    if test_name in ["Test 1", "Test 2", "Test 3"]:
        if use_openai_eff:
            config['OpenAI'] = {
                "model": openai_model_eff,
                "system_prompt": "Return a structured classification with optional confidence 0..1.",
                "user_prompt_template": f"Allowed labels: {allowed_labels if allowed_labels else '[unconstrained]'}.\nPick exactly ONE of these values in 'classification_result'.\nText: {{text}}\nRespond as JSON with keys: classification_result, rationale, confidence (0..1 optional)."
            }
        if use_ollama_eff:  # OpenRouter path
            config['OpenRouter'] = {
                "model": openrouter_model_eff,
                "provider": "OpenRouter",
                "system_prompt": f"Return ONLY compact JSON with keys: classification_result, rationale, confidence (0..1 optional). Allowed labels: {allowed_labels}.",
                "user_prompt_template": "{text}"
            }
        if use_ollama_local_eff:
            config['Ollama (Local)'] = {
                "model": ollama_model_eff,
                "provider": "Local Ollama",
                "system_prompt": "You only reply with JSON that matches the provided schema.",
                "user_prompt_template": f"Allowed labels: {allowed_labels if allowed_labels else '[unconstrained]'}\nText: {{text}}"
            }
        if test_name in ["Test 2", "Test 3"] and third_kind_eff != "None":
            config[f'Third Model ({third_kind_eff})'] = {
                "model": third_model_eff,
                "provider": third_kind_eff,
                "system_prompt": "Respond ONLY with JSON having keys classification_result, rationale, confidence (0..1 optional).",
                "user_prompt_template": "{text}"
            }

    # --- Judge Model (Test 3) ---
    if test_name == "Test 3":
        judge_model = st.session_state.get('judge_model', 'openai/gpt-5-mini')
        config['Judge'] = {
            "model": judge_model,
            "provider": _get_provider_from_model_id(judge_model),
            "instructions": JUDGE_INSTRUCTIONS
        }

    # --- Pruner Model (Test 4) ---
    if test_name == "Test 4":
        pruner_model = st.session_state.get('pruner_model', 'openai/gpt-5-mini')
        config['Pruner'] = {
            "model": pruner_model,
            "provider": _get_provider_from_model_id(pruner_model),
            "instructions": PRUNER_INSTRUCTIONS
        }

    st.session_state.last_run_configs[test_name] = config

def display_run_config(test_name: str):
    """Displays the captured configuration in a Streamlit expander."""
    if "last_run_configs" in st.session_state and test_name in st.session_state.last_run_configs:
        with st.expander("View Configuration for Last Run"):
            config_data = st.session_state.last_run_configs[test_name]
            if not config_data:
                st.write("No models were configured for this run.")
                return

            for provider, details in config_data.items():
                st.subheader(f"⚙️ {provider} Configuration")
                st.text(f"Model: {details['model']}")
                if "provider" in details:
                    st.text(f"Provider: {details['provider']}")

                if "instructions" in details:
                    st.text("Prompt / Instructions:")
                    st.code(details['instructions'], language='markdown')
                else:
                    if "system_prompt" in details:
                        st.text("System Prompt:")
                        st.code(details['system_prompt'], language='text')
                    if "user_prompt_template" in details:
                        st.text("User Prompt Template (where `{text}` is the input):")
                        st.code(details['user_prompt_template'], language='text')
                st.divider()

def _non_empty(s):
    return s.fillna("").astype(str).str.strip().replace("nan","").replace("None","").ne("").sum()

# ---------- Rigorous Reporting with Scikit-learn and LLM Explanation ----------
def generate_classification_report(y_true, y_pred, model_name, explain: bool = False):
    y_true_norm = [_normalize_label(s) for s in y_true]
    y_pred_norm = [_normalize_label(s) for s in y_pred]

    valid_indices = [i for i, (t, p) in enumerate(zip(y_true_norm, y_pred_norm)) if t and p]
    if not valid_indices:
        st.warning(f"No valid predictions for {model_name} to generate a report.")
        return None

    y_true_filtered = [y_true_norm[i] for i in valid_indices]
    y_pred_filtered = [y_pred_norm[i] for i in valid_indices]

    report_dict = classification_report(y_true_filtered, y_pred_filtered, output_dict=True, zero_division=0)

    st.subheader(f"Classification Report: {model_name}")
    st.dataframe(pd.DataFrame(report_dict).transpose().style.format("{:.2f}"))

    st.subheader(f"Confusion Matrix: {model_name}")
    labels = sorted(list(set(y_true_filtered) | set(y_pred_filtered)))
    cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=labels)

    # Interactive Plotly heatmap
    fig_cm = go.Figure(data=go.Heatmap(
        z=cm,
        x=labels,
        y=labels,
        colorscale='Blues',
        text=cm,
        texttemplate='%{text}',
        textfont={"size": 12},
        hovertemplate='True: %{y}<br>Predicted: %{x}<br>Count: %{z}<extra></extra>',
        colorbar=dict(title="Count")
    ))

    fig_cm.update_layout(
        title=f"Confusion Matrix: {model_name}",
        xaxis_title="Predicted",
        yaxis_title="True",
        height=max(400, len(labels) * 50),
        width=max(500, len(labels) * 50),
        xaxis={'side': 'bottom'},
        yaxis={'autorange': 'reversed'}
    )

    st.plotly_chart(fig_cm, use_container_width=True, config=PLOTLY_CONFIG)

    # --- NEW: Per-Class Performance Radar ---
    st.subheader(f"Per-Class Performance Radar: {model_name}")

    # Extract per-class metrics
    class_metrics = []
    for label in labels:
        if label in report_dict:
            class_metrics.append({
                'Class': label,
                'Precision': report_dict[label]['precision'],
                'Recall': report_dict[label]['recall'],
                'F1-Score': report_dict[label]['f1-score']
            })

    if class_metrics:
        fig_radar = go.Figure()

        for metric in ['Precision', 'Recall', 'F1-Score']:
            fig_radar.add_trace(go.Scatterpolar(
                r=[m[metric] for m in class_metrics],
                theta=[m['Class'] for m in class_metrics],
                fill='toself',
                name=metric
            ))

        fig_radar.update_layout(
            polar=dict(
                radialaxis=dict(visible=True, range=[0, 1])
            ),
            showlegend=True,
            height=400
        )

        st.plotly_chart(fig_radar, use_container_width=True, config=PLOTLY_CONFIG)

    # --- NEW: LLM Explanation ---
    if explain:
        with st.spinner(f"Asking an LLM to explain the '{model_name}' confusion matrix..."):
            try:
                cm_df = pd.DataFrame(cm, index=labels, columns=labels)
                cm_string = cm_df.to_string()
                prompt = f"""
As a data science expert, analyze this confusion matrix for the '{model_name}' model.

**Confusion Matrix:**
```
{cm_string}
```

**Instructions:**
1.  **Overall Performance:** Briefly state how well the model is performing in general.
2.  **Strengths:** Identify which classes the model predicts most accurately (high values on the diagonal).
3.  **Weaknesses/Confusions:** Point out the most significant misclassifications (large off-diagonal values). For each, clearly state 'The model confused class A (True) with class B (Predicted) X times.'
4.  **Conclusion:** Provide a concise summary and a potential next step for improving the model based on these confusions.

Keep the explanation clear, concise, and easy to understand. Use markdown for formatting.
"""
                explanation = asyncio.run(generate_text_async(prompt))
                st.markdown(explanation)
            except Exception as e:
                st.warning(f"Could not generate LLM explanation for the confusion matrix: {e}")

    return report_dict

# --- PATCH 3: Gemini Code Execution Helper ---

async def run_gemini_code_execution(
    system_prompt: str,
    user_contents: List[Any],
    model: str = "gemini-2.5-flash",
    max_turns: int = 5
) -> Tuple[List[types.Part], str, float]:
    """
    Runs a conversation with Gemini 2.5 Flash, enabling code execution tool for self-refinement.
    Returns (Final Parts, Best Code, Best Performance).
    """

    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY not set.")

    # CRITICAL FIX: Create client without async context manager
    # genai.Client doesn't support async context managers
    client = genai.Client(api_key=GEMINI_API_KEY)

    # Configure tool usage: Code Execution ONLY
    config = types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    )

    # Initial message structure for the *first* call
    initial_content_parts = [types.Part.from_text(text=system_prompt)] + [types.Part.from_text(text=c) if isinstance(c, str) else c for c in user_contents]

    history = [types.Content(parts=initial_content_parts)] # History starts with the prompt

    best_performance = -1.0
    best_attempt_code = ""

    # Synchronous worker function to call the Gemini API
    def sync_api_call(current_history):
        return client.models.generate_content(
            model=model,
            contents=current_history,
            config=config,
        )

    # Run loop
    for turn in range(max_turns):
        st.info(f"Agent Turn {turn+1}/{max_turns}: Sending context to Gemini...")

        try:
            # Execute the synchronous API call in a separate thread
            response = await asyncio.to_thread(sync_api_call, history)

            # Track the API call
            st.session_state.cost_tracker.update(
                provider="Google", model=model, api="generate_content",
                raw_response_obj=response, pricing_resolver=combined_price_lookup
            )

            # Add model response (parts) to history
            if response.candidates and response.candidates[0].content:
                model_content = response.candidates[0].content
                history.append(model_content)

                # Analyze parts from the current turn
                current_code = ""
                current_accuracy = 0.0

                st.markdown(f"**Gemini Reasoning (Turn {turn+1}):**")

                for part in model_content.parts:
                    if part.text:
                        st.text(part.text)
                    if part.executable_code:
                        current_code = part.executable_code.code
                        st.code(current_code, language='python')
                    if part.code_execution_result and part.code_execution_result.output:
                        code_output = part.code_execution_result.output
                        st.text(f"Code Execution Output:\n{code_output}")

                        # Extract accuracy from output
                        match = re.search(r"Accuracy: (\d+\.\d+)", code_output)
                        if match:
                            current_accuracy = float(match.group(1))
                            st.success(f"Performance Score: {current_accuracy:.4f}")

                # Update best performance
                if current_accuracy > best_performance:
                    best_performance = current_accuracy
                    best_attempt_code = current_code
                    st.balloons()
                    st.markdown(f"**NEW MAX PERFORMANCE ({best_performance:.4f}) CAPTURED**")

                # Prepare for next turn: Only send a small prompt to continue the conversation
                if turn < max_turns - 1:
                    history.append(types.Content(parts=[types.Part.from_text(text="Based on the code execution result (if any), please refine the AgentFramework code and re-test to maximize accuracy. Provide the full executable Python code.")]))

                # Check for termination condition
                if response.candidates[0].finish_reason == types.FinishReason.STOP and current_accuracy > 0:
                    st.info("Agent stopped self-refinement early.")
                    break

        except Exception as e:
            st.error(f"Gemini Code Execution failed at turn {turn+1}: {e}")
            break

    # Return the full history and the best attempt found
    final_parts = history[-1].parts if history else []
    return final_parts, best_attempt_code, best_performance

# --- ORCHESTRATOR INFRASTRUCTURE CLASSES ---

class Budget:
    """Budget manager with dual modes: turn-based or cost-based tracking"""
    def __init__(self,
                 mode: str = "turns",  # "turns" or "cost"
                 max_turns: int = 10,
                 max_cost_usd: float = 5.0,
                 max_tokens: int = 1_000_000):
        self.mode = mode
        self.max_turns = max_turns
        self.max_cost = max_cost_usd
        self.max_tokens = max_tokens

        self.current_turn = 0
        self.spent_cost = 0.0
        self.spent_tokens = 0

    def left(self) -> float:
        """Returns remaining budget as fraction (0.0 to 1.0)"""
        if self.mode == "turns":
            return (self.max_turns - self.current_turn) / self.max_turns if self.max_turns > 0 else 0.0
        else:  # "cost" mode
            cost_left = (self.max_cost - self.spent_cost) / self.max_cost if self.max_cost > 0 else 0.0
            token_left = (self.max_tokens - self.spent_tokens) / self.max_tokens if self.max_tokens > 0 else 0.0
            return min(cost_left, token_left)

    def advance_turn(self):
        """Increment turn counter"""
        self.current_turn += 1

    def consume(self, cost: float, tokens: int):
        """Track actual spending (useful for reporting in both modes)"""
        self.spent_cost += cost
        self.spent_tokens += tokens

    def exhausted(self) -> bool:
        """Check if budget is exhausted"""
        if self.mode == "turns":
            return self.current_turn >= self.max_turns
        else:
            return (self.spent_cost >= self.max_cost or
                    self.spent_tokens >= self.max_tokens)

@dataclass
class TurnMetrics:
    """Metrics tracked per turn for progress visualization"""
    turn: int
    tasks_attempted: int
    tasks_verified: int
    best_accuracy: float
    improvement: float  # delta from previous turn
    cost_spent: float
    tokens_used: int
    timestamp: float

@dataclass
class OrchestratorResult:
    """Unified result format for all orchestrator modes and patterns"""
    mode: str  # "inference", "analysis", "research"
    coordination_pattern: str  # "solo", "subagent", "multi_agent"
    final_score: float  # Accuracy, success rate, or quality score
    total_turns: int
    best_turn: int
    total_cost: float
    total_tokens: int
    solution: Any  # Code, consensus, or synthesis
    history: List[TurnMetrics]
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class Task:
    """Task representation for orchestrator"""
    id: str
    goal: str
    parent_ids: Set[str] = field(default_factory=set)
    inputs: Dict[str, Any] = field(default_factory=dict)
    priority: float = 0.0
    novelty_score: float = 0.0
    estimated_roi: float = 0.0
    code: Optional[str] = None

    def signature(self) -> str:
        """Hash for deduplication"""
        content = f"{self.goal}|{self.code}|{sorted(self.inputs.items())}"
        return hashlib.sha256(content.encode()).hexdigest()

@dataclass
class VerificationResult:
    """Verification result with claims and evidence"""
    task_id: str
    claims: List[str]
    evidence: Dict[str, Any]
    verdict: str  # "verified", "failed", "partial"
    confidence: float
    outputs: Any

class TaskCache:
    """Cache for task results with deduplication"""
    def __init__(self):
        self.cache: Dict[str, VerificationResult] = {}
        self.verified_tasks: Set[str] = set()

    def has(self, signature: str) -> bool:
        return signature in self.cache

    def get(self, signature: str) -> Optional[VerificationResult]:
        return self.cache.get(signature)

    def store(self, signature: str, result: VerificationResult):
        self.cache[signature] = result
        if result.verdict == "verified":
            self.verified_tasks.add(result.task_id)

class KnowledgeIndex:
    """Knowledge index for storing verified outputs"""
    def __init__(self):
        self.entries: List[Dict[str, Any]] = []
        self.embeddings = None  # Would use actual embeddings in production

    async def store(self, outputs: Any, verified: VerificationResult):
        """Flatten, embed, and tag verified outputs"""
        entry = {
            "task_id": verified.task_id,
            "outputs": outputs,
            "verdict": verified.verdict,
            "confidence": verified.confidence,
            "claims": verified.claims,
            "timestamp": time.time()
        }
        self.entries.append(entry)

    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """Search for relevant verified knowledge"""
        # Simplified: would use actual semantic search
        return self.entries[-top_k:]

class FineTuneDatasetCollector:
    """Collects examples for fine-tuning datasets"""
    def __init__(self):
        self.examples: List[Dict[str, Any]] = []

    def add_example(self, input_data: Any, output_data: Any, metadata: Optional[Dict] = None):
        """Add a training example"""
        self.examples.append({
            "input": input_data,
            "output": output_data,
            "metadata": metadata or {}
        })

    def export(self) -> List[Dict]:
        """Export collected examples"""
        return self.examples

# ============================================
# Agent Coordination Patterns
# ============================================

class AgentCoordinationPattern(Enum):
    """
    Coordination patterns define how agents work together.

    SOLO: Single agent executes the task independently
    SUBAGENT: Hierarchical delegation with specialized subagents (decomposer, generator, evaluator, etc.)
    MULTI_AGENT: Peer collaboration with independent proposals, cross-review, and consensus
    LEAF_SCAFFOLD: Hierarchical multi-agent scaffold with supervisor and specialized leaf agents
    """
    SOLO = "solo"
    SUBAGENT = "subagent"
    MULTI_AGENT = "multi_agent"
    LEAF_SCAFFOLD = "leaf_scaffold"


# ============================================
# Leaf Agent Scaffold Integration
# ============================================

class GeminiLLMClient:
    """Wrapper for Gemini API to use with leaf agents."""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = genai.Client(api_key=api_key)

    async def generate(self, prompt: str) -> str:
        """Generate text response."""
        response = await asyncio.to_thread(
            lambda: self.client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt
            )
        )

        # Track cost
        if st.session_state.cost_tracker:
            st.session_state.cost_tracker.update(
                provider="Google",
                model="gemini-2.5-flash",
                api="generate_content",
                raw_response_obj=response,
                pricing_resolver=custom_gemini_price_lookup
            )

        return response.text

    async def generate_with_code_exec(self, prompt: str) -> str:
        """Generate with code execution."""
        try:
            response = await asyncio.to_thread(
                lambda: self.client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        tools=[types.Tool(code_execution={})]
                    )
                )
            )

            # Track cost
            if st.session_state.cost_tracker:
                st.session_state.cost_tracker.update(
                    provider="Google",
                    model="gemini-2.5-flash",
                    api="generate_content",
                    raw_response_obj=response,
                    pricing_resolver=custom_gemini_price_lookup
                )

            # Extract code execution results
            output_parts = []
            if response and hasattr(response, 'candidates') and response.candidates:
                for part in response.candidates[0].content.parts:
                    if hasattr(part, 'text') and part.text:
                        output_parts.append(part.text)
                    elif hasattr(part, 'code_execution_result'):
                        if hasattr(part.code_execution_result, 'output') and part.code_execution_result.output:
                            output_parts.append(part.code_execution_result.output)

            result = "\n".join(output_parts) if output_parts else "Code execution completed but no output was generated."
            return result

        except Exception as e:
            # Return error message instead of raising to allow graceful handling
            return f"Code execution failed: {str(e)}"


class GeminiTaskPlanner(TaskPlanner):
    """Task planner using Gemini API."""

    def __init__(self, available_agents: List[AgentType], llm_client):
        super().__init__(available_agents)
        self.llm_client = llm_client

    async def _call_llm_for_planning(self, prompt: str) -> str:
        """Call Gemini for task planning."""
        response = await asyncio.to_thread(
            lambda: self.llm_client.client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
        )

        # Track cost
        if st.session_state.cost_tracker:
            st.session_state.cost_tracker.update(
                provider="Google",
                model="gemini-2.5-flash",
                api="generate_content",
                raw_response_obj=response,
                pricing_resolver=custom_gemini_price_lookup
            )

        return response.text


class GeminiResultSynthesizer(ResultSynthesizer):
    """Result synthesizer using Gemini API."""

    def __init__(self, llm_client):
        self.llm_client = llm_client

    async def _call_llm_for_synthesis(self, prompt: str) -> str:
        """Call Gemini for result synthesis."""
        return await self.llm_client.generate(prompt)


class UnifiedOrchestrator:
    """
    Three modes for different problem types:
    1. Direct Inference: Pattern matching (classification, prediction)
    2. Computational Analysis: Statistics, simulations, optimization
    3. Research Tasks: Multi-source information gathering with decomposition

    Three coordination patterns:
    1. Solo: Single agent executes independently
    2. Subagent: Hierarchical delegation with specialized subagents
    3. Multi-Agent: Peer collaboration with proposals, review, and consensus

    This creates a 3×3 matrix of agent architectures (mode × pattern).
    """
    def __init__(
        self,
        goal: str,
        test_data: Optional[List[Dict]] = None,
        budget: Budget = None,
        mode: Optional[str] = None,
        coordination_pattern: Optional[str] = None,
        peer_agent_roles: Optional[List[str]] = None
    ):
        self.goal = goal
        self.test_data = test_data or []
        self.budget = budget or Budget(mode="turns", max_turns=5)

        # Auto-detect mode if not specified
        if mode is None:
            self.mode = self._detect_mode(goal)
        else:
            self.mode = mode  # "inference", "analysis", or "research"

        # Auto-detect coordination pattern if not specified
        if coordination_pattern is None:
            self.coordination_pattern = self._detect_coordination_pattern(goal, self.mode)
        else:
            self.coordination_pattern = coordination_pattern  # "solo", "subagent", or "multi_agent"

        # Multi-agent configuration
        self.peer_agent_roles = peer_agent_roles or self._get_default_peer_roles(self.mode)

        # Mode-specific components
        self.cache = TaskCache()
        self.index = KnowledgeIndex()
        self.policies: Dict[str, float] = defaultdict(float)  # learnable weights
        self.dataset_collector = FineTuneDatasetCollector()
        self.research_results = []

        # Turn-based tracking
        self.turn_history: List[TurnMetrics] = []
        self.best_performance = 0.0
        self.best_code = None
        self.best_turn = 0

        # CRITICAL: Initialize iteration counter
        self.iteration = 0

        # Track per-item failures for inference mode
        self.failure_analysis: List[Dict] = []

        # USE UNIFIED EXECUTION TRACKER (not custom event system)
        self.tracker = st.session_state.get('execution_tracker')
        if not self.tracker:
            self.tracker = ExecutionTracker()
            st.session_state['execution_tracker'] = self.tracker

        # Initialize persistent dashboard logger for Test 5
        self.dashboard_logger = None
        if 'test5_dashboard_logger' in st.session_state:
            self.dashboard_logger = st.session_state['test5_dashboard_logger']

        # Initialize stateful components (memory, security, self-correction)
        self.memory_manager = None
        self.security_agent = None
        self.self_correction_manager = None

        # These will be initialized when run() is called
        self._stateful_components_initialized = False

    def _detect_mode(self, goal: str) -> str:
        """Auto-detect which mode to use based on goal."""
        goal_lower = goal.lower()

        # Research indicators
        research_keywords = ['search', 'research', 'find information', 'tell me about',
                            'what is', 'who is', 'compare', 'analyze market']
        if any(kw in goal_lower for kw in research_keywords):
            return "research"

        # Analysis indicators
        analysis_keywords = ['analyze', 'compute', 'calculate', 'simulate',
                           'optimize', 'compare performance']
        if any(kw in goal_lower for kw in analysis_keywords):
            return "analysis"

        # Default to inference if test data provided
        if self.test_data:
            return "inference"

        return "research"  # Default fallback

    def _detect_coordination_pattern(self, goal: str, mode: str) -> str:
        """
        Auto-detect coordination pattern based on task complexity.

        Heuristics:
        - Solo: Simple, single-step tasks
        - Subagent: Complex tasks requiring decomposition and specialization
        - Multi-Agent: Tasks requiring diverse perspectives or consensus
        """
        goal_lower = goal.lower()

        # Multi-agent indicators (diverse perspectives needed)
        multi_agent_keywords = [
            'consensus', 'multiple perspectives', 'diverse', 'compare approaches',
            'peer review', 'collaborative', 'team', 'different angles',
            'cross-validate', 'reconcile'
        ]
        if any(kw in goal_lower for kw in multi_agent_keywords):
            return AgentCoordinationPattern.MULTI_AGENT.value

        # Subagent indicators (complex workflow requiring specialization)
        subagent_keywords = [
            'pipeline', 'workflow', 'multi-step', 'decompose', 'delegate',
            'specialized', 'hierarchical', 'orchestrate', 'coordinate',
            'iterative refinement', 'evaluate and refine'
        ]
        if any(kw in goal_lower for kw in subagent_keywords):
            return AgentCoordinationPattern.SUBAGENT.value

        # Complexity heuristics
        word_count = len(goal.split())
        has_multiple_objectives = goal.count(',') >= 2 or goal.count('and') >= 2

        # Complex tasks → Subagent
        if word_count > 30 or has_multiple_objectives:
            return AgentCoordinationPattern.SUBAGENT.value

        # Default to solo for simple tasks
        return AgentCoordinationPattern.SOLO.value

    def _get_default_peer_roles(self, mode: str) -> List[str]:
        """Get default peer agent roles based on mode."""
        if mode == "inference":
            return ["Pattern Analyst", "Rule Designer", "Evaluator"]
        elif mode == "analysis":
            return ["Data Scientist", "Algorithm Designer", "Code Reviewer"]
        elif mode == "research":
            return ["Market Analyst", "Technical Expert", "Domain Specialist"]
        else:
            return ["Agent 1", "Agent 2", "Agent 3"]

    async def decompose(self, goal: str, context: Dict) -> List[Task]:
        """Break goal into prioritized subtasks with proper error handling.

        Mode-specific behavior:
        - inference: Single refinement task analyzing failures
        - analysis: Single code generation task
        - research: Multiple parallel research tasks
        """

        # Track iteration properly
        iteration = len(self.turn_history) + 1

        if self.mode == "inference":
            # ITERATIVE REFINEMENT MODE: Single task that analyzes failures and improves code
            if iteration == 1:
                # First turn: generate initial solution
                prompt = f"""
Generate a complete Python script that predicts tool sequences.

TEST DATA (first 5 examples):
{json.dumps(self.test_data[:5], indent=2)}

Requirements:
- Complete predict_tools(query) function
- Test harness that creates 'predictions' list
- Print "Accuracy: X.XXXX" to stdout

Return ONLY executable Python code.
"""
            else:
                # Subsequent turns: analyze failures and refine
                failures_count = int((1 - self.best_performance) * len(self.test_data))

                prompt = f"""
CURRENT CODE (accuracy {self.best_performance:.3f}):
```python
{self.best_code}
```

This code fails on {failures_count} of {len(self.test_data)} test cases.

SAMPLE TEST DATA (showing patterns to learn):
{json.dumps(self.test_data[:5], indent=2)}

YOUR TASK:
1. Analyze why the current code fails on {failures_count} cases
2. Identify patterns in the data (keywords? tool combinations? edge cases?)
3. Modify the EXISTING code to fix these issues
4. Return the COMPLETE improved code

Focus on fixing actual errors, not rewriting from scratch.
Return ONLY executable Python code.
"""

            # Single task for iterative refinement
            task = Task(
                id=f"refine_iteration_{iteration}",
                goal="Analyze failures and improve the predictor code",
                novelty_score=0.7,
                estimated_roi=0.9
            )

            # Skip LLM call for task generation in iterative mode
            st.info(f"✅ Iterative mode: Turn {iteration} refinement task")
            return [task]

        else:
            # RESEARCH MODE: Multiple parallel tasks (original behavior)
            prompt = f"""
You are designing code to predict tool sequences from queries.

CURRENT STATE:
- Iteration: {iteration}
- Best Accuracy: {self.best_performance:.3f}

SAMPLE DATA:
{json.dumps(self.test_data[:3], indent=2)}

Generate 3 CONCRETE coding tasks. Each must be implementable as a single Python function.

GOOD TASK: "Write a keyword matching function that maps 'check status' → ['check_device']"
BAD TASK: "Design a tool registry architecture"

GOOD TASK: "Add regex pattern matching for IP addresses to trigger 'get_ip' tool"
BAD TASK: "Implement deterministic parsing logic"

GOOD TASK: "Create a dictionary mapping common verbs (check, get, set) to tool names"
BAD TASK: "Improve the prompt template"

Return JSON array with SPECIFIC, CODE-FOCUSED tasks:
[
  {{
    "id": "task_1",
    "goal": "Write keyword_to_tools() dict mapping common words to tool names",
    "depends_on": [],
    "novelty": 0.8,
    "roi": 0.9
  }},
  {{
    "id": "task_2",
    "goal": "Add if/elif logic to check for 'status' keyword and return ['check_device']",
    "depends_on": [],
    "novelty": 0.7,
    "roi": 0.8
  }},
  {{
    "id": "task_3",
    "goal": "Implement query.lower().split() to extract words for matching",
    "depends_on": [],
    "novelty": 0.6,
    "roi": 0.7
  }}
]

Each task should describe EXACTLY what code to write, not abstract concepts.
"""

        try:
            # Use the selected judge/pruner model (flexible routing)
            model = st.session_state.get('judge_model', 'openai/gpt-5-mini')

            # Route based on provider
            provider = _get_provider_from_model_id(model)

            if provider == "openai" and OPENAI_API_KEY:
                client = AsyncOpenAI(api_key=OPENAI_API_KEY)
                native_model = _to_native_model_id(model)
                resp = await client.chat.completions.create(
                    model=native_model,
                    messages=[{"role": "user", "content": prompt}],
                    response_format={"type": "json_object"}
                )

                # Track cost
                st.session_state.cost_tracker.update(
                    provider="OpenAI", model=native_model,
                    api="chat.completions.create",
                    raw_response_obj=resp,
                    pricing_resolver=combined_price_lookup
                )

                content = resp.choices[0].message.content

            elif OPENROUTER_API_KEY:
                # Fallback to OpenRouter
                headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}"}
                body = {
                    "model": _to_openrouter_model_id(model),
                    "messages": [{"role": "user", "content": prompt}],
                    "response_format": {"type": "json_object"}
                }

                async with httpx.AsyncClient(timeout=60) as client_http:
                    resp = await client_http.post(
                        "https://openrouter.ai/api/v1/chat/completions",
                        headers=headers,
                        json=body
                    )
                    resp.raise_for_status()
                    data = resp.json()

                    # Track cost
                    st.session_state.cost_tracker.update(
                        provider="OpenRouter",
                        model=_to_openrouter_model_id(model),
                        api="chat.completions",
                        raw_response_json=data,
                        pricing_resolver=custom_openrouter_price_lookup
                    )

                    content = data["choices"][0]["message"]["content"]
            else:
                st.error("❌ No LLM provider available for decomposition")
                return self._get_fallback_tasks()

            # Parse response
            tasks_data = json.loads(content)

            # Handle both direct array and wrapped formats
            if isinstance(tasks_data, dict):
                tasks_data = tasks_data.get("subtasks", tasks_data.get("tasks", []))

            tasks = []
            for t in tasks_data:
                task = Task(
                    id=t["id"],
                    goal=t["goal"],
                    parent_ids=set(t.get("depends_on", [])),
                    novelty_score=float(t.get("novelty", 0.5)),
                    estimated_roi=float(t.get("roi", 0.5))
                )
                tasks.append(task)

            # CRITICAL FIX: Ensure we never return empty list
            if not tasks or len(tasks) == 0:
                st.warning("⚠️ LLM returned no tasks. Using fallback.")
                return self._get_fallback_tasks()

            st.info(f"✅ Decomposed into {len(tasks)} tasks")
            return tasks

        except Exception as e:
            st.error(f"❌ Decomposition failed: {e}")
            st.exception(e)  # Show full traceback

            # Return fallback tasks instead of empty list
            return self._get_fallback_tasks()

    def _get_fallback_tasks(self) -> List[Task]:
        """Return fallback tasks when decomposition fails."""
        return [
            Task(
                id="fallback_1",
                goal="Implement basic tool sequence parser with regex pattern matching",
                novelty_score=0.8,
                estimated_roi=0.9
            ),
            Task(
                id="fallback_2",
                goal="Add error handling and logging for failed predictions",
                novelty_score=0.6,
                estimated_roi=0.7
            ),
            Task(
                id="fallback_3",
                goal="Create test harness to evaluate accuracy on sample data",
                novelty_score=0.7,
                estimated_roi=0.8
            )
        ]

    def prioritize(self, tasks: List[Task]) -> List[Task]:
        """Score tasks by novelty * coverage * ROI"""
        for task in tasks:
            # Apply learned policies
            policy_weight = self.policies.get(task.goal[:20], 1.0)

            task.priority = (
                task.novelty_score * 0.4 +
                task.estimated_roi * 0.4 +
                policy_weight * 0.2
            )

        return sorted(tasks, key=lambda t: t.priority, reverse=True)

    def parents_verified(self, task: Task) -> bool:
        """Check if all parent tasks are verified"""
        return all(pid in self.cache.verified_tasks for pid in task.parent_ids)

    def execute_and_evaluate(self, code: str, sample_size: int = 25) -> float:
        """Execute generated code and measure accuracy against test data subset with safety checks."""
        try:
            # SECURITY: Validate code syntax before execution
            try:
                compile(code, '<string>', 'exec')
            except SyntaxError as e:
                st.error(f"❌ Invalid Python syntax in generated code: {e}")
                return 0.0

            # Use same subset size as in dispatch_sub_agent
            test_subset = self.test_data[:sample_size]

            # Create isolated namespace with test data
            namespace = {
                "test_data": test_subset,
                "json": json,
                "re": re,
                "List": List,
                "Dict": Dict,
                "Any": Any
            }

            # SECURITY: Execute with timeout (10 seconds for small subset)
            import signal

            def timeout_handler(signum, frame):
                raise TimeoutError("Code execution timeout")

            # Set timeout (only works on Unix-like systems)
            try:
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(10)  # 10 second timeout

                # Execute the code
                exec(code, namespace)

                signal.alarm(0)  # Cancel timeout
            except AttributeError:
                # Windows doesn't support SIGALRM, use threading timeout instead
                import threading

                exec_exception = None

                def exec_with_exception():
                    nonlocal exec_exception
                    try:
                        exec(code, namespace)
                    except Exception as e:
                        exec_exception = e

                exec_thread = threading.Thread(target=exec_with_exception)
                exec_thread.daemon = True
                exec_thread.start()
                exec_thread.join(timeout=10.0)

                if exec_thread.is_alive():
                    st.error("❌ Code execution timeout (10 seconds)")
                    return 0.0

                if exec_exception:
                    raise exec_exception

            # Extract predictions (code should create 'predictions' variable)
            predictions = namespace.get('predictions', [])

            if not predictions:
                st.warning("⚠️ Code did not create 'predictions' variable")
                return 0.0

            # Validate predictions structure
            if not isinstance(predictions, list):
                st.warning(f"⚠️ 'predictions' should be a list, got {type(predictions).__name__}")
                return 0.0

            if len(predictions) != len(test_subset):
                st.warning(f"⚠️ Prediction count mismatch: {len(predictions)} vs {len(test_subset)}")
                # Try to calculate accuracy anyway with available predictions
                test_subset = test_subset[:len(predictions)]

            # Calculate accuracy
            correct = 0
            for i, (pred, expected) in enumerate(zip(predictions, test_subset)):
                # Validate prediction structure
                if not isinstance(pred, dict):
                    continue

                pred_seq = pred.get('predicted_sequence', [])
                exp_seq = expected.get('expected_sequence', [])

                # Normalize to strings for comparison
                if isinstance(pred_seq, list):
                    pred_seq = [str(x).strip() for x in pred_seq]
                else:
                    pred_seq = []

                if isinstance(exp_seq, list):
                    exp_seq = [str(x).strip() for x in exp_seq]
                else:
                    exp_seq = []

                if pred_seq == exp_seq:
                    correct += 1

            accuracy = correct / len(test_subset) if test_subset else 0.0
            return accuracy

        except TimeoutError:
            st.error("❌ Code execution timeout")
            return 0.0
        except Exception as e:
            st.warning(f"⚠️ Code execution failed: {e}")
            import traceback
            st.text(traceback.format_exc()[:500])
            return 0.0

    async def dispatch_sub_agent(self, task: Task) -> Any:
        """Execute task via Gemini code execution with PROGRESSIVE BATCHING."""

        # CRITICAL FIX: Progressive batch sizing to avoid Gemini timeout
        # Gemini Code Execution can't handle 149 examples in one run (resource/time limits)
        # Solution: Start small (20), grow with confidence (50, 100)
        iteration = len(self.turn_history) + 1

        if iteration == 1:
            batch_size = 20  # Start small to ensure success
        elif iteration == 2:
            batch_size = 50  # Expand if Turn 1 worked
        else:
            batch_size = min(100, len(self.test_data))  # Max 100 at a time

        test_subset = self.test_data[:batch_size]

        st.info(f"🔄 **Turn {iteration}: Training on {batch_size} examples** (progressive batching to avoid timeout)")
        st.caption(f"Note: Will evaluate on full {len(self.test_data)} examples locally after code generation")

        # Show diverse examples for learning
        diverse_samples = []
        if len(test_subset) >= 10:
            diverse_samples = [
                test_subset[0],  # First
                test_subset[len(test_subset)//4],  # 25%
                test_subset[len(test_subset)//2],  # 50%
                test_subset[3*len(test_subset)//4],  # 75%
                test_subset[-1]  # Last
            ]
        else:
            diverse_samples = test_subset[:min(5, len(test_subset))]

        sample_data = json.dumps(diverse_samples, indent=2)

        if iteration == 1:
            # First turn: generate initial solution (FUNCTION ONLY)
            prompt = f"""
Generate a predict_tools function that predicts tool sequences from queries.

SAMPLE DATA (batch 1 of ~8 batches, showing {len(diverse_samples)} diverse examples):
{sample_data}

Your function will be tested on ALL {len(self.test_data)} examples across multiple batches.

Requirements:
- Function signature: def predict_tools(query: str) -> list
- Return list of tool names like ["check_device"], ["get_incident"], etc.
- Handle various patterns: device checks, incidents, IP lookups, authentication, database queries
- Must work on ANY query, not just these examples

EXAMPLE PATTERNS TO HANDLE:
- "check device status" → ["check_device"]
- "incident INC-123" → ["get_incident"]
- "lookup IP 192.168.1.1" → ["get_ip_info"]
- "authenticate user and query database" → ["authenticate_user", "query_database"]

Return ONLY the predict_tools function code (no test harness, no test data).
"""
        else:
            # Subsequent turns: refine based on failures
            failures_count = int((1 - self.best_performance) * len(self.test_data))

            prompt = f"""
CURRENT FUNCTION (accuracy {self.best_performance:.3f} on full {len(self.test_data)} examples):
```python
{self.best_code}
```

This function fails on {failures_count} of {len(self.test_data)} test cases.

SAMPLE DATA (showing diverse examples):
{sample_data}

TASK: Improve the predict_tools function to handle more patterns.

Common failure patterns to address:
- Multi-step sequences (e.g., authenticate then query)
- IP address detection
- Incident ID patterns
- Inventory/ordering flows
- Edge cases

Return ONLY the improved predict_tools function code.
"""

        try:
            # CRITICAL FIX: Create client without async context manager
            # genai.Client doesn't support async context managers
            client = genai.Client(api_key=GEMINI_API_KEY)

            def sync_call():
                return client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=[types.Content(parts=[types.Part.from_text(text=prompt)])],
                    config=types.GenerateContentConfig(
                        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
                    )
                )

            # Add 90 second timeout (Gemini code execution can be slow)
            try:
                response = await asyncio.wait_for(
                    asyncio.to_thread(sync_call),
                    timeout=90.0
                )
            except asyncio.TimeoutError:
                st.error(f"⏱️ Task {task.id} timed out after 90 seconds")
                return {
                    "task_id": task.id,
                    "code": None,
                    "accuracy": 0.0,
                    "error": "Timeout: Gemini code execution exceeded 90 seconds"
                }

            # Track cost
            st.session_state.cost_tracker.update(
                provider="Google", model="gemini-2.5-flash",
                api="generate_content",
                raw_response_obj=response,
                pricing_resolver=custom_gemini_price_lookup
            )

            # Update budget
            cost = st.session_state.cost_tracker.totals['total_cost_usd']
            tokens = st.session_state.cost_tracker.totals['total_tokens']
            self.budget.consume(cost, tokens)

            st.write(f"💰 Budget consumed: ${cost:.4f}, {tokens:,} tokens (Remaining: {self.budget.left()*100:.1f}%)")

            # DEBUG: See what we got from Gemini
            with st.expander(f"🔍 Debug: Response structure for {task.id}", expanded=False):
                st.write(f"- Candidates: {len(response.candidates)}")

                if response.candidates:
                    parts = response.candidates[0].content.parts
                    st.write(f"- Parts: {len(parts)}")

                    for i, part in enumerate(parts):
                        if part.text:
                            st.write(f"  Part {i}: text ({len(part.text)} chars)")
                            st.text(part.text[:300])  # Show first 300 chars
                        if part.executable_code:
                            st.write(f"  Part {i}: executable_code ({len(part.executable_code.code)} chars)")
                            st.code(part.executable_code.code[:500], language='python')  # Show first 500 chars
                        if part.code_execution_result:
                            st.write(f"  Part {i}: code_execution_result")
                            st.text(part.code_execution_result.output[:300])

            # Extract outputs - handle BOTH executable_code and markdown-wrapped code
            code = None
            gemini_reported_accuracy = 0.0
            execution_output = ""

            for part in response.candidates[0].content.parts:
                # Method 1: Direct executable_code field (preferred)
                if part.executable_code:
                    code = part.executable_code.code
                    st.success(f"✅ Task {task.id}: Generated {len(code)} chars (executable_code)")

                # Method 2: Code in text (markdown wrapped) - CRITICAL FIX
                if part.text and not code:
                    text = part.text

                    # Extract code from markdown blocks
                    code_match = re.search(r'```python\n(.*?)```', text, re.DOTALL)
                    if code_match:
                        code = code_match.group(1)
                        st.success(f"✅ Task {task.id}: Extracted {len(code)} chars from markdown")

                # Get execution output
                if part.code_execution_result:
                    execution_output = part.code_execution_result.output or ""
                    match = re.search(r"Accuracy:\s*(\d+\.\d+)", execution_output)
                    if match:
                        gemini_reported_accuracy = float(match.group(1))

            # CRITICAL: Validate extracted code
            if not code:
                # Show what we got for debugging
                all_text = " ".join(p.text for p in response.candidates[0].content.parts if p.text)
                st.error(f"❌ Task {task.id}: No code found in response")
                st.text(all_text[:500])
                return {
                    "task_id": task.id,
                    "code": None,
                    "accuracy": 0.0,
                    "error": "No code extracted from Gemini response"
                }

            # Validate code syntax
            try:
                compile(code, '<string>', 'exec')
            except SyntaxError as e:
                st.error(f"❌ Task {task.id}: Invalid Python syntax in generated code")
                st.code(code[:500], language='python')
                st.text(f"Syntax error: {e}")
                return {
                    "task_id": task.id,
                    "code": None,
                    "accuracy": 0.0,
                    "error": f"Invalid Python syntax: {e}"
                }

            # Check for required function (predict_tools)
            if 'def predict_tools' not in code:
                st.warning(f"⚠️ Task {task.id}: Code missing 'predict_tools' function")
                st.code(code[:500], language='python')
                # Don't fail completely - let execution try anyway
                # return {
                #     "task_id": task.id,
                #     "code": None,
                #     "accuracy": 0.0,
                #     "error": "Missing predict_tools function"
                # }

                # Return failed result
                return {
                    "task_id": task.id,
                    "code": None,
                    "accuracy": 0.0,
                    "error": "No code extracted from response",
                    "response": response
                }

            # Return code for map-reduce evaluation
            # (Accuracy will be measured by testing on ALL batches in run_turn_with_map_reduce)
            st.success(f"✅ Code extraction complete - ready for map-reduce evaluation")

            if execution_output:
                st.write(f"💬 Gemini output: {execution_output[:150]}...")

            return {"code": code, "accuracy": 0.0, "response": response, "output": execution_output}

        except Exception as e:
            st.error(f"❌ Task {task.id} execution failed: {e}")
            st.exception(e)
            return {"code": None, "accuracy": 0.0, "error": str(e)}

    async def verify(self, outputs: Dict[str, Any]) -> VerificationResult:
        """Formal verification: claims + evidence → verdict"""
        code = outputs.get("code")
        accuracy = outputs.get("accuracy", 0.0)
        task_id = outputs.get("task_id", "unknown")

        # Handle error cases
        if outputs.get("error"):
            return VerificationResult(
                task_id=task_id,
                claims=["Task failed with error"],
                evidence={"error": outputs["error"]},
                verdict="failed",
                confidence=0.0,
                outputs=outputs
            )

        if not code:
            return VerificationResult(
                task_id=task_id,
                claims=["No code generated"],
                evidence={},
                verdict="failed",
                confidence=0.0,
                outputs=outputs
            )

        # Generate claims
        claims = [
            f"Code executes without errors",
            f"Achieves accuracy of {accuracy:.3f}",
            f"Handles all test cases"
        ]

        # Collect evidence
        evidence = {
            "accuracy": accuracy,
            "code_length": len(code),
            "improvement": accuracy - self.best_performance
        }

        # Determine verdict
        if accuracy > self.best_performance:
            verdict = "verified"
            confidence = min(1.0, accuracy)
        elif accuracy >= self.best_performance * 0.95:
            verdict = "partial"
            confidence = accuracy
        else:
            verdict = "failed"
            confidence = accuracy

        return VerificationResult(
            task_id=task_id,
            claims=claims,
            evidence=evidence,
            verdict=verdict,
            confidence=confidence,
            outputs=outputs
        )

    def record_turn(self, tasks_attempted: int, tasks_verified: int, current_best: float):
        """Record metrics for current turn"""
        turn = self.budget.current_turn
        improvement = current_best - self.best_performance if self.turn_history else current_best

        metrics = TurnMetrics(
            turn=turn,
            tasks_attempted=tasks_attempted,
            tasks_verified=tasks_verified,
            best_accuracy=current_best,
            improvement=improvement,
            cost_spent=self.budget.spent_cost,
            tokens_used=self.budget.spent_tokens,
            timestamp=time.time()
        )

        self.turn_history.append(metrics)

        if current_best > self.best_performance:
            self.best_performance = current_best
            self.best_turn = turn

    def converged_by_turns(self) -> bool:
        """Check convergence based on turn history (for turn mode)"""
        if len(self.turn_history) < 3:
            return False

        # Check last 3 turns for improvements
        recent = self.turn_history[-3:]
        improvements = [t.improvement for t in recent]

        # No improvement in last 3 turns (with small tolerance)
        if all(abs(imp) <= 0.001 for imp in improvements):
            st.info("Converged: No improvement in last 3 turns")
            return True

        return False

    def converged(self) -> bool:
        """Check if marginal value is below threshold (for cost mode)"""
        if len(self.turn_history) < 3:
            return False  # Minimum iterations

        # Check recent improvement rate from turn history (more reliable than index)
        recent = self.turn_history[-3:]
        improvements = [t.improvement for t in recent]

        # Average improvement over last 3 turns
        avg_improvement = sum(improvements) / len(improvements) if improvements else 0.0

        # Converged if average improvement is negligible
        if avg_improvement < 0.01:
            st.info(f"Converged: Avg improvement {avg_improvement:.4f} < 0.01")
            return True

        return False

    def reflect_and_update_policies(self):
        """Update policies based on what worked"""
        # Analyze successful tasks
        verified = [
            e for e in self.index.entries
            if e["verdict"] == "verified"
        ]

        # Update policy weights for successful task types
        for entry in verified[-3:]:  # Last 3 successes
            task_prefix = entry["task_id"][:20]
            self.policies[task_prefix] += 0.1  # Boost successful patterns

        st.info(f"Policy update: {len(verified)} verified tasks inform future prioritization")

    async def test_code_on_batch(self, code: str, batch_data: List[Dict], batch_num: int) -> Dict:
        """Test code on one batch and return results with safety checks."""
        try:
            # SECURITY: Validate code syntax before execution
            try:
                compile(code, '<string>', 'exec')
            except SyntaxError as e:
                return {
                    "batch_num": batch_num,
                    "error": f"Invalid Python syntax: {e}",
                    "accuracy": 0.0,
                    "predictions": [],
                    "correct": 0,
                    "total": len(batch_data)
                }

            namespace = {"test_data": batch_data, "json": json, "re": re}

            # SECURITY: Execute with timeout
            import signal
            import threading

            exec_exception = None

            def exec_with_exception():
                nonlocal exec_exception
                try:
                    exec(code, namespace)
                except Exception as e:
                    exec_exception = e

            # Use threading for cross-platform timeout
            exec_thread = threading.Thread(target=exec_with_exception)
            exec_thread.daemon = True
            exec_thread.start()
            exec_thread.join(timeout=15.0)  # 15 second timeout for batch

            if exec_thread.is_alive():
                return {
                    "batch_num": batch_num,
                    "error": "Code execution timeout (15 seconds)",
                    "accuracy": 0.0,
                    "predictions": [],
                    "correct": 0,
                    "total": len(batch_data)
                }

            if exec_exception:
                raise exec_exception

            # Check if predict_tools exists
            if 'predict_tools' not in namespace:
                return {
                    "batch_num": batch_num,
                    "error": "predict_tools function not found in code",
                    "accuracy": 0.0,
                    "predictions": [],
                    "correct": 0,
                    "total": len(batch_data)
                }

            predict_tools = namespace['predict_tools']

            # Validate it's callable
            if not callable(predict_tools):
                return {
                    "batch_num": batch_num,
                    "error": "predict_tools is not a function",
                    "accuracy": 0.0,
                    "predictions": [],
                    "correct": 0,
                    "total": len(batch_data)
                }

            # Generate predictions
            predictions = []
            for item in batch_data:
                try:
                    pred = predict_tools(item.get('query', ''))
                    # Ensure pred is a list
                    if not isinstance(pred, list):
                        pred = [pred] if pred else []
                    predictions.append({'predicted_sequence': pred})
                except Exception as e:
                    # Log individual prediction failures but continue
                    predictions.append({'predicted_sequence': []})

            # Calculate batch accuracy
            correct = 0
            for p, e in zip(predictions, batch_data):
                pred_seq = p.get('predicted_sequence', [])
                exp_seq = e.get('expected_sequence', [])

                # Normalize both to lists of strings
                if isinstance(pred_seq, list):
                    pred_seq = [str(x).strip() for x in pred_seq]
                else:
                    pred_seq = []

                if isinstance(exp_seq, list):
                    exp_seq = [str(x).strip() for x in exp_seq]
                else:
                    exp_seq = []

                if pred_seq == exp_seq:
                    correct += 1

            return {
                "batch_num": batch_num,
                "predictions": predictions,
                "accuracy": correct / len(batch_data) if batch_data else 0.0,
                "correct": correct,
                "total": len(batch_data)
            }
        except Exception as e:
            return {
                "batch_num": batch_num,
                "error": str(e),
                "accuracy": 0.0,
                "predictions": [],
                "correct": 0,
                "total": len(batch_data)
            }

    async def run_turn_with_map_reduce(self, turn: int, batch_size: int = 20) -> float:
        """Run one turn by processing ALL data in batches (map-reduce), then merging results."""

        turn_id = f"turn_{turn}"
        test_name = "Test 5"

        # Emit turn start event
        if self.tracker:
            self.tracker.emit(
                test_name=test_name,
                event_type="start",
                agent_id=turn_id,
                agent_name=f"Turn {turn}",
                agent_type="main_agent",
                total_examples=len(self.test_data),
                batch_size=batch_size
            )

        st.markdown(f"### Turn {turn}: Processing {len(self.test_data)} examples in batches of {batch_size}")

        # Step 1: Divide data into batches
        batches = []
        for i in range(0, len(self.test_data), batch_size):
            batches.append(self.test_data[i:i + batch_size])

        st.write(f"📦 Split into {len(batches)} batches")

        # Step 2: Generate code using first batch as sample
        gen_id = f"{turn_id}_generation"
        if self.tracker:
            self.tracker.emit(
                test_name=test_name,
                event_type="start",
                agent_id=gen_id,
                agent_name="Code Generation",
                agent_type="sub_agent",
                parent=turn_id
            )

        context = {"verified_count": len(self.cache.verified_tasks), "best_perf": self.best_performance}
        tasks = await self.decompose(self.goal, context)

        # CRITICAL FIX: Handle empty tasks list
        if not tasks or len(tasks) == 0:
            st.error("❌ No tasks generated from decompose. Creating fallback task.")
            tasks = [Task(
                id=f"fallback_turn_{turn}",
                goal="Generate predict_tools function for tool sequence prediction",
                novelty_score=0.8,
                estimated_roi=0.9
            )]

        st.write(f"✅ Decomposed into {len(tasks)} tasks")

        st.write(f"⚙️ Generating code using batch 1 of {len(batches)} as sample...")
        result = await self.dispatch_sub_agent(tasks[0])

        generated_code = result.get("code", None)

        if not generated_code:
            st.error("❌ No code generated")
            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="error",
                    agent_id=gen_id,
                    agent_name="Code Generation",
                    agent_type="sub_agent",
                    error="No code generated"
                )
                self.tracker.emit(
                    test_name=test_name,
                    event_type="error",
                    agent_id=turn_id,
                    agent_name=f"Turn {turn}",
                    agent_type="main_agent",
                    error="Code generation failed"
                )
            return 0.0

        st.success(f"✅ Code generated! ({len(generated_code)} chars)")
        if self.tracker:
            self.tracker.emit(
                test_name=test_name,
                event_type="complete",
                agent_id=gen_id,
                agent_name="Code Generation",
                agent_type="sub_agent",
                output_size=len(generated_code),
                success=True
            )

        # Security audit: Check generated code
        if self.security_agent:
            is_safe, status, warnings = self.security_agent.audit_code(
                code=generated_code,
                agent="CodeGenerator",
                turn=turn
            )

            if not is_safe:
                st.warning(f"⚠️ Security warnings detected: {', '.join(warnings)}")

        # Memory: Store generated code in archival memory
        if self.memory_manager:
            self.memory_manager.insert_archival(
                content=f"Turn {turn} generated code: {generated_code[:200]}...",
                tags=["code_generation", f"turn_{turn}"],
                source_agent="CodeGenerator",
                turn=turn
            )

        # Step 3: MAP - Test code on ALL batches in parallel
        eval_id = f"{turn_id}_evaluation"
        if self.tracker:
            self.tracker.emit(
                test_name=test_name,
                event_type="start",
                agent_id=eval_id,
                agent_name="Batch Evaluation",
                agent_type="sub_agent",
                parent=turn_id,
                batch_count=len(batches)
            )

        st.write(f"🔍 Testing code on all {len(batches)} batches in parallel...")

        # Add progress indicator for batch processing
        progress_bar = st.progress(0.0)
        status_text = st.empty()
        status_text.text(f"Processing 0/{len(batches)} batches...")

        # Create tasks for parallel execution
        batch_tasks = [
            self.test_code_on_batch(generated_code, batch, i)
            for i, batch in enumerate(batches)
        ]

        # Execute all batches in parallel with progress updates
        # Note: asyncio.gather doesn't provide incremental progress, so we show indeterminate progress
        status_text.text(f"Processing {len(batches)} batches in parallel...")
        progress_bar.progress(0.5)  # Show we're in progress

        batch_results = await asyncio.gather(*batch_tasks)

        progress_bar.progress(1.0)
        status_text.text(f"✅ Completed {len(batches)} batches")

        # Step 4: REDUCE - Combine results with validation
        all_predictions = []
        total_correct = 0
        total_examples = 0
        errors = []

        for result in batch_results:
            # Validate result structure
            if not isinstance(result, dict):
                errors.append(f"Batch returned invalid result type: {type(result).__name__}")
                continue

            # Check for errors first
            if result.get("error"):
                batch_num = result.get("batch_num", "unknown")
                errors.append(f"Batch {batch_num}: {result['error']}")
                continue

            # Validate required keys exist
            required_keys = ["predictions", "correct", "total", "accuracy", "batch_num"]
            missing_keys = [k for k in required_keys if k not in result]
            if missing_keys:
                batch_num = result.get("batch_num", "unknown")
                errors.append(f"Batch {batch_num}: Missing keys {missing_keys}")
                continue

            # Validate predictions is a list
            if not isinstance(result["predictions"], list):
                errors.append(f"Batch {result['batch_num']}: predictions is not a list")
                continue

            # All validations passed - accumulate results
            all_predictions.extend(result["predictions"])
            total_correct += result["correct"]
            total_examples += result["total"]
            st.write(f"  ✓ Batch {result['batch_num']}: {result['accuracy']:.3f} ({result['correct']}/{result['total']})")

        if errors:
            st.error(f"⚠️ {len(errors)} batch(es) failed:")
            for err in errors[:5]:  # Show first 5 errors
                st.warning(f"  • {err}")
            if len(errors) > 5:
                st.warning(f"  ... and {len(errors) - 5} more errors")

        # Calculate overall accuracy
        overall_accuracy = total_correct / total_examples if total_examples > 0 else 0.0

        st.metric(f"Turn {turn} Overall Accuracy", f"{overall_accuracy:.3f}",
                 delta=f"+{overall_accuracy - self.best_performance:.3f}" if overall_accuracy > self.best_performance else None,
                 help=f"Tested on all {total_examples} examples")

        if self.tracker:
            self.tracker.emit(
                test_name=test_name,
                event_type="complete",
                agent_id=eval_id,
                agent_name="Batch Evaluation",
                agent_type="sub_agent",
                accuracy=overall_accuracy,
                examples_tested=total_examples,
                batches_processed=len(batches)
            )

        # Step 5: Update best if improved
        if overall_accuracy > self.best_performance:
            improvement = overall_accuracy - self.best_performance
            self.best_performance = overall_accuracy
            self.best_code = generated_code
            self.best_turn = turn
            st.success(f"🎉 **New best!** {self.best_performance:.3f} (+{improvement:.3f})")

            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="complete",
                    agent_id=turn_id,
                    agent_name=f"Turn {turn}",
                    agent_type="main_agent",
                    accuracy=overall_accuracy,
                    is_best=True,
                    improvement=improvement
                )
        else:
            st.info(f"No improvement: {overall_accuracy:.3f} (best remains {self.best_performance:.3f})")

            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="complete",
                    agent_id=turn_id,
                    agent_name=f"Turn {turn}",
                    agent_type="main_agent",
                    accuracy=overall_accuracy,
                    is_best=False,
                    improvement=0.0
                )

        # Log memory snapshot at end of turn
        if self.memory_manager:
            self.memory_manager.get_snapshot(turn=turn)

        # Log Gantt task for this turn
        if self.dashboard_logger:
            self.dashboard_logger.log_gantt_task(
                task_id=turn_id,
                agent=f"Turn {turn}",
                agent_type="main_agent",
                start_time=time.time() - 60,  # Approximate (would need actual start time)
                end_time=time.time(),
                status="complete",
                accuracy=overall_accuracy,
                is_best=overall_accuracy > self.best_performance
            )

        return overall_accuracy

    async def run_inference_mode(self):
        """Direct inference mode: Pattern matching for prediction/classification tasks."""
        st.subheader("Direct Inference Mode: Iterative Refinement (Map-Reduce Batching)")

        for turn in range(1, self.budget.max_turns + 1):
            if self.budget.mode == "turns":
                self.budget.advance_turn()

            st.markdown(f"## Turn {turn} (Budget: {self.budget.left()*100:.1f}% remaining)")

            # Show current state
            if self.best_performance > 0:
                failures = int((1 - self.best_performance) * len(self.test_data))
                st.write(f"📊 Current best: {self.best_performance:.3f} ({failures} failures out of {len(self.test_data)} examples)")
            else:
                st.write(f"📊 Turn 1: Generating initial solution for {len(self.test_data)} examples")

            # Run turn with map-reduce batching
            accuracy = await self.run_turn_with_map_reduce(turn, batch_size=20)

            # Track metrics
            self.record_turn(
                tasks_attempted=1,
                tasks_verified=1 if accuracy > 0 else 0,
                current_best=accuracy
            )

            # Check convergence
            if self.converged_by_turns():
                st.info("🏁 Converged: no improvement in last 3 turns")
                break

        return self.best_code, self.best_performance, self.turn_history

    # ============================================
    # MODE 3: Research Tasks (Decomposition)
    # ============================================

    async def decompose_research(self, goal: str, context: Dict) -> List[Task]:
        """Break research goal into parallel information gathering tasks."""

        iteration = len(self.cache.verified_tasks)

        prompt = f"""
Break this research goal into 3-5 independent subtasks:

GOAL: {goal}

CONTEXT:
- Completed tasks: {iteration}
- Knowledge gathered: {len(self.index.entries)} entries

Create concrete, parallelizable subtasks. Each should:
- Gather specific information from distinct sources
- Be independently executable
- Contribute to answering the main goal

Examples for "Research George Morgan, Symbolica AI, and their AI Engineering position":
- Task 1: Search for George Morgan's LinkedIn profile and role
- Task 2: Find Symbolica AI company information and funding history
- Task 3: Search for recent statements on AI Engineering from Symbolica
- Task 4: Locate news about their latest funding round
- Task 5: Find technical blog posts or papers from the team

Return JSON:
[
  {{
    "id": "research_1",
    "goal": "Search LinkedIn for George Morgan at Symbolica AI",
    "source_type": "linkedin",
    "depends_on": [],
    "novelty": 0.9
  }},
  ...
]
"""

        model = st.session_state.get('judge_model', 'openai/gpt-5-mini')
        provider = _get_provider_from_model_id(model)

        if provider == "openai" and OPENAI_API_KEY:
            client = AsyncOpenAI(api_key=OPENAI_API_KEY)
            native_model = _to_native_model_id(model)
            resp = await client.chat.completions.create(
                model=native_model,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )
            content = resp.choices[0].message.content
        else:
            # OpenRouter fallback
            headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}"}
            body = {
                "model": _to_openrouter_model_id(model),
                "messages": [{"role": "user", "content": prompt}],
                "response_format": {"type": "json_object"}
            }
            async with httpx.AsyncClient(timeout=60) as client_http:
                resp = await client_http.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers=headers,
                    json=body
                )
                data = resp.json()
                content = data["choices"][0]["message"]["content"]

        tasks_data = json.loads(content)
        if isinstance(tasks_data, dict):
            tasks_data = tasks_data.get("subtasks", tasks_data.get("tasks", []))

        tasks = []
        for t in tasks_data:
            task = Task(
                id=t["id"],
                goal=t["goal"],
                parent_ids=set(t.get("depends_on", [])),
                novelty_score=float(t.get("novelty", 0.5)),
                estimated_roi=float(t.get("roi", 0.8))
            )
            tasks.append(task)

        return tasks

    async def execute_research_task(self, task: Task) -> Dict:
        """Execute a research task using web search or other tools."""

        st.write(f"🔍 Researching: {task.goal}")

        # Use Gemini for research
        prompt = f"""
Research this specific question: {task.goal}

Provide:
- Key findings (3-5 bullet points)
- Sources found
- Confidence level (0-1)

Return JSON: {{"findings": [...], "sources": [...], "confidence": 0.8}}
"""

        # CRITICAL FIX: Create client without async context manager
        # genai.Client doesn't support async context managers
        client = genai.Client(api_key=GEMINI_API_KEY)

        response = await asyncio.to_thread(
            lambda: client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
        )

        result = json.loads(response.text)
        return {
            "task_id": task.id,
            "findings": result.get("findings", []),
            "sources": result.get("sources", []),
            "confidence": result.get("confidence", 0.5)
        }

    async def run_research_mode(self):
        """Execute research workflow with task decomposition."""

        st.header("Research Mode: Multi-Source Information Gathering")
        test_name = "Test 5"

        all_findings = []

        for turn in range(1, self.budget.max_turns + 1):
            turn_id = f"research_turn_{turn}"

            # Emit turn start
            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="start",
                    agent_id=turn_id,
                    agent_name=f"Research Turn {turn}",
                    agent_type="research_turn"
                )

            st.subheader(f"Research Turn {turn}")

            # Decompose into research tasks
            context = {"completed": len(all_findings)}
            tasks = await self.decompose_research(self.goal, context)

            st.write(f"Generated {len(tasks)} research tasks:")
            for task in tasks:
                st.write(f"  - {task.goal}")

            # Execute tasks in parallel
            research_coros = [self.execute_research_task(task) for task in tasks]
            results = await asyncio.gather(*research_coros)

            # Store in knowledge index
            for result in results:
                await self.index.store(result, VerificationResult(
                    task_id=result['task_id'],
                    claims=result['findings'],
                    evidence={"sources": result['sources']},
                    verdict="verified",
                    confidence=result['confidence'],
                    outputs=result
                ))
                all_findings.extend(result['findings'])

            # Emit turn completion
            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="complete",
                    agent_id=turn_id,
                    agent_name=f"Research Turn {turn}",
                    agent_type="research_turn",
                    findings_count=len(all_findings),
                    tasks_completed=len(results)
                )

            # Check if we have enough information
            if len(all_findings) >= 15:  # Arbitrary threshold
                st.info("Sufficient information gathered")
                break

        # Synthesize final report
        st.divider()
        st.subheader("Research Summary")

        synthesis_prompt = f"""
Synthesize these research findings into a coherent report:

ORIGINAL GOAL: {self.goal}

FINDINGS:
{json.dumps(all_findings, indent=2)}

Create a structured report with:
- Executive summary
- Key findings organized by topic
- Sources and confidence levels
- Recommendations or next steps
"""

        # CRITICAL FIX: Create client without async context manager
        # genai.Client doesn't support async context managers
        client = genai.Client(api_key=GEMINI_API_KEY)

        synthesis = await asyncio.to_thread(
            lambda: client.models.generate_content(
                model="gemini-2.5-flash",
                contents=synthesis_prompt
            )
        )

        st.markdown(synthesis.text)

        return {
            "findings_count": len(all_findings),
            "knowledge_entries": len(self.index.entries),
            "synthesis": synthesis.text
        }

    def _initialize_stateful_components(self):
        """Initialize memory, security, and self-correction components with logging."""
        if self._stateful_components_initialized:
            return

        # Initialize dashboard logger
        if not self.dashboard_logger:
            test_id = f"test5_{self.mode}_{self.coordination_pattern}"
            dataset_info = {
                "size": len(self.test_data),
                "type": "classification" if self.test_data else "research"
            }
            configuration = {
                "mode": self.mode,
                "coordination_pattern": self.coordination_pattern,
                "budget_mode": self.budget.mode,
                "max_turns": self.budget.max_turns
            }

            self.dashboard_logger = DashboardLogger(
                test_id=test_id,
                test_type="orchestrator",
                test_name="Test 5",
                model="gemini-2.5-flash",
                dataset_info=dataset_info,
                configuration=configuration
            )

            # Store in session state for persistence
            st.session_state['test5_dashboard_logger'] = self.dashboard_logger

        # Initialize memory manager
        self.memory_manager = MemoryManager(logger=self.dashboard_logger)

        # Set purpose based on goal
        self.memory_manager.update_block(
            block_name="Purpose",
            new_content=f"My current goal is: {self.goal}",
            turn=0,
            trigger="initialization",
            change_summary="Set initial purpose from user goal"
        )

        # Override Tool Guidelines with demo-specific policy if available
        if 'demo_memory_policy' in st.session_state:
            self.memory_manager.update_block(
                block_name="Tool Guidelines",
                new_content=st.session_state['demo_memory_policy'],
                turn=0,
                trigger="DEMO_INITIALIZATION",
                change_summary=f"Loaded {st.session_state.get('demo_scenario', 'demo')} scenario policy"
            )
            st.info(f"🧠 Memory initialized with {st.session_state.get('demo_scenario', 'demo').upper()} policy")

        # Initialize security audit agent
        self.security_agent = SecurityAuditAgent(logger=self.dashboard_logger)

        # Initialize self-correction manager
        self.self_correction_manager = SelfCorrectionManager(
            memory_manager=self.memory_manager,
            logger=self.dashboard_logger
        )

        self._stateful_components_initialized = True

        # Log initial memory snapshot
        self.memory_manager.get_snapshot(turn=0)

    async def run(self):
        """Main orchestrator with mode-specific execution strategies and coordination patterns."""

        test_name = "Test 5"

        # Initialize stateful components (memory, security, self-correction)
        self._initialize_stateful_components()

        # Emit orchestrator start event
        if self.tracker:
            self.tracker.emit(
                test_name=test_name,
                event_type="start",
                agent_id="orchestrator",
                agent_name="Unified Orchestrator",
                agent_type="orchestrator",
                mode=self.mode,
                coordination_pattern=self.coordination_pattern,
                goal=self.goal[:100]  # Truncate for metadata
            )

        st.info(f"🎯 Mode: **{self.mode.upper()}** | 🤝 Pattern: **{self.coordination_pattern.upper()}**")

        try:
            # Route based on coordination pattern
            if self.coordination_pattern == AgentCoordinationPattern.SOLO.value:
                # Solo execution (existing implementations)
                result = await self._run_solo()

            elif self.coordination_pattern == AgentCoordinationPattern.SUBAGENT.value:
                # Subagent orchestration (hierarchical delegation)
                result = await self.run_with_subagents()

            elif self.coordination_pattern == AgentCoordinationPattern.MULTI_AGENT.value:
                # Multi-agent collaboration (peer consensus)
                result = await self.run_with_multi_agent()

            elif self.coordination_pattern == AgentCoordinationPattern.LEAF_SCAFFOLD.value:
                # Leaf agent scaffold (hierarchical multi-agent with supervisor)
                # Get selected agent types from session state or use defaults
                selected_agent_types = st.session_state.get('selected_agent_types',
                    ["web_researcher", "code_executor", "content_generator"])
                result = await self.run_with_leaf_scaffold(selected_agent_types)

            else:
                st.error(f"Unknown coordination pattern: {self.coordination_pattern}")
                result = {}

            # Emit completion event
            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="complete",
                    agent_id="orchestrator",
                    agent_name="Unified Orchestrator",
                    agent_type="orchestrator",
                    result=str(result)[:200]  # Truncate
                )

            # Finalize dashboard logger with summary metrics
            if self.dashboard_logger:
                summary_metrics = {
                    "final_score": result.get("final_score", 0.0) if isinstance(result, dict) else 0.0,
                    "total_turns": len(self.turn_history),
                    "best_turn": self.best_turn,
                    "total_cost": self.budget.spent_cost,
                    "total_tokens": self.budget.spent_tokens,
                    "mode": self.mode,
                    "coordination_pattern": self.coordination_pattern,
                    "memory_stats": self.memory_manager.stats if self.memory_manager else {},
                    "rethink_count": len(self.self_correction_manager.rethink_history) if self.self_correction_manager else 0
                }

                self.dashboard_logger.finalize_run(summary_metrics)

                st.success(f"✅ Run logs saved to: `{self.dashboard_logger.run_dir}`")

            return result

        except Exception as e:
            # Emit error event
            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="error",
                    agent_id="orchestrator",
                    agent_name="Unified Orchestrator",
                    agent_type="orchestrator",
                    error=str(e)
                )

            # Finalize logger even on error (mark as incomplete)
            if self.dashboard_logger:
                try:
                    self.dashboard_logger.finalize_run({
                        "status": "error",
                        "error": str(e),
                        "total_turns": len(self.turn_history)
                    })
                except:
                    pass  # Don't fail on logging errors

            raise

    async def _run_solo(self):
        """Execute in solo mode (single agent, existing implementations)."""

        # Route to appropriate execution strategy based on mode
        if self.mode == "inference":
            # Inference → Prompt Optimization
            result = await self.run_inference_prompt_optimization()

        elif self.mode == "analysis":
            # Analysis → Code Generation
            result = await self.run_analysis_code_generation()

        elif self.mode == "research":
            # Research → Hybrid (decompose first, then decide per subtask)
            result = await self.run_research_hybrid()
        else:
            st.error(f"Unknown mode: {self.mode}")
            result = {}

        return result

    # ============================================
    # COORDINATION PATTERN: Subagent Orchestration
    # ============================================

    async def run_with_subagents(self):
        """
        Hierarchical delegation with specialized subagents.

        Workflow:
        1. Decomposer: Break down the goal into subtasks
        2. Generator: Generate solutions for each subtask
        3. Evaluator: Evaluate the solutions
        4. Analyzer: Analyze failures and suggest improvements
        5. Synthesizer: Combine results into final solution
        """

        st.subheader(f"Subagent Orchestration: {self.mode.upper()} Mode")
        test_name = "Test 5"

        all_results = []

        for turn in range(1, self.budget.max_turns + 1):
            st.markdown(f"## Turn {turn}: Subagent Workflow")

            # ===== SUBAGENT 1: Decomposer =====
            decomposer_id = f"decomposer_turn_{turn}"
            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="start",
                    agent_id=decomposer_id,
                    agent_name="Decomposer",
                    agent_type="subagent"
                )

            st.write("🔍 **Decomposer**: Breaking down goal into subtasks...")
            context = {"turn": turn, "previous_results": all_results}
            subtasks = await self.decompose(self.goal, context)

            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="complete",
                    agent_id=decomposer_id,
                    agent_name="Decomposer",
                    agent_type="subagent",
                    tasks_generated=len(subtasks)
                )

            st.write(f"  → Generated {len(subtasks)} subtasks")

            # ===== SUBAGENT 2: Generator =====
            generator_id = f"generator_turn_{turn}"
            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="start",
                    agent_id=generator_id,
                    agent_name="Generator",
                    agent_type="subagent"
                )

            st.write("⚙️ **Generator**: Creating solutions...")

            # Generate solutions based on mode
            if self.mode == "inference":
                solution = await self._generate_inference_solution(subtasks)
            elif self.mode == "analysis":
                solution = await self._generate_analysis_solution(subtasks)
            elif self.mode == "research":
                solution = await self._generate_research_solution(subtasks)
            else:
                solution = {"error": "Unknown mode"}

            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="complete",
                    agent_id=generator_id,
                    agent_name="Generator",
                    agent_type="subagent",
                    solution_generated=True
                )

            st.write("  → Solution generated")

            # ===== SUBAGENT 3: Evaluator =====
            evaluator_id = f"evaluator_turn_{turn}"
            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="start",
                    agent_id=evaluator_id,
                    agent_name="Evaluator",
                    agent_type="subagent"
                )

            st.write("📊 **Evaluator**: Assessing solution quality...")
            evaluation = await self._evaluate_solution(solution)

            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="complete",
                    agent_id=evaluator_id,
                    agent_name="Evaluator",
                    agent_type="subagent",
                    accuracy=evaluation.get('accuracy', 0.0),
                    score=evaluation.get('score', 0.0)
                )

            st.write(f"  → Score: {evaluation.get('score', 0.0):.3f}")

            # ===== SUBAGENT 4: Analyzer =====
            analyzer_id = f"analyzer_turn_{turn}"
            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="start",
                    agent_id=analyzer_id,
                    agent_name="Analyzer",
                    agent_type="subagent"
                )

            st.write("🔬 **Analyzer**: Analyzing failures and suggesting improvements...")
            analysis = await self._analyze_failures(solution, evaluation)

            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="complete",
                    agent_id=analyzer_id,
                    agent_name="Analyzer",
                    agent_type="subagent",
                    failures_found=len(analysis.get('failures', [])),
                    suggestions=len(analysis.get('suggestions', []))
                )

            st.write(f"  → Found {len(analysis.get('failures', []))} issues")

            # Store results
            all_results.append({
                "turn": turn,
                "solution": solution,
                "evaluation": evaluation,
                "analysis": analysis
            })

            # Check convergence
            if evaluation.get('score', 0.0) >= 0.95:
                st.success("✅ High quality solution achieved!")
                break

        # ===== SUBAGENT 5: Synthesizer =====
        synthesizer_id = "synthesizer_final"
        if self.tracker:
            self.tracker.emit(
                test_name=test_name,
                event_type="start",
                agent_id=synthesizer_id,
                agent_name="Synthesizer",
                agent_type="subagent"
            )

        st.divider()
        st.write("🎯 **Synthesizer**: Combining results into final solution...")
        final_solution = await self._synthesize_results(all_results)

        if self.tracker:
            self.tracker.emit(
                test_name=test_name,
                event_type="complete",
                agent_id=synthesizer_id,
                agent_name="Synthesizer",
                agent_type="subagent",
                final_score=final_solution.get('score', 0.0)
            )

        return final_solution

    # ============================================
    # COORDINATION PATTERN: Multi-Agent Collaboration
    # ============================================

    async def run_with_multi_agent(self):
        """
        Peer collaboration with independent proposals, cross-review, and consensus.

        Workflow:
        1. Propose: Each peer agent independently proposes a solution
        2. Review: Agents cross-review each other's proposals
        3. Synthesize: Build consensus from all proposals and reviews
        4. Evaluate: Jointly evaluate the consensus solution
        """

        st.subheader(f"Multi-Agent Collaboration: {self.mode.upper()} Mode")
        test_name = "Test 5"

        st.write(f"👥 **Peer Agents**: {', '.join(self.peer_agent_roles)}")

        all_consensus = []

        for turn in range(1, self.budget.max_turns + 1):
            st.markdown(f"## Turn {turn}: Multi-Agent Collaboration")

            # ===== ROUND 1: Propose =====
            st.write("### Round 1: Independent Proposals")
            proposals = []

            # Add progress indicator for proposals
            proposal_progress = st.progress(0.0)
            proposal_status = st.empty()

            for idx, role in enumerate(self.peer_agent_roles):
                agent_id = f"agent_{role.replace(' ', '_').lower()}_turn_{turn}_propose"

                proposal_status.text(f"💡 {role}: Generating proposal... ({idx+1}/{len(self.peer_agent_roles)})")

                if self.tracker:
                    self.tracker.emit(
                        test_name=test_name,
                        event_type="start",
                        agent_id=agent_id,
                        agent_name=f"{role} (Propose)",
                        agent_type="peer_agent"
                    )

                proposal = await self._agent_propose(role, self.goal, all_consensus)
                proposals.append({"role": role, "proposal": proposal})

                if self.tracker:
                    self.tracker.emit(
                        test_name=test_name,
                        event_type="complete",
                        agent_id=agent_id,
                        agent_name=f"{role} (Propose)",
                        agent_type="peer_agent",
                        proposal_length=len(str(proposal))
                    )

                # Update progress
                proposal_progress.progress((idx + 1) / len(self.peer_agent_roles))
                st.write(f"  ✓ **{role}**: Proposal submitted")

            proposal_status.text(f"✅ All {len(self.peer_agent_roles)} proposals received")
            proposal_progress.empty()  # Clear progress bar

            # ===== ROUND 2: Review =====
            st.write("### Round 2: Cross-Review")
            reviews = []

            # Add progress indicator for reviews
            review_progress = st.progress(0.0)
            review_status = st.empty()

            for idx, reviewer_role in enumerate(self.peer_agent_roles):
                agent_id = f"agent_{reviewer_role.replace(' ', '_').lower()}_turn_{turn}_review"

                review_status.text(f"🔍 {reviewer_role}: Reviewing proposals... ({idx+1}/{len(self.peer_agent_roles)})")

                if self.tracker:
                    self.tracker.emit(
                        test_name=test_name,
                        event_type="start",
                        agent_id=agent_id,
                        agent_name=f"{reviewer_role} (Review)",
                        agent_type="peer_agent"
                    )

                # Review all proposals except own
                other_proposals = [p for p in proposals if p['role'] != reviewer_role]
                review = await self._agent_review(reviewer_role, other_proposals)
                reviews.append({"reviewer": reviewer_role, "review": review})

                if self.tracker:
                    self.tracker.emit(
                        test_name=test_name,
                        event_type="complete",
                        agent_id=agent_id,
                        agent_name=f"{reviewer_role} (Review)",
                        agent_type="peer_agent",
                        proposals_reviewed=len(other_proposals)
                    )

                # Update progress
                review_progress.progress((idx + 1) / len(self.peer_agent_roles))
                st.write(f"  ✓ **{reviewer_role}**: Review completed")

            review_status.text(f"✅ All {len(self.peer_agent_roles)} reviews completed")
            review_progress.empty()  # Clear progress bar

            # ===== ROUND 3: Synthesize =====
            st.write("### Round 3: Consensus Building")
            synthesizer_id = f"synthesizer_turn_{turn}"

            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="start",
                    agent_id=synthesizer_id,
                    agent_name="Consensus Builder",
                    agent_type="synthesizer"
                )

            st.write("🤝 **Consensus Builder**: Synthesizing proposals and reviews...")
            consensus = await self._build_consensus(proposals, reviews)

            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="complete",
                    agent_id=synthesizer_id,
                    agent_name="Consensus Builder",
                    agent_type="synthesizer",
                    consensus_built=True
                )

            st.write("  → Consensus achieved")

            # ===== ROUND 4: Joint Evaluation =====
            st.write("### Round 4: Joint Evaluation")
            evaluator_id = f"joint_evaluator_turn_{turn}"

            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="start",
                    agent_id=evaluator_id,
                    agent_name="Joint Evaluator",
                    agent_type="evaluator"
                )

            st.write("📊 **Joint Evaluator**: Assessing consensus solution...")
            evaluation = await self._joint_evaluate(consensus, self.peer_agent_roles)

            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="complete",
                    agent_id=evaluator_id,
                    agent_name="Joint Evaluator",
                    agent_type="evaluator",
                    score=evaluation.get('score', 0.0),
                    agreement=evaluation.get('agreement', 0.0)
                )

            st.write(f"  → Score: {evaluation.get('score', 0.0):.3f}, Agreement: {evaluation.get('agreement', 0.0):.3f}")

            # Store consensus
            all_consensus.append({
                "turn": turn,
                "proposals": proposals,
                "reviews": reviews,
                "consensus": consensus,
                "evaluation": evaluation
            })

            # Check convergence
            if evaluation.get('score', 0.0) >= 0.95 and evaluation.get('agreement', 0.0) >= 0.9:
                st.success("✅ High quality consensus achieved!")
                break

        # Return final consensus
        return {
            "strategy": "multi_agent",
            "peer_roles": self.peer_agent_roles,
            "consensus_history": all_consensus,
            "final_consensus": all_consensus[-1]['consensus'] if all_consensus else {},
            "final_score": all_consensus[-1]['evaluation'].get('score', 0.0) if all_consensus else 0.0
        }

    # ============================================
    # COORDINATION PATTERN: Leaf Agent Scaffold
    # ============================================

    async def run_with_leaf_scaffold(self, selected_agent_types: List[str]):
        """
        Execute using hierarchical leaf agent scaffold.

        Workflow:
        1. Supervisor decomposes task into specialized sub-tasks
        2. Delegates to appropriate leaf agents
        3. Manages dependencies between sub-tasks
        4. Synthesizes final result from all agent outputs

        Args:
            selected_agent_types: List of agent type strings (e.g., ["web_researcher", "code_executor"])
        """

        try:
            st.subheader(f"🌳 Leaf Agent Scaffold: {self.mode.upper()} Mode")
            test_name = "Test 5"

            # Initialize LLM client
            if not GEMINI_API_KEY:
                st.error("❌ Gemini API key required for leaf agent scaffold")
                return {"error": "Missing API key"}

            st.write("🔧 Initializing Gemini LLM client...")
            llm_client = GeminiLLMClient(GEMINI_API_KEY)
            st.success("✅ LLM client initialized")

            # Create leaf agents based on selection
            st.write(f"🔧 Creating leaf agents from selection: {selected_agent_types}")
            leaf_agents = []

            agent_type_map = {
                "web_researcher": (AgentType.WEB_RESEARCHER, WebResearchAgent),
                "code_executor": (AgentType.CODE_EXECUTOR, CodeExecutorAgent),
                "content_generator": (AgentType.CONTENT_GENERATOR, ContentGeneratorAgent),
                "validator": (AgentType.VALIDATOR, ValidatorAgent)
            }

            for agent_type_str in selected_agent_types:
                if agent_type_str in agent_type_map:
                    agent_type, agent_class = agent_type_map[agent_type_str]
                    st.write(f"  • Creating {agent_type_str}...")
                    agent = agent_class(llm_client)
                    leaf_agents.append(agent)
                    st.write(f"    ✅ {agent.name} created")

            if not leaf_agents:
                st.error("❌ No valid leaf agents selected")
                return {"error": "No valid agents"}

            st.success(f"✅ Created {len(leaf_agents)} leaf agents: {', '.join([a.name for a in leaf_agents])}")

            # Create supervisor with custom planner and synthesizer
            st.write("🔧 Creating supervisor agent...")
            supervisor = SupervisorAgent(leaf_agents, memory_manager=self.memory_manager)
            supervisor.task_planner = GeminiTaskPlanner(
                available_agents=[a.agent_type for a in leaf_agents],
                llm_client=llm_client
            )
            supervisor.result_synthesizer = GeminiResultSynthesizer(llm_client)
            st.success("✅ Supervisor agent created with policy-based memory")

            # Track supervisor start
            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="start",
                    agent_id="supervisor",
                    agent_name="Supervisor Agent",
                    agent_type="supervisor"
                )

            # ===== STEP 1: Task Planning =====
            st.markdown("### 📋 Task Planning & Decomposition")

            planning_status = st.empty()
            planning_status.text("🧠 Supervisor analyzing task and decomposing into sub-tasks...")

            try:
                sub_tasks = await supervisor.task_planner.decompose(self.goal, self.mode)
                planning_status.empty()

                st.success(f"✅ Decomposed into {len(sub_tasks)} specialized sub-tasks")

                # Display sub-tasks
                with st.expander("📝 View Sub-Task Breakdown", expanded=True):
                    for idx, task in enumerate(sub_tasks, 1):
                        agent_emoji = {
                            AgentType.WEB_RESEARCHER: "🔍",
                            AgentType.CODE_EXECUTOR: "💻",
                            AgentType.CONTENT_GENERATOR: "✍️",
                            AgentType.VALIDATOR: "✅"
                        }.get(task.agent_type, "🤖")

                        st.markdown(f"**Sub-Task {idx}** {agent_emoji} `{task.agent_type.value}`")
                        st.write(f"  → {task.description}")
                        if task.dependencies:
                            st.caption(f"  Dependencies: {', '.join(task.dependencies)}")
                        st.divider()

            except Exception as e:
                planning_status.empty()
                st.error(f"❌ Task planning failed: {str(e)}")
                return {"error": f"Task planning failed: {str(e)}"}

            st.divider()

            # ===== STEP 2: Execute Sub-Tasks =====
            st.markdown("### 🚀 Executing Sub-Tasks")

            results = []

            # Add overall progress
            overall_progress = st.progress(0.0)
            overall_status = st.empty()

            for idx, sub_task in enumerate(sub_tasks, 1):
                agent_emoji = {
                    AgentType.WEB_RESEARCHER: "🔍",
                    AgentType.CODE_EXECUTOR: "💻",
                    AgentType.CONTENT_GENERATOR: "✍️",
                    AgentType.VALIDATOR: "✅"
                }.get(sub_task.agent_type, "🤖")

                st.markdown(f"#### {agent_emoji} Sub-Task {idx}/{len(sub_tasks)}: {sub_task.agent_type.value}")

                overall_status.text(f"Executing: {sub_task.description}...")

                # Track agent start
                agent_id = f"leaf_agent_{sub_task.agent_type.value}_{sub_task.id}"
                if self.tracker:
                    self.tracker.emit(
                        test_name=test_name,
                        event_type="start",
                        agent_id=agent_id,
                        agent_name=f"{sub_task.agent_type.value}",
                        agent_type="leaf_agent",
                        sub_task_id=sub_task.id
                    )

                with st.spinner(f"Executing: {sub_task.description}..."):
                    result = await supervisor.execute_single_task(sub_task)

                # Track agent completion
                if self.tracker:
                    self.tracker.emit(
                        test_name=test_name,
                        event_type="complete",
                        agent_id=agent_id,
                        agent_name=f"{sub_task.agent_type.value}",
                        agent_type="leaf_agent",
                        sub_task_id=sub_task.id,
                        success=result.success
                    )

                if result.success:
                    st.success(f"✅ {result.agent_name} completed successfully")
                    with st.expander(f"📄 View Output from {result.agent_name}"):
                        st.write(result.output)
                        if result.metadata:
                            st.caption(f"Metadata: {result.metadata}")
                else:
                    st.error(f"❌ {result.agent_name} failed: {result.error}")

                results.append(result)

                # ===== SELF-CORRECTING RESEARCH PIPELINE: Policy Update Logic =====
                # Process validation results and update memory with policy constraints
                if result.success and sub_task.agent_type == AgentType.VALIDATOR and self.memory_manager:
                    validation_result = result.metadata.get("validation_result")

                    if validation_result:
                        try:
                            # Convert to Pydantic schema for type-safe processing
                            artifact = convert_validation_to_artifact(validation_result)

                            # Display validation summary
                            st.markdown("#### 🔍 Validation Results")
                            col1, col2 = st.columns(2)
                            with col1:
                                verdict_emoji = "✅" if artifact.final_verdict == "Verified" else "⚠️"
                                st.metric("Verdict", f"{verdict_emoji} {artifact.final_verdict}")
                            with col2:
                                confidence_color = "normal" if artifact.confidence_score >= 0.7 else "inverse"
                                st.metric("Confidence", f"{artifact.confidence_score:.2%}",
                                         delta=None if artifact.confidence_score >= 0.7 else "Low confidence",
                                         delta_color=confidence_color)

                            # Show red flags if any
                            if artifact.red_flags:
                                with st.expander(f"⚠️ {len(artifact.red_flags)} Red Flag(s) Detected", expanded=True):
                                    for flag in artifact.red_flags:
                                        risk = flag.get("hallucination_risk", 0.0)
                                        risk_emoji = "🔴" if risk >= 0.8 else "🟡" if risk >= 0.5 else "🟢"
                                        st.markdown(f"{risk_emoji} **Claim:** {flag.get('claim', 'N/A')}")
                                        st.caption(f"Risk: {risk:.1%} | Reason: {flag.get('reason', 'N/A')} | Sources: {flag.get('sources_found', 0)}")
                                        st.divider()

                            # Use existing PolicyUpdater to process validation result
                            policy_updated = PolicyUpdater.process_validation_result(
                                validation_result,
                                self.memory_manager,
                                turn=idx
                            )

                            if policy_updated:
                                st.success(f"🧠 **MEMORY UPDATED!** Added {len(artifact.policy_updates)} constraint(s) to Tool Guidelines.")

                                # Display structured policy updates with severity
                                with st.expander("📋 View New Policy Constraints", expanded=True):
                                    for constraint in artifact.policy_updates:
                                        severity_emoji = {
                                            "CRITICAL": "🔴",
                                            "HIGH": "🟠",
                                            "MEDIUM": "🟡",
                                            "LOW": "🟢"
                                        }.get(constraint.severity, "⚪")

                                        st.markdown(f"{severity_emoji} **[{constraint.severity}]** {constraint.constraint_text}")
                                        st.caption(f"ID: `{constraint.constraint_id}` | Priority: {constraint.priority:.1%}")
                                        st.divider()

                                # Log to dashboard if available
                                if self.dashboard_logger:
                                    self.dashboard_logger.log_rethink_event(
                                        turn=idx,
                                        block_name="Tool Guidelines",
                                        trigger="VALIDATION_FAILURE_POLICY_UPDATE",
                                        change_summary=f"Added {len(artifact.policy_updates)} constraint(s) from validation",
                                        old_content_preview="[See Memory Inspector]",
                                        new_content_preview=f"Added constraints: {', '.join([c.constraint_id for c in artifact.policy_updates[:2]])}..."
                                    )
                            else:
                                # Validation passed or no policy updates needed
                                if artifact.confidence_score >= 0.7:
                                    st.info(f"✅ Validation passed (confidence: {artifact.confidence_score:.2%}). No policy updates needed.")

                        except Exception as e:
                            st.error(f"❌ Policy update failed: {e}")
                            st.exception(e)

                # Update progress
                overall_progress.progress((idx) / len(sub_tasks))

            overall_status.text(f"✅ All {len(sub_tasks)} sub-tasks completed")
            overall_progress.empty()

            st.divider()

            # ===== STEP 3: Synthesize Results =====
            st.markdown("### 🎯 Synthesizing Final Result")

            synthesis_status = st.empty()
            synthesis_status.text("🧠 Supervisor synthesizing results from all agents...")

            try:
                final_result = await supervisor.result_synthesizer.synthesize(
                    self.goal,
                    results
                )
                synthesis_status.empty()

                st.success("✅ Synthesis complete!")

                # Track supervisor completion
                if self.tracker:
                    self.tracker.emit(
                        test_name=test_name,
                        event_type="complete",
                        agent_id="supervisor",
                        agent_name="Supervisor Agent",
                        agent_type="supervisor",
                        total_sub_tasks=len(sub_tasks),
                        successful_tasks=final_result.metadata.get('successful_tasks', 0)
                    )

            except Exception as e:
                synthesis_status.empty()
                st.error(f"❌ Result synthesis failed: {str(e)}")
                return {"error": f"Synthesis failed: {str(e)}"}

            # Display final result
            st.markdown("### 📊 Final Synthesized Result")
            st.markdown(final_result.answer)

            # Show execution metadata
            with st.expander("📈 Execution Metadata"):
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total Agents", len(final_result.contributing_agents))
                with col2:
                    st.metric("Successful Tasks", final_result.metadata.get('successful_tasks', 0))
                with col3:
                    st.metric("Failed Tasks", final_result.metadata.get('failed_tasks', 0))

                st.json(final_result.metadata)

            # Show contributing agents
            with st.expander("👥 Contributing Agents"):
                for agent_name in final_result.contributing_agents:
                    st.write(f"  • {agent_name}")

            return {
                "strategy": "leaf_scaffold",
                "leaf_agents": [a.name for a in leaf_agents],
                "sub_tasks": len(sub_tasks),
                "final_result": final_result.answer,
                "metadata": final_result.metadata
            }

        except Exception as e:
            st.error(f"❌ Error in leaf agent scaffold execution: {str(e)}")
            st.exception(e)
            return {"error": str(e)}

    # ============================================
    # Helper Methods for Subagent Orchestration
    # ============================================

    async def _generate_inference_solution(self, subtasks: List[Task]) -> Dict:
        """Generate solution for inference mode (subagent pattern)."""
        # Delegate to existing inference logic
        best_code, best_accuracy, history = await self.run_inference_mode()
        return {
            "type": "inference",
            "code": best_code,
            "accuracy": best_accuracy,
            "history": history
        }

    async def _generate_analysis_solution(self, subtasks: List[Task]) -> Dict:
        """Generate solution for analysis mode (subagent pattern)."""
        # Delegate to existing analysis logic
        best_code, best_perf, history = await self.run_inference_mode()
        return {
            "type": "analysis",
            "code": best_code,
            "performance": best_perf,
            "history": history
        }

    async def _generate_research_solution(self, subtasks: List[Task]) -> Dict:
        """Generate solution for research mode (subagent pattern)."""
        # Execute research tasks
        results = await asyncio.gather(*[
            self.execute_research_task_smart(task) for task in subtasks
        ])
        return {
            "type": "research",
            "results": results,
            "findings_count": sum(len(r.get('findings', [])) for r in results)
        }

    async def _evaluate_solution(self, solution: Dict) -> Dict:
        """Evaluate solution quality."""
        if solution.get('type') == 'inference':
            return {
                "score": solution.get('accuracy', 0.0),
                "accuracy": solution.get('accuracy', 0.0),
                "metric": "accuracy"
            }
        elif solution.get('type') == 'analysis':
            return {
                "score": solution.get('performance', 0.0),
                "performance": solution.get('performance', 0.0),
                "metric": "performance"
            }
        elif solution.get('type') == 'research':
            findings_count = solution.get('findings_count', 0)
            score = min(findings_count / 15.0, 1.0)  # Normalize to 0-1
            return {
                "score": score,
                "findings_count": findings_count,
                "metric": "completeness"
            }
        else:
            return {"score": 0.0, "metric": "unknown"}

    async def _analyze_failures(self, solution: Dict, evaluation: Dict) -> Dict:
        """Analyze failures and suggest improvements."""
        failures = []
        suggestions = []

        if evaluation.get('score', 0.0) < 0.9:
            failures.append(f"Score below threshold: {evaluation.get('score', 0.0):.3f}")
            suggestions.append("Consider refining the approach or adding more iterations")

        return {
            "failures": failures,
            "suggestions": suggestions,
            "needs_refinement": len(failures) > 0
        }

    async def _synthesize_results(self, all_results: List[Dict]) -> Dict:
        """Synthesize all turn results into final solution."""
        if not all_results:
            return {"score": 0.0, "solution": None}

        # Get best result by score
        best_result = max(all_results, key=lambda r: r['evaluation'].get('score', 0.0))

        return {
            "strategy": "subagent",
            "best_turn": best_result['turn'],
            "solution": best_result['solution'],
            "score": best_result['evaluation'].get('score', 0.0),
            "total_turns": len(all_results)
        }

    # ============================================
    # Helper Methods for Multi-Agent Collaboration
    # ============================================

    async def _agent_propose(self, role: str, goal: str, previous_consensus: List[Dict]) -> Dict:
        """Agent proposes a solution from their perspective with robust error handling."""

        # Build context from previous consensus
        context = ""
        if previous_consensus:
            context = f"\n\nPrevious consensus (Turn {len(previous_consensus)}): {previous_consensus[-1].get('consensus', {})}"

        prompt = f"""You are a {role} agent. Propose a solution for this goal:

GOAL: {goal}

MODE: {self.mode}

Your perspective as {role}:
- Focus on your domain expertise
- Propose concrete, actionable solutions
- Consider trade-offs from your viewpoint
{context}

Return JSON with your proposal:
{{
    "approach": "Your proposed approach",
    "rationale": "Why this approach is good from your perspective",
    "key_points": ["Point 1", "Point 2", "Point 3"]
}}
"""

        # Default fallback response
        fallback_response = {
            "approach": f"Proposal from {role}",
            "rationale": "Unable to generate proposal",
            "key_points": []
        }

        if not GEMINI_API_KEY:
            st.warning(f"⚠️ {role}: No Gemini API key available")
            fallback_response["rationale"] = "API key not set"
            return fallback_response

        try:
            client = genai.Client(api_key=GEMINI_API_KEY)
            response = await asyncio.to_thread(
                lambda: client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json"
                    )
                )
            )

            # Track cost
            if st.session_state.cost_tracker:
                st.session_state.cost_tracker.update(
                    provider="Google",
                    model="gemini-2.5-flash",
                    api="generate_content",
                    raw_response_obj=response,
                    pricing_resolver=custom_gemini_price_lookup
                )

            # Parse and validate response
            try:
                proposal = json.loads(response.text)

                # Validate required fields
                required_fields = ["approach", "rationale", "key_points"]
                if not all(field in proposal for field in required_fields):
                    st.warning(f"⚠️ {role}: Incomplete proposal, using fallback")
                    # Fill in missing fields
                    for field in required_fields:
                        if field not in proposal:
                            proposal[field] = fallback_response[field]

                # Validate key_points is a list
                if not isinstance(proposal.get("key_points"), list):
                    proposal["key_points"] = []

                return proposal

            except json.JSONDecodeError as e:
                st.error(f"❌ {role}: Failed to parse JSON response: {e}")
                fallback_response["rationale"] = f"JSON parse error: {str(e)[:100]}"
                return fallback_response

        except Exception as e:
            st.error(f"❌ {role}: Proposal generation failed: {e}")
            fallback_response["rationale"] = f"Error: {str(e)[:100]}"
            return fallback_response

    async def _agent_review(self, reviewer_role: str, proposals: List[Dict]) -> Dict:
        """Agent reviews other proposals with robust error handling."""

        # Default fallback response
        fallback_response = {
            "strengths": [],
            "concerns": [],
            "suggestions": []
        }

        try:
            proposals_text = "\n\n".join([
                f"Proposal from {p.get('role', 'Unknown')}:\n{json.dumps(p.get('proposal', {}), indent=2)}"
                for p in proposals if isinstance(p, dict)
            ])
        except Exception as e:
            st.warning(f"⚠️ {reviewer_role}: Failed to format proposals: {e}")
            return fallback_response

        prompt = f"""You are a {reviewer_role} agent. Review these proposals:

{proposals_text}

Provide constructive feedback from your perspective:
- Strengths of each proposal
- Weaknesses or concerns
- Suggestions for improvement

Return JSON:
{{
    "strengths": ["Strength 1", "Strength 2"],
    "concerns": ["Concern 1", "Concern 2"],
    "suggestions": ["Suggestion 1", "Suggestion 2"]
}}
"""

        if not GEMINI_API_KEY:
            st.warning(f"⚠️ {reviewer_role}: No Gemini API key available")
            return fallback_response

        try:
            client = genai.Client(api_key=GEMINI_API_KEY)
            response = await asyncio.to_thread(
                lambda: client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json"
                    )
                )
            )

            # Track cost
            if st.session_state.cost_tracker:
                st.session_state.cost_tracker.update(
                    provider="Google",
                    model="gemini-2.5-flash",
                    api="generate_content",
                    raw_response_obj=response,
                    pricing_resolver=custom_gemini_price_lookup
                )

            # Parse and validate response
            try:
                review = json.loads(response.text)

                # Validate required fields and ensure they're lists
                for field in ["strengths", "concerns", "suggestions"]:
                    if field not in review or not isinstance(review[field], list):
                        review[field] = []

                return review

            except json.JSONDecodeError as e:
                st.error(f"❌ {reviewer_role}: Failed to parse review JSON: {e}")
                return fallback_response

        except Exception as e:
            st.error(f"❌ {reviewer_role}: Review generation failed: {e}")
            return fallback_response

    async def _build_consensus(self, proposals: List[Dict], reviews: List[Dict]) -> Dict:
        """Build consensus from all proposals and reviews."""

        proposals_text = "\n\n".join([
            f"Proposal from {p['role']}:\n{json.dumps(p['proposal'], indent=2)}"
            for p in proposals
        ])

        reviews_text = "\n\n".join([
            f"Review from {r['reviewer']}:\n{json.dumps(r['review'], indent=2)}"
            for r in reviews
        ])

        prompt = f"""Build a consensus solution by synthesizing these proposals and reviews:

PROPOSALS:
{proposals_text}

REVIEWS:
{reviews_text}

Create a consensus that:
- Incorporates the best ideas from all proposals
- Addresses concerns raised in reviews
- Balances different perspectives
- Is actionable and concrete

Return JSON:
{{
    "consensus_approach": "The agreed-upon approach",
    "key_decisions": ["Decision 1", "Decision 2", "Decision 3"],
    "incorporated_ideas": {{"Agent 1": "Idea from Agent 1", "Agent 2": "Idea from Agent 2"}},
    "addressed_concerns": ["Concern 1 addressed", "Concern 2 addressed"]
}}
"""

        if not GEMINI_API_KEY:
            return {
                "consensus_approach": "Consensus approach",
                "key_decisions": [],
                "incorporated_ideas": {},
                "addressed_concerns": []
            }

        try:
            client = genai.Client(api_key=GEMINI_API_KEY)
            response = await asyncio.to_thread(
                lambda: client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json"
                    )
                )
            )

            # Track cost
            if st.session_state.cost_tracker:
                st.session_state.cost_tracker.update(
                    provider="Google",
                    model="gemini-2.5-flash",
                    api="generate_content",
                    raw_response_obj=response,
                    pricing_resolver=custom_gemini_price_lookup
                )

            return json.loads(response.text)
        except Exception as e:
            return {
                "consensus_approach": f"Error: {str(e)}",
                "key_decisions": [],
                "incorporated_ideas": {},
                "addressed_concerns": []
            }

    async def _joint_evaluate(self, consensus: Dict, peer_roles: List[str]) -> Dict:
        """Jointly evaluate the consensus solution."""

        # Get individual evaluations from each peer
        individual_scores = []

        for role in peer_roles:
            prompt = f"""You are a {role} agent. Evaluate this consensus solution:

CONSENSUS:
{json.dumps(consensus, indent=2)}

Rate the solution from your perspective (0.0 to 1.0):
- Quality: How good is the solution?
- Completeness: Does it address all aspects?
- Feasibility: Can it be implemented?

Return JSON:
{{
    "quality": 0.8,
    "completeness": 0.9,
    "feasibility": 0.85,
    "overall": 0.85,
    "comments": "Your evaluation comments"
}}
"""

            if not GEMINI_API_KEY:
                individual_scores.append(0.5)
                continue

            try:
                client = genai.Client(api_key=GEMINI_API_KEY)
                response = await asyncio.to_thread(
                    lambda: client.models.generate_content(
                        model="gemini-2.5-flash",
                        contents=prompt,
                        config=types.GenerateContentConfig(
                            response_mime_type="application/json"
                        )
                    )
                )

                # Track cost
                if st.session_state.cost_tracker:
                    st.session_state.cost_tracker.update(
                        provider="Google",
                        model="gemini-2.5-flash",
                        api="generate_content",
                        raw_response_obj=response,
                        pricing_resolver=custom_gemini_price_lookup
                    )

                eval_result = json.loads(response.text)
                individual_scores.append(eval_result.get('overall', 0.5))
            except Exception:
                individual_scores.append(0.5)

        # Calculate agreement (how close are the scores?)
        if len(individual_scores) > 1:
            avg_score = sum(individual_scores) / len(individual_scores)
            variance = sum((s - avg_score) ** 2 for s in individual_scores) / len(individual_scores)
            agreement = max(0.0, 1.0 - variance)  # Higher agreement = lower variance
        else:
            avg_score = individual_scores[0] if individual_scores else 0.0
            agreement = 1.0

        return {
            "score": avg_score,
            "agreement": agreement,
            "individual_scores": individual_scores,
            "metric": "consensus_quality"
        }

    # ============================================
    # MODE 1: Inference (Prompt Optimization)
    # ============================================

    async def run_inference_prompt_optimization(self):
        """Inference mode: Optimize prompts for pattern matching tasks.

        Uses batched testing (map-reduce) because we're testing predictions
        on a dataset repeatedly.
        """

        st.subheader("Inference Mode: Prompt Optimization with Batched Testing")

        # Delegate to existing inference mode implementation (uses map-reduce)
        best_code, best_accuracy, history = await self.run_inference_mode()

        return {
            "strategy": "prompt_optimization",
            "best_prompt": best_code,
            "best_accuracy": best_accuracy,
            "history": history
        }

    # ============================================
    # MODE 2: Analysis (Code Generation - Single Execution)
    # ============================================

    async def run_analysis_code_generation(self):
        """Analysis mode: Generate code for computational tasks.

        Uses SINGLE execution per turn (no batching) because we're generating
        code once and executing it once to get results.
        """

        st.subheader("Analysis Mode: Code Generation (Single Execution)")

        test_name = "Test 5"
        best_code = None
        best_output = None
        best_score = 0.0

        for turn in range(1, self.budget.max_turns + 1):
            if self.budget.mode == "turns":
                self.budget.advance_turn()

            st.markdown(f"## Turn {turn} (Budget: {self.budget.left()*100:.1f}% remaining)")

            turn_id = f"analysis_turn_{turn}"

            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="start",
                    agent_id=turn_id,
                    agent_name=f"Analysis Turn {turn}",
                    agent_type="main_agent"
                )

            # Build code generation prompt
            context_str = ""
            if self.test_data:
                context_str = f"""
Available dataset:
- Size: {len(self.test_data)} items
- Sample: {json.dumps(self.test_data[:3], indent=2)}
"""

            # Include previous attempt if exists
            previous_context = ""
            if best_code:
                previous_context = f"""
Previous attempt (score: {best_score:.3f}):
```python
{best_code}
```

Previous output:
{best_output[:500] if best_output else 'No output'}

Please improve upon this solution.
"""

            code_prompt = f"""Generate Python code to accomplish this computational task:

GOAL: {self.goal}

{context_str}

{previous_context}

Requirements:
- Complete, executable code
- Use pandas, numpy, scipy, matplotlib as needed
- Print clear results to stdout
- Handle errors gracefully

Return ONLY executable Python code.
"""

            st.write(f"**Turn {turn}**: Generating computational code...")

            try:
                # Single code execution - no batching
                if not GEMINI_API_KEY:
                    st.error("GEMINI_API_KEY required for code generation")
                    break

                client = genai.Client(api_key=GEMINI_API_KEY)

                response = await asyncio.to_thread(
                    lambda: client.models.generate_content(
                        model="gemini-2.5-flash",
                        contents=code_prompt,
                        config=types.GenerateContentConfig(
                            tools=[types.Tool(code_execution=types.ToolCodeExecution)]
                        )
                    )
                )

                # Track cost
                if st.session_state.cost_tracker:
                    st.session_state.cost_tracker.update(
                        provider="Google",
                        model="gemini-2.5-flash",
                        api="generate_content",
                        raw_response_obj=response,
                        pricing_resolver=custom_gemini_price_lookup
                    )

                # Extract code and output
                generated_code = None
                execution_output = None

                for part in response.candidates[0].content.parts:
                    if part.executable_code:
                        generated_code = part.executable_code.code
                        st.success(f"Generated {len(generated_code)} chars of code")

                    if part.code_execution_result:
                        execution_output = part.code_execution_result.output

                if not generated_code:
                    st.error("No code generated")
                    continue

                # Show code
                with st.expander("View Generated Code"):
                    st.code(generated_code, language='python')

                # Show output
                if execution_output:
                    st.subheader("Execution Results")
                    st.text(execution_output[:1000])  # Limit display

                    # Score based on successful execution
                    has_error = "error" in execution_output.lower() or "traceback" in execution_output.lower()
                    current_score = 0.0 if has_error else 1.0

                    if current_score > best_score:
                        best_code = generated_code
                        best_output = execution_output
                        best_score = current_score
                        self.best_turn = turn
                        st.success(f"✅ New best solution (score: {best_score:.3f})")
                    else:
                        st.info(f"Score: {current_score:.3f} (best: {best_score:.3f})")

                # Record turn
                self.record_turn(
                    tasks_attempted=1,
                    tasks_verified=1 if current_score > 0 else 0,
                    current_best=current_score
                )

                if self.tracker:
                    self.tracker.emit(
                        test_name=test_name,
                        event_type="complete",
                        agent_id=turn_id,
                        score=current_score,
                        has_code=bool(generated_code)
                    )

                # Stop if we have a successful execution
                if best_score >= 1.0:
                    st.success("✅ Successful execution achieved!")
                    break

                # Check budget
                if self.budget.exhausted():
                    st.warning("Budget exhausted")
                    break

            except Exception as e:
                st.error(f"Code generation failed: {e}")
                if self.tracker:
                    self.tracker.emit(
                        test_name=test_name,
                        event_type="complete",
                        agent_id=turn_id,
                        error=str(e)
                    )
                continue

        return (best_code, best_score, self.turn_history)

    # ============================================
    # MODE 3: Research (Hybrid Strategy)
    # ============================================

    async def run_research_hybrid(self):
        """Research mode: Decompose into subtasks, each uses appropriate strategy."""

        st.subheader("Research Mode: Multi-Strategy Task Execution")

        test_name = "Test 5"
        all_findings = []

        for turn in range(1, self.budget.max_turns + 1):
            turn_id = f"research_turn_{turn}"

            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="start",
                    agent_id=turn_id,
                    agent_name=f"Research Turn {turn}",
                    agent_type="research_turn"
                )

            st.markdown(f"## Research Turn {turn}")

            # Decompose into subtasks
            context = {"completed": len(all_findings)}
            tasks = await self.decompose_research(self.goal, context)

            st.write(f"Generated {len(tasks)} research subtasks:")
            for task in tasks:
                st.write(f"  - {task.goal}")

            # Execute tasks with intelligent strategy selection
            results = await asyncio.gather(*[
                self.execute_research_task_smart(task)
                for task in tasks
            ])

            # Store findings
            for result in results:
                if result.get('strategy') == 'code_generation':
                    # Computational analysis result
                    await self.index.store(result, VerificationResult(
                        task_id=result['task_id'],
                        claims=[f"Computed: {result.get('computation_type', 'analysis')}"],
                        evidence={"code": result.get('code', ''), "output": result.get('output', '')},
                        verdict="verified",
                        confidence=1.0,
                        outputs=result
                    ))
                    all_findings.append({
                        "type": "computation",
                        "task": result['task_id'],
                        "output": result.get('output', ''),
                        "code": result.get('code', '')
                    })
                else:
                    # Information gathering result
                    await self.index.store(result, VerificationResult(
                        task_id=result['task_id'],
                        claims=result['findings'],
                        evidence={"sources": result['sources']},
                        verdict="verified",
                        confidence=result['confidence'],
                        outputs=result
                    ))
                    all_findings.extend(result['findings'])

            if self.tracker:
                self.tracker.emit(
                    test_name=test_name,
                    event_type="complete",
                    agent_id=turn_id,
                    agent_name=f"Research Turn {turn}",
                    agent_type="research_turn",
                    findings_count=len(all_findings),
                    tasks_completed=len(results)
                )

            # Check if sufficient information gathered
            if len(all_findings) >= 15:
                st.info("Sufficient information gathered")
                break

        # Synthesize final report
        st.divider()
        st.subheader("Research Summary")

        synthesis = await self.synthesize_research_findings(all_findings)

        st.markdown(synthesis)

        return {
            "findings_count": len(all_findings),
            "knowledge_entries": len(self.index.entries),
            "synthesis": synthesis
        }

    async def execute_research_task_smart(self, task: Task) -> Dict:
        """
        Execute research task with intelligent strategy selection.

        Decision logic:
        - Keywords: "compute", "calculate", "analyze data", "statistics" → Code generation
        - Keywords: "search", "find", "research", "what is" → Information gathering
        """

        goal_lower = task.goal.lower()

        # Check if task requires computation
        computational_keywords = [
            'compute', 'calculate', 'analyze data', 'statistics',
            'mean', 'median', 'correlation', 'regression',
            'optimize', 'simulate', 'model'
        ]

        needs_computation = any(kw in goal_lower for kw in computational_keywords)

        if needs_computation:
            st.write(f"🔢 **Computational Task**: {task.goal}")
            return await self.execute_computational_subtask(task)
        else:
            st.write(f"🔍 **Research Task**: {task.goal}")
            return await self.execute_research_task(task)

    async def execute_computational_subtask(self, task: Task) -> Dict:
        """Execute computational subtask using code generation."""

        prompt = f"""Generate Python code to accomplish this computational task:

TASK: {task.goal}

Requirements:
- Use pandas, numpy, scipy, or matplotlib as needed
- Print results to stdout
- Handle errors gracefully

Return ONLY executable Python code.
"""

        if not GEMINI_API_KEY:
            return {
                "task_id": task.id,
                "strategy": "code_generation",
                "error": "GEMINI_API_KEY not set",
                "findings": [],
                "sources": [],
                "confidence": 0.0
            }

        try:
            client = genai.Client(api_key=GEMINI_API_KEY)

            response = await asyncio.to_thread(
                lambda: client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
                    )
                )
            )

            # Extract code and output
            code = None
            output = None

            for part in response.candidates[0].content.parts:
                if part.executable_code:
                    code = part.executable_code.code
                if part.code_execution_result:
                    output = part.code_execution_result.output

            return {
                "task_id": task.id,
                "strategy": "code_generation",
                "computation_type": "analysis",
                "code": code,
                "output": output,
                "findings": [f"Computation completed: {task.goal}"],
                "sources": ["Gemini Code Execution"],
                "confidence": 1.0
            }

        except Exception as e:
            return {
                "task_id": task.id,
                "strategy": "code_generation",
                "error": str(e),
                "findings": [],
                "sources": [],
                "confidence": 0.0
            }

    async def synthesize_research_findings(self, findings: List[Dict]) -> str:
        """Synthesize research findings into coherent report."""

        prompt = f"""Synthesize these research findings into a coherent report:

ORIGINAL GOAL: {self.goal}

FINDINGS:
{json.dumps(findings, indent=2)}

Create a structured report with:
- Executive summary
- Key findings organized by topic
- Computational results (if any)
- Sources and confidence levels
- Recommendations or next steps
"""

        if not GEMINI_API_KEY:
            return "Error: GEMINI_API_KEY not set"

        try:
            client = genai.Client(api_key=GEMINI_API_KEY)

            synthesis = await asyncio.to_thread(
                lambda: client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=prompt
                )
            )

            return synthesis.text

        except Exception as e:
            return f"Error synthesizing findings: {str(e)}"

# --- PATCH 4: Structured Summary Helper (Point 4) ---

async def get_structured_summary_and_refinement(test_report: str, refinement_code: Optional[str] = None) -> TestSummaryAndRefinement:
    """Generates the structured summary and refinement suggestions using LLM."""

    SYSTEM_PROMPT = "You are an expert AI performance analyst and prompt engineer. Analyze the provided test report and suggest actionable improvements, providing the refined Python code or prompt structure if applicable."
    SUMMARY_SCHEMA = TestSummaryAndRefinement.model_json_schema()

    # PATCH 14: Create a strong, explicit instruction set naming the required Pydantic keys
    instruction_text = """
    Analyze the test report and suggest improvements.
    You MUST return JSON with the following required keys:
    1. 'findings_summary' (string): A concise summary of the test results and model performance.
    2. 'key_suggestions' (array of strings): Actionable suggestions for prompt, architecture, or tool improvement.
    3. 'suggested_improvement_code' (optional string): The refined Python code, agent framework, or prompt string.
    4. 'suggested_improvement_prompt_reasoning' (optional string): Detailed reasoning for the refinement.
    Ensure your JSON output strictly adheres to these exact key names.
    """

    payload = {
        "test_report": test_report,
        "refinement_artifact": refinement_code if refinement_code else "N/A",
        "instructions": instruction_text # Use the strict, key-naming instruction set
    }

    try:
        if use_openai and OPENAI_API_KEY:
            client = AsyncOpenAI(api_key=OPENAI_API_KEY)
            # The patched openai_structured_json ensures 'JSON' keyword is used,
            # and this strong instruction set ensures correct key names.
            raw_result = await openai_structured_json(client, OPENAI_MODEL, SYSTEM_PROMPT, payload)
        elif use_ollama and OPENROUTER_API_KEY:
            # OpenRouter (which is typically more reliable with schemas)
            raw_result = await openrouter_json(OPENROUTER_MODEL, SYSTEM_PROMPT, payload, "summary_refinement", SUMMARY_SCHEMA)
        else:
            return TestSummaryAndRefinement(findings_summary="No LLM provider configured for summarization.", key_suggestions=["Check API keys."], suggested_improvement_code=None)

        # Pydantic validation now runs on the (hopefully) correct keys
        return TestSummaryAndRefinement.model_validate(raw_result)

    except Exception as e:
        # Keep the existing robust error handling
        st.error(f"Error generating structured summary: {e}")
        # Note: If validation fails again, it means the LLM is ignoring the instructions entirely.
        return TestSummaryAndRefinement(findings_summary=f"Failed to generate structured summary due to error: {e}", key_suggestions=[], suggested_improvement_code=None)

async def display_final_summary_for_test(test_name: str, report_text: str, artifact: Optional[str] = None):
    st.divider()
    st.subheader(f"Final Analysis & Refinement for {test_name}")

    summary_result = await get_structured_summary_and_refinement(report_text, artifact)

    st.markdown(f"**Summary:** {summary_result.findings_summary}")
    st.markdown("**Key Suggestions:**")
    st.json(summary_result.key_suggestions)

    if summary_result.suggested_improvement_code:
        st.subheader("✨ Refined Prompt/Code Suggestion")
        # Determine if it's code or just a prompt
        lang = 'python' if 'class' in summary_result.suggested_improvement_code or 'def ' in summary_result.suggested_improvement_code else 'markdown'
        st.code(summary_result.suggested_improvement_code, language=lang)
    if summary_result.suggested_improvement_prompt_reasoning:
        st.subheader("💡 Reasoning for Refinement")
        st.markdown(summary_result.suggested_improvement_prompt_reasoning)

# ------------------------------------------------------------

def run_classification_flow(
    include_third_model: bool = False,
    use_openai_override: Optional[bool] = None,
    use_ollama_override: Optional[bool] = None,
    openrouter_model_override: Optional[str] = None,
    use_ollama_local_override: Optional[bool] = None,
    ollama_base_url_override: Optional[str] = None,
    ollama_model_override: Optional[str] = None,
    third_kind_override: Optional[str] = None,
    third_model_override: Optional[str] = None,
):
    """
    Centralized function to run the async classification process.
    Reads per-test settings (overrides) and updates the session state DataFrame.

    Args:
        include_third_model (bool): If True, includes the third model in the run.
        *_override: Per-test overrides for provider toggles and model IDs.
    """
    # Effective toggles/models: per-test overrides take precedence over sidebar/global
    use_openai_eff = use_openai_override if use_openai_override is not None else use_openai
    use_ollama_eff = use_ollama_override if use_ollama_override is not None else use_ollama
    use_ollama_local_eff = use_ollama_local_override if use_ollama_local_override is not None else use_ollama_local
    openrouter_model_eff = openrouter_model_override if openrouter_model_override else OPENROUTER_MODEL
    ollama_base_url_eff = ollama_base_url_override if ollama_base_url_override else OLLAMA_BASE_URL
    ollama_model_eff = ollama_model_override if ollama_model_override else OLLAMA_MODEL
    third_kind_eff = third_kind_override if third_kind_override is not None else (THIRD_KIND if include_third_model else "None")
    third_model_eff = third_model_override if third_model_override is not None else (THIRD_MODEL if include_third_model else "")

    # Check if any provider is selected for the current run
    provider_enabled = (
        use_openai_eff or use_ollama_eff or use_ollama_local_eff or
        (include_third_model and third_kind_eff != "None")
    )
    if not provider_enabled:
        st.warning("Please enable at least one provider in the test's configuration to run classification.")
        return  # Stop execution if no models are configured to run

    # Track start time for history
    start_time = time.time()

    # Initialize test history if needed
    if 'test_history' not in st.session_state:
        st.session_state.test_history = []

    loop = asyncio.get_event_loop()
    with st.spinner("Classifying... this may take a few minutes."):
        try:
            df_run = _subset_for_run(st.session_state.df, ROW_LIMIT_N).copy()

            out_df = loop.run_until_complete(
                _classify_df_async(
                    df_run,
                    use_openai_eff,
                    use_ollama_eff,
                    openrouter_model_eff,
                    use_ollama_local_eff,
                    ollama_base_url_eff,
                    ollama_model_eff,
                    third_kind=third_kind_eff,
                    third_model=third_model_eff,
                )
            )

            # Robustly update the main DataFrame
            st.session_state.df.loc[out_df.index, out_df.columns] = out_df
            st.success(f"Classification complete on {len(df_run)} rows.")

            # Track test completion in history
            duration = time.time() - start_time
            metadata = st.session_state.last_progress_metadata.get('classification', {})
            batch_count = len(metadata.get('batch_timestamps', []))

            # Count active models (effective settings)
            model_count = sum([use_openai_eff, use_ollama_eff, use_ollama_local_eff, include_third_model and third_kind_eff != "None"])

            st.session_state.test_history.append({
                'name': 'Classification Test',
                'icon': '🔍',
                'description': f'Classified {len(df_run)} items across {model_count} model(s)',
                'status': 'complete',
                'duration': duration,
                'batch_count': batch_count,
                'cost': st.session_state.cost_tracker.totals.get('total_cost_usd', 0.0),
                'timestamp': time.time()
            })

        except Exception as e:
            st.error(f"An error occurred during classification: {e}")
            st.exception(e) # Show full traceback for better debugging

            # Track error in history
            duration = time.time() - start_time
            st.session_state.test_history.append({
                'name': 'Classification Test',
                'icon': '🔍',
                'description': f'Error: {str(e)[:50]}...',
                'status': 'error',
                'duration': duration,
                'batch_count': 0,
                'cost': 0.0,
                'timestamp': time.time()
            })


# ---------- Model Callers (OpenAI, Ollama, OpenRouter) ----------
async def classify_with_openai(text: str, allowed: List[str]) -> ClassificationWithConf:
    """
    Classify text using OpenAI with structured output.
    Routes through OpenRouter or native API based on API_ROUTING_MODE.
    Respects per-test override model via st.session_state['openai_model_override'] if set.
    """
    # Determine selected model (per-test override takes precedence)
    selected_oai_model = st.session_state.get('openai_model_override', OPENAI_MODEL)

    # Route through OpenRouter if configured
    if API_ROUTING_MODE == "openrouter":
        return await classify_with_openrouter(
            text, allowed,
            model=_to_openrouter_model_id(selected_oai_model, "openai")
        )

    # Use native OpenAI API
    if not OPENAI_API_KEY:
        raise ValueError("OPENAI_API_KEY not set")

    # Convert to native model ID (strip provider prefix if present)
    native_model = _to_native_model_id(selected_oai_model)

    async with _rate_limiter:
        client = AsyncOpenAI(api_key=OPENAI_API_KEY)
        allowed_hint = f"Allowed labels: {allowed if allowed else '[unconstrained]'}.\nPick exactly ONE of these values in 'classification_result'."
        try:
            resp = await client.chat.completions.parse(
                model=native_model,
                messages=[
                    {"role": "system", "content": "Return a structured classification with optional confidence 0..1."},
                    {"role": "user", "content": f"{allowed_hint}\nText: {text}\nRespond as JSON with keys: classification_result, rationale, confidence (0..1 optional)."}
                ],
                response_format=ClassificationWithConf
            )
            # Track the API call
            st.session_state.cost_tracker.update(
                provider="OpenAI", model=native_model, api="chat.completions.parse",
                raw_response_obj=resp, pricing_resolver=combined_price_lookup
            )
            parsed = resp.choices[0].message.parsed
            return parsed if isinstance(parsed, ClassificationWithConf) else ClassificationWithConf.model_validate(parsed)
        except Exception:
            comp = await client.chat.completions.create(
                model=native_model,
                messages=[
                    {"role": "system", "content": "Respond ONLY with JSON having keys classification_result, rationale, confidence (0..1 optional)."},
                    {"role": "user", "content": f"{allowed_hint}\nText: {text}"}
                ],
            )
            # Track the fallback API call
            st.session_state.cost_tracker.update(
                provider="OpenAI", model=native_model, api="chat.completions.create",
                raw_response_obj=comp, pricing_resolver=combined_price_lookup
            )
            content = comp.choices[0].message.content or "{}"
            if content.strip().startswith("```"):
                content = content.strip().split("\n", 1)[1].rsplit("```", 1)[0]
            data = json.loads(content)
            if "confidence" in data:
                try:
                    data["confidence"] = float(data["confidence"])
                except Exception:
                    data["confidence"] = None
            return ClassificationWithConf.model_validate(data)

async def openai_structured_json(client: AsyncOpenAI, model: str, system: str, user_jsonable: Any) -> Dict[str, Any]:
    # Ensure the system prompt includes an explicit request for JSON
    enhanced_system = system + " Respond ONLY with the requested JSON object."

    # Always use the native model ID with the OpenAI SDK
    native_model = _to_native_model_id(model)

    comp = await client.chat.completions.create(
        model=native_model,
        messages=[
            {"role":"system","content":enhanced_system},
            {"role":"user","content":json.dumps(user_jsonable, indent=2)}
        ],
        response_format={"type":"json_object"}
    )
    # Track the API call
    st.session_state.cost_tracker.update(
        provider="OpenAI", model=native_model, api="chat.completions.create",
        raw_response_obj=comp, pricing_resolver=combined_price_lookup
    )
    content = comp.choices[0].message.content or "{}"
    return json.loads(content)


# --- PATCH 28: Gemini Classification Function (with routing support) ---
async def classify_with_gemini(text: str, allowed: List[str], model: str) -> ClassificationWithConf:
    """
    Classify text using Google Gemini with structured output.
    Routes through OpenRouter or native API based on API_ROUTING_MODE.
    """
    # Route through OpenRouter if configured
    if API_ROUTING_MODE == "openrouter":
        return await classify_with_openrouter(text, allowed, model=_to_openrouter_model_id(model, "google"))

    # Use native Google Genai API
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY not set.")

    # Convert to native model ID (strip provider prefix if present)
    native_model = _to_native_model_id(model)

    # CRITICAL FIX: Create client without async context manager
    # genai.Client doesn't support async context managers
    async with _rate_limiter:
        try:
            client = genai.Client(api_key=GEMINI_API_KEY)

            schema_dict = ClassificationWithConf.model_json_schema()

            # System prompt enforces the schema and allowed labels
            system_prompt = f"You are a text classifier. Return ONLY a single JSON object that strictly adheres to the provided schema. The 'classification_result' MUST be one of the following: {allowed if allowed else 'any string'}."

            def sync_api_call():
                return client.models.generate_content(
                    model=native_model,
                    contents=[
                        types.Content(parts=[types.Part.from_text(text=system_prompt)]),
                        types.Content(parts=[types.Part.from_text(text=f"Text to classify: {text}")]),
                    ],
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json",
                        response_schema=schema_dict
                    ),
                )

            response = await asyncio.to_thread(sync_api_call)

            # Track the API call
            st.session_state.cost_tracker.update(
                provider="Google", model=native_model, api="generate_content",
                raw_response_obj=response, pricing_resolver=custom_gemini_price_lookup
            )

            data = json.loads(response.text)
            return ClassificationWithConf.model_validate(data)

        except Exception as e:
            return ClassificationWithConf(
                classification_result="",
                rationale=f"Gemini Error: {e}",
                confidence=None
            )
# ---------------------------------------------------------------


async def classify_with_ollama(base_url: str, model: str, text: str, allowed: List[str]) -> ClassificationWithConf:
    root, chat_url = _normalize_ollama_root(base_url), _normalize_ollama_root(base_url) + "/api/generate"
    schema = {"type": "object", "properties": {"classification_result": ({"type": "string", "enum": allowed} if allowed else {"type": "string"}), "rationale": {"type": "string"}, "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}}, "required": ["classification_result", "rationale"], "additionalProperties": False}
    messages = [{"role": "system", "content": "You only reply with JSON that matches the provided schema."}, {"role": "user", "content": f"Allowed labels: {allowed if allowed else '[unconstrained]'}\nText: {text}"}]
    async with _rate_limiter, httpx.AsyncClient(timeout=120) as hc:
        resp = await hc.post(chat_url, json={"model": model, "messages": messages, "stream": False, "format": schema})
        resp.raise_for_status()
        data = resp.json()
        # Track the API call
        st.session_state.cost_tracker.update(
            provider="Ollama", model=model, api="chat",
            raw_response_json=data, pricing_resolver=combined_price_lookup
        )
        content = (data.get("message", {}) or {}).get("content", "") if isinstance(data, dict) else ""
        try:
            obj = json.loads(content) if content else data
            if "confidence" in obj:
                try: obj["confidence"] = float(obj["confidence"])
                except Exception: obj["confidence"] = None
            return ClassificationWithConf.model_validate(obj)
        except ValidationError as ve:
            raise ValueError(f"Ollama validation error: {ve}")

async def ollama_json(base_url: str, model: str, system: str, user_jsonable: Any) -> Dict[str, Any]:
    body = {"model": model, "messages": [{"role":"system","content":system}, {"role":"user","content":json.dumps(user_jsonable, indent=2)}], "stream": False, "format": "json"}
    async with httpx.AsyncClient(timeout=120) as hc:
        r = await hc.post(_normalize_ollama_root(base_url) + "/api/generate", json=body)
        r.raise_for_status()
        data = r.json()
        # Track the API call
        st.session_state.cost_tracker.update(
            provider="Ollama", model=model, api="generate",
            raw_response_json=data, pricing_resolver=combined_price_lookup
        )
        txt = (data.get("message", {}) or {}).get("content") or data.get("response") or "{}"
        return json.loads(txt)

async def classify_with_openrouter(text: str, allowed: List[str], model: Optional[str] = None) -> ClassificationWithConf:
    if not OPENROUTER_API_KEY: raise ValueError("OPENROUTER_API_KEY not set")
    messages = [{"role": "system", "content": ("Return ONLY compact JSON with keys: classification_result, rationale, confidence (0..1 optional)." + (f" Allowed labels: {allowed}." if allowed else ""))}, {"role": "user", "content": text}]
    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
    async with _rate_limiter, httpx.AsyncClient(timeout=120) as hc:
        body1 = {"model": model or OPENROUTER_MODEL, "messages": messages}
        try:
            r1 = await hc.post(OPENROUTER_URL, headers=headers, json=body1)
            r1.raise_for_status()
            data1 = r1.json()
            # Track the API call
            st.session_state.cost_tracker.update(
                provider="OpenRouter", model=model or OPENROUTER_MODEL, api="chat.completions",
                raw_response_json=data1, pricing_resolver=custom_openrouter_price_lookup
            )
            content1 = (((data1 or {}).get("choices") or [{}])[0].get("message") or {}).get("content", "{}")
            if isinstance(content1, str):
                if content1.strip().startswith("```"): content1 = content1.strip().split("\n", 1)[1].rsplit("```", 1)[0]
                obj = json.loads(content1)
            else: obj = content1
            if "confidence" in obj:
                try: obj["confidence"] = float(obj["confidence"])
                except Exception: obj["confidence"] = None
            return ClassificationWithConf.model_validate(obj)
        except Exception as e1:
            schema = {"type": "object", "properties": {"classification_result": ({"type": "string", "enum": allowed} if allowed else {"type": "string"}), "rationale": {"type": "string"}, "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}}, "required": ["classification_result", "rationale"], "additionalProperties": False}
            body2 = {"model": model or OPENROUTER_MODEL, "messages": messages, "response_format": {"type": "json_schema", "json_schema": {"name": "classification", "strict": True, "schema": schema}}}
            try:
                r2 = await hc.post(OPENROUTER_URL, headers=headers, json=body2)
                r2.raise_for_status()
                data2 = r2.json()
                # Track the fallback API call
                st.session_state.cost_tracker.update(
                    provider="OpenRouter", model=model or OPENROUTER_MODEL, api="chat.completions",
                    raw_response_json=data2, pricing_resolver=custom_openrouter_price_lookup
                )
                content2 = (((data2 or {}).get("choices") or [{}])[0].get("message") or {}).get("content", "{}")
                obj2 = json.loads(content2) if isinstance(content2, str) else content2
                if "confidence" in obj2:
                    try: obj2["confidence"] = float(obj2["confidence"])
                    except Exception: obj2["confidence"] = None
                return ClassificationWithConf.model_validate(obj2)
            except Exception as e2:
                return ClassificationWithConf(classification_result="", rationale=f"OpenRouter error: {e1} | {e2}", confidence=None)

async def openrouter_json(model: str, system: str, user_jsonable: Any, schema_name: str, schema: Dict[str, Any]) -> Dict[str, Any]:
    if not OPENROUTER_API_KEY:
        raise ValueError("OPENROUTER_API_KEY not set")

    # Base headers + optional referral metadata (per OpenRouter docs)
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }
    _ref = st.secrets.get("OPENROUTER_HTTP_REFERER") or st.secrets.get("SITE_URL")
    _title = st.secrets.get("OPENROUTER_X_TITLE") or st.secrets.get("SITE_NAME")
    if _ref:
        headers["HTTP-Referer"] = _ref
    if _title:
        headers["X-Title"] = _title

    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": json.dumps(user_jsonable, indent=2)},
    ]

    # First try: JSON schema format (omit 'strict' to maximize compatibility)
    body_schema = {
        "model": model,
        "messages": messages,
        "response_format": {
            "type": "json_schema",
            "json_schema": {
                "name": schema_name,
                "schema": schema,
            },
        },
    }

    async with httpx.AsyncClient(timeout=120) as hc:
        try:
            r = await hc.post(OPENROUTER_URL, headers=headers, json=body_schema)
            r.raise_for_status()
            data = r.json()
            st.session_state.cost_tracker.update(
                provider="OpenRouter", model=model, api="chat.completions",
                raw_response_json=data, pricing_resolver=custom_openrouter_price_lookup,
            )
            content = (((data or {}).get("choices") or [{}])[0].get("message") or {}).get("content", "{}")
            return json.loads(content) if isinstance(content, str) else content
        except httpx.HTTPStatusError as e:
            # Fallback for 400/422 or other request validation errors: use json_object
            status = getattr(e.response, "status_code", None)
            if status not in (400, 422):
                raise

            body_fallback = {
                "model": model,
                "messages": messages,
                "response_format": {"type": "json_object"},
            }
            r2 = await hc.post(OPENROUTER_URL, headers=headers, json=body_fallback)
            r2.raise_for_status()
            data2 = r2.json()
            st.session_state.cost_tracker.update(
                provider="OpenRouter", model=model, api="chat.completions",
                raw_response_json=data2, pricing_resolver=custom_openrouter_price_lookup,
            )
            content2 = (((data2 or {}).get("choices") or [{}])[0].get("message") or {}).get("content", "{}")
            return json.loads(content2) if isinstance(content2, str) else content2

async def generate_text_async(prompt: str) -> str:
    """Generates text using the first available provider (prioritizes OpenRouter)."""
    # Prioritize OpenRouter if enabled
    if use_ollama and OPENROUTER_API_KEY:
        try:
            headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}"}
            messages = [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": prompt}]
            body = {"model": OPENROUTER_MODEL, "messages": messages, "max_tokens": 512}
            async with httpx.AsyncClient(timeout=60) as hc:
                r = await hc.post(OPENROUTER_URL, headers=headers, json=body)
                r.raise_for_status()
                data = r.json()
                # Track the API call
                st.session_state.cost_tracker.update(
                    provider="OpenRouter", model=OPENROUTER_MODEL, api="chat.completions",
                    raw_response_json=data, pricing_resolver=custom_openrouter_price_lookup
                )
                return data.get("choices", [{}])[0].get("message", {}).get("content", "Error: No content.")
        except Exception as e:
            # If OpenRouter fails, fall through to OpenAI if it's enabled
            if not (use_openai and OPENAI_API_KEY):
                return f"Error during OpenRouter call: {e}"

    # Fallback to OpenAI if enabled
    if use_openai and OPENAI_API_KEY:
        try:
            client = AsyncOpenAI(api_key=OPENAI_API_KEY)
            comp = await client.chat.completions.create(model=OPENAI_MODEL, messages=[{"role": "user", "content": prompt}], max_tokens=512)
            # Track the API call
            st.session_state.cost_tracker.update(
                provider="OpenAI", model=OPENAI_MODEL, api="chat.completions.create",
                raw_response_obj=comp, pricing_resolver=combined_price_lookup
            )
            return comp.choices[0].message.content or "Error: No content."
        except Exception as e:
            return f"Error during OpenAI call: {e}"

    return "Error: No text generation provider is configured or enabled."


# ---------- Core Runner (UPDATED FOR LATENCY) ----------
async def _classify_df_async(df: pd.DataFrame, use_openai: bool, use_ollama: bool, openrouter_model: str, use_ollama_local: bool = False, ollama_base_url: str = "", ollama_model: str = "", third_kind: str = "None", third_model: str = "") -> pd.DataFrame:
    labels = _allowed_labels(df)
    for col in SKELETON_COLUMNS:
        if col not in df.columns:
            df[col] = None

    # This inner worker function remains the same
    async def worker(idx: int, text: str):
        oai_res = {"result": None, "latency": 0.0}
        oll_res = {"result": None, "latency": 0.0}
        thd_res = {"result": None, "latency": 0.0}
        gmn_res = {"result": None, "latency": 0.0}  # PATCH 28: Gemini Result

        async def run_task(task_fn, result_dict):
            start_time = time.monotonic()
            try: result_dict["result"] = await _retry(task_fn)
            except Exception as e: result_dict["result"] = ClassificationWithConf(classification_result="", rationale=f"Error: {e}", confidence=None)
            result_dict["latency"] = time.monotonic() - start_time

        tasks_to_run = []
        if use_openai: tasks_to_run.append(run_task(lambda: classify_with_openai(text, labels), oai_res))
        if use_ollama:
            async def openrouter_task():
                try: return await _retry(lambda: classify_with_openrouter(text, labels, model=openrouter_model))
                except Exception as e1:
                    if use_ollama_local:
                        try: return await _retry(lambda: classify_with_ollama(ollama_base_url, ollama_model, text, labels))
                        except Exception as e2: raise Exception(f"OpenRouter failed: {e1}; Local Ollama failed: {e2}")
                    raise e1
            tasks_to_run.append(run_task(openrouter_task, oll_res))
        elif use_ollama_local:
            tasks_to_run.append(run_task(lambda: classify_with_ollama(ollama_base_url, ollama_model, text, labels), oll_res))

        # PATCH 28: Add Gemini task if enabled
        if st.session_state.get('use_gemini'):
            gemini_model = st.session_state.get('gemini_model', 'gemini-2.5-flash')
            tasks_to_run.append(run_task(lambda: classify_with_gemini(text, labels, gemini_model), gmn_res))

        if third_kind != "None":
            async def third_task_fn():
                if third_kind == "OpenAI":
                    client = AsyncOpenAI(api_key=OPENAI_API_KEY)
                    comp = await client.chat.completions.create(model=third_model, messages=[{"role": "system", "content": "Respond ONLY with JSON having keys classification_result, rationale, confidence (0..1 optional)."}, {"role": "user", "content": text}])
                    content = comp.choices[0].message.content or "{}"
                    if content.strip().startswith("```"): content = content.strip().split("\n", 1)[1].rsplit("```", 1)[0]
                    data = json.loads(content)
                    if "confidence" in data:
                        try: data["confidence"] = float(data["confidence"])
                        except Exception: data["confidence"] = None
                    return ClassificationWithConf.model_validate(data)
                elif third_kind == "OpenRouter":
                    return await classify_with_openrouter(text, labels, model=third_model)
            tasks_to_run.append(run_task(third_task_fn, thd_res))

        await asyncio.gather(*tasks_to_run)  # Gather all results
        return idx, oai_res, oll_res, thd_res, gmn_res  # PATCH 28: Return Gemini results

    # --- BATCHING LOGIC WITH PROGRESS METADATA COLLECTION ---

    # Get execution tracker
    tracker = st.session_state.get('execution_tracker')
    test_name = "Classification Test"  # Will be set dynamically based on which test is running

    # Emit start event
    if tracker:
        tracker.emit(test_name, "start", "classification_run", "Classification Run",
                    "orchestrator", total_items=len(df))

    # Define a reasonable batch size. 100 is safe for most systems.
    BATCH_SIZE = 100

    # Create a list of all coroutines to be run, but don't schedule them yet.
    all_coroutines = [worker(idx, q) for idx, row in df.iterrows() if (q := str(row.get("query", "")).strip())]

    if not all_coroutines:
        return df

    progress = st.progress(0.0, text="Running classification...")
    all_results = []
    total_coroutines = len(all_coroutines)

    # Collect progress metadata for replay visualization
    progress_metadata = {
        'batch_timestamps': [],
        'batch_sizes': [],
        'cumulative_counts': [],
        'batch_latencies': [],
        'success_rates': []
    }

    start_time = time.time()

    # Process the coroutines in batches
    for i in range(0, total_coroutines, BATCH_SIZE):
        batch_num = i // BATCH_SIZE + 1
        batch_id = f"batch_{batch_num}"
        batch_start = time.time()

        # Emit batch start
        if tracker:
            tracker.emit(test_name, "start", batch_id, f"Batch {batch_num}",
                        "batch", parent_id="classification_run",
                        batch_size=min(BATCH_SIZE, total_coroutines - i))

        batch_coroutines = all_coroutines[i:i + BATCH_SIZE]

        # Run only the current batch
        batch_results = await asyncio.gather(*batch_coroutines)
        all_results.extend(batch_results)

        batch_end = time.time()

        # Collect metadata
        completed_count = len(all_results)
        batch_latency = batch_end - batch_start

        # Calculate success rate for this batch
        successful = sum(1 for _, oai, oll, thd, gmn in batch_results
                        if (oai["result"] is not None or oll["result"] is not None or
                            thd["result"] is not None or gmn["result"] is not None))
        success_rate = successful / len(batch_results) if batch_results else 0.0

        progress_metadata['batch_timestamps'].append(batch_end - start_time)
        progress_metadata['batch_sizes'].append(len(batch_results))
        progress_metadata['cumulative_counts'].append(completed_count)
        progress_metadata['batch_latencies'].append(batch_latency)
        progress_metadata['success_rates'].append(success_rate)

        # Emit batch complete
        if tracker:
            tracker.emit(test_name, "complete", batch_id, f"Batch {batch_num}",
                        "batch", parent_id="classification_run",
                        items_processed=len(batch_results),
                        success_rate=success_rate,
                        latency=batch_latency)

        # Update progress after each batch completes
        progress.progress(completed_count / total_coroutines, text=f"Running classification... ({completed_count}/{total_coroutines})")

    progress.empty()

    # Store progress metadata in session state for replay
    if 'last_progress_metadata' not in st.session_state:
        st.session_state.last_progress_metadata = {}
    st.session_state.last_progress_metadata['classification'] = progress_metadata

    # Emit completion
    if tracker:
        tracker.emit(test_name, "complete", "classification_run", "Classification Run",
                    "orchestrator", total_items=len(df),
                    total_batches=len(progress_metadata['batch_timestamps']))

    # --- RESULT PROCESSING ---
    for idx, oai_res, oll_res, thd_res, gmn_res in all_results:  # PATCH 28: Process Gemini results
        if (oll := oll_res["result"]) is not None:
            df.loc[idx, "classification_result_openrouter_mistral"] = _normalize_label(str(oll.classification_result or ""))
            df.loc[idx, "classification_result_openrouter_mistral_rationale"] = oll.rationale or "(no rationale)"
            df.loc[idx, "classification_result_openrouter_mistral_confidence"] = oll.confidence
            df.loc[idx, "latency_openrouter_mistral"] = oll_res["latency"]
        if (oai := oai_res["result"]) is not None:
            df.loc[idx, "classification_result_openai"] = _normalize_label(str(oai.classification_result or ""))
            df.loc[idx, "classification_result_openai_rationale"] = oai.rationale or "(no rationale)"
            df.loc[idx, "classification_result_openai_confidence"] = oai.confidence
            df.loc[idx, "latency_openai"] = oai_res["latency"]
        if (gmn := gmn_res["result"]) is not None:  # PATCH 28: Write Gemini results
            df.loc[idx, "classification_result_gemini"] = _normalize_label(str(gmn.classification_result or ""))
            df.loc[idx, "classification_result_gemini_rationale"] = gmn.rationale or "(no rationale)"
            df.loc[idx, "classification_result_gemini_confidence"] = gmn.confidence
            df.loc[idx, "latency_gemini"] = gmn_res["latency"]
        if (thd := thd_res["result"]) is not None:
            df.loc[idx, "classification_result_third"] = _normalize_label(str(thd.classification_result or ""))
            df.loc[idx, "classification_result_third_rationale"] = thd.rationale or "(no rationale)"
            df.loc[idx, "classification_result_third_confidence"] = thd.confidence
            df.loc[idx, "latency_third"] = thd_res["latency"]

    return df

# ---------- NEW: Smarter Weighted Pick (Test 2) ----------
def _smarter_weighted_pick_row(row: pd.Series, f1_maps: Dict[str, Dict[str, float]]) -> Tuple[Optional[str], Optional[str]]:
    scores = {}
    model_preds = {
        "mistral": {"pred": row.get("classification_result_openrouter_mistral"), "conf": row.get("classification_result_openrouter_mistral_confidence")},
        "gpt5": {"pred": row.get("classification_result_openai"), "conf": row.get("classification_result_openai_confidence")},
        "third": {"pred": row.get("classification_result_third"), "conf": row.get("classification_result_third_confidence")}
    }

    for model, data in model_preds.items():
        pred = _normalize_label(data["pred"])
        if pred and model in f1_maps:
            conf = float(data["conf"] or 0.0)
            class_f1 = f1_maps[model].get(pred, 0.0) # Use F1 for the predicted class
            scores[model] = class_f1 * conf

    if not scores:
        return None, None

    model_pick = max(scores, key=scores.get)
    label_pick = model_preds[model_pick]["pred"]
    return model_pick, label_pick

# --- UPDATED: Flexible Judge and Pruner Functions ---
JUDGE_SCHEMA = {"type": "object","properties": {"final_choice_model": {"type": "string", "description": "One of: mistral, gpt5, third"}, "final_label": {"type": "string"}, "judge_rationale": {"type": "string"}},"required": ["final_choice_model", "final_label", "judge_rationale"], "additionalProperties": False}
JUDGE_INSTRUCTIONS = "You are a neutral judge... Return ONLY JSON with: final_choice_model, final_label, judge_rationale."

async def run_judge_flexible(payload: Dict[str, Any], model: str = None) -> Dict[str, Any]:
    """Run judge with flexible model selection."""
    if model is None:
        model = st.session_state.get('judge_model', 'openai/gpt-5-mini')

    # Route based on API_ROUTING_MODE or provider
    provider = _get_provider_from_model_id(model)

    if API_ROUTING_MODE == "openrouter" or provider in ["mistralai", "anthropic", "meta-llama", "deepseek"]:
        # Use OpenRouter
        return await openrouter_json(_to_openrouter_model_id(model), JUDGE_INSTRUCTIONS, payload, "judge", JUDGE_SCHEMA)
    elif provider == "openai":
        # Use native OpenAI
        native_model = _to_native_model_id(model)
        return await openai_structured_json(AsyncOpenAI(api_key=OPENAI_API_KEY), native_model, JUDGE_INSTRUCTIONS, payload)
    else:
        # Default to OpenRouter
        return await openrouter_json(_to_openrouter_model_id(model), JUDGE_INSTRUCTIONS, payload, "judge", JUDGE_SCHEMA)

# Legacy functions for backward compatibility
async def run_judge_ollama(payload: Dict[str, Any]) -> Dict[str, Any]:
    return await run_judge_flexible(payload, st.session_state.get('judge_model', OPENROUTER_MODEL))

async def run_judge_openai(payload: Dict[str, Any]) -> Dict[str, Any]:
    return await run_judge_flexible(payload, st.session_state.get('judge_model', OPENAI_MODEL))
PRUNER_SCHEMA = {
    "type": "object",
    "properties": {
        "kept_context_keys": {
            "type": "array",
            "description": "An array of essential context keys to keep.",
            "items": {
                "type": "string",
                "enum": ["instruction", "summary", "user_messages", "agent_responses", "tool_logs"]
            }
        },
        "action": {
            "type": "string",
            "enum": ["general_answer", "kb_lookup", "tool_call"],
            "description": "The best next action to take."
        },
        "prune_rationale": {
            "type": "string",
            "description": "Briefly explain why you chose these keys and this action."
        }
    },
    "required": ["kept_context_keys", "action", "prune_rationale"],
    "additionalProperties": False
}

PRUNER_INSTRUCTIONS = """
You are an expert AI assistant that analyzes conversational context to plan the next step.
Your goal is to identify the minimum essential context needed to answer the user's `new_question` and decide on the correct `action`.

AVAILABLE CONTEXT KEYS: ["instruction", "summary", "user_messages", "agent_responses", "tool_logs"]


Think step-by-step:
1.  **Analyze the `new_question`**: What is the user's core intent?
2.  **Review the `context` items**: Determine which of the available keys are directly relevant to the question. For example, if the question is about a past tool result, keep `tool_logs`.
3.  **Choose the `action` based on this logic**:
    - `tool_call`: Use this if the `new_question` requires **new, external information** that is not present in the provided context.
    - `kb_lookup`: Use this if the answer is likely **already present** within the provided `context` (e.g., in `agent_responses` or `tool_logs`).
    - `general_answer`: Use this for conversational follow-ups, summarizations, or clarifications that don't require tools or deep context.

Return ONLY a valid JSON object matching the required schema.
"""

async def run_pruner(payload: Dict[str, Any], model: str = None) -> Dict[str, Any]:
    """Run pruner with flexible model selection."""
    if model is None:
        model = st.session_state.get('pruner_model', 'openai/gpt-5-mini')

    # Route based on API_ROUTING_MODE or provider
    provider = _get_provider_from_model_id(model)

    try:
        if API_ROUTING_MODE == "openrouter" or provider in ["mistralai", "anthropic", "meta-llama", "deepseek"]:
            # Use OpenRouter
            return await openrouter_json(_to_openrouter_model_id(model), PRUNER_INSTRUCTIONS, payload, "pruner", PRUNER_SCHEMA)
        elif provider == "openai":
            # Use native OpenAI
            native_model = _to_native_model_id(model)
            return await openai_structured_json(AsyncOpenAI(api_key=OPENAI_API_KEY), native_model, PRUNER_INSTRUCTIONS, payload)
        else:
            # Default to OpenRouter
            return await openrouter_json(_to_openrouter_model_id(model), PRUNER_INSTRUCTIONS, payload, "pruner", PRUNER_SCHEMA)
    except Exception as e:
        # Fallback to OpenAI if available
        if OPENAI_API_KEY and provider != "openai":
            try:
                return await openai_structured_json(AsyncOpenAI(api_key=OPENAI_API_KEY), _to_native_model_id(OPENAI_MODEL), PRUNER_INSTRUCTIONS, payload)
            except:
                pass
        raise e

# ======================= Sidebar & Main Layout =======================
with st.sidebar:
    st.header("Providers & Models (Deprecated)")
    st.info("Model selection moved into each test section. Configure per test below. This sidebar block will be removed.")

    # --- PATCH 26/29: Unified Format Function & UI Updates ---
    def format_model_option(model_id: str) -> str:
        """Format model ID with metadata for display in dropdown (unified for all providers)."""
        if model_id == "Custom...":
            return model_id

        # Check Gemini first due to potential model naming conflicts
        if model_id in GEMINI_MODEL_METADATA:
            metadata = GEMINI_MODEL_METADATA.get(model_id, {})
            context = metadata.get('context', 'N/A')
            input_cost = metadata.get('input_cost', 'N/A')
            output_cost = metadata.get('output_cost', 'N/A')
            return f"{model_id} (Ctx: {context}, In: {input_cost}/M, Out: {output_cost}/M)"

        # Determine provider based on ID structure
        if model_id in OPENROUTER_MODEL_METADATA:
            metadata = OPENROUTER_MODEL_METADATA.get(model_id, {})
            context = metadata.get('context', 'N/A')
            input_cost = metadata.get('input_cost', 'N/A')
            output_cost = metadata.get('output_cost', 'N/A')
            display_name = model_id.split('/')[-1]
            return f"{display_name} (Ctx: {context}, In: {input_cost}/M, Out: {output_cost}/M)"

        if model_id in OPENAI_MODEL_METADATA:
            metadata = OPENAI_MODEL_METADATA.get(model_id, {})
            context = metadata.get('context', 'N/A')
            input_cost = metadata.get('input_cost', 'N/A')
            output_cost = metadata.get('output_cost', 'N/A')
            return f"{model_id} (Ctx: {context}, In: {input_cost}/M, Out: {output_cost}/M)"

        if model_id in OLLAMA_MODEL_METADATA:
            metadata = OLLAMA_MODEL_METADATA.get(model_id, {})
            context = metadata.get('context', 'N/A')
            info = metadata.get('local_info', 'N/A')
            return f"{model_id} (Ctx: {context}, Info: {info})"

        return model_id
    # -------------------------------------------------------------

    # 1. OpenRouter Selection (Main Model)
    _openrouter_model_ids = list(OPENROUTER_MODEL_METADATA.keys()) + ["Custom..."]
    _default_idx_or = _openrouter_model_ids.index(OPENROUTER_MODEL) if OPENROUTER_MODEL in _openrouter_model_ids else (len(_openrouter_model_ids) - 1)
    _selected_or = st.selectbox("OpenRouter model", options=_openrouter_model_ids, index=_default_idx_or, format_func=format_model_option, key="openrouter_main_select")
    OPENROUTER_MODEL = st.text_input("Custom OpenRouter model ID", value=OPENROUTER_MODEL) if _selected_or == "Custom..." else _selected_or
    if use_ollama and not OPENROUTER_API_KEY: st.warning("OPENROUTER_API_KEY not set.")

    # 2. Gemini Selection (PATCH 29)
    use_gemini = st.checkbox("Use Google Gemini (structured output)", value=False, key='use_gemini')
    _gemini_model_ids = list(GEMINI_MODEL_METADATA.keys())
    GEMINI_MODEL = st.secrets.get("GEMINI_MODEL", "gemini-2.5-flash")
    _default_idx_gmn = _gemini_model_ids.index(GEMINI_MODEL) if GEMINI_MODEL in _gemini_model_ids else 0
    selected_gemini = st.selectbox("Gemini model", options=_gemini_model_ids, index=_default_idx_gmn, format_func=format_model_option, key="gemini_main_select")
    st.session_state['gemini_model'] = selected_gemini
    if use_gemini and not GEMINI_API_KEY: st.warning("GEMINI_API_KEY not set.")

    # 3. OpenAI Selection
    use_openai = st.checkbox("Use OpenAI (structured output)", value=True)
    _openai_model_ids = list(OPENAI_MODEL_METADATA.keys())
    _default_idx_oai = _openai_model_ids.index(OPENAI_MODEL) if OPENAI_MODEL in _openai_model_ids else 0
    OPENAI_MODEL = st.selectbox("OpenAI model", options=_openai_model_ids, index=_default_idx_oai, format_func=format_model_option, key="openai_main_select")
    if use_openai and not OPENAI_API_KEY: st.warning("OPENAI_API_KEY not set.")

    # 4. Ollama Selection (Local)
    use_ollama_local = st.checkbox("Use Ollama (local/private)", value=False)
    OLLAMA_BASE_URL = st.text_input("Ollama base URL", value=OLLAMA_BASE_URL)
    _ollama_model_ids = list(OLLAMA_MODEL_METADATA.keys())
    _default_idx_ollama = _ollama_model_ids.index(OLLAMA_MODEL) if OLLAMA_MODEL in _ollama_model_ids else (len(_ollama_model_ids) - 1)
    _selected_ollama = st.selectbox("Ollama model", options=_ollama_model_ids, index=_default_idx_ollama, format_func=format_model_option, key="ollama_main_select")
    OLLAMA_MODEL = st.text_input("Custom Ollama model ID", value=OLLAMA_MODEL) if _selected_ollama == "Custom..." else _selected_ollama

    st.divider()
    st.subheader("Third Model (Test 2/3)")
    THIRD_KIND = st.selectbox("Third model kind", ["None", "OpenRouter", "OpenAI"], index=["None","OpenRouter","OpenAI"].index(THIRD_KIND if THIRD_KIND in ["None","OpenRouter","OpenAI"] else "None"))
    if THIRD_KIND == "OpenRouter":
        THIRD_MODEL = st.selectbox("Third model (OpenRouter)", options=_openrouter_model_ids, format_func=format_model_option, index=0, key="openrouter_third_select")
    elif THIRD_KIND == "OpenAI":
        THIRD_MODEL = st.selectbox("Third model (OpenAI)", options=_openai_model_ids, format_func=format_model_option, index=0, key="openai_third_select")
    else: THIRD_MODEL = ""
    st.divider()
    st.subheader("DataFrame Controls")
    if st.button("Clear results"):
        for c in st.session_state.df.columns:
            if c not in ["query", "classification"]: st.session_state.df[c] = None
        st.info("Cleared model outputs")
    st.subheader("Row limit for tests")
    _limit_choice = st.selectbox("Rows to test", list(ROW_LIMIT_OPTIONS.keys()), index=3)
    ROW_LIMIT_N = ROW_LIMIT_OPTIONS[_limit_choice]
    st.divider()
    st.subheader("Analysis Options")
    explain_cm = st.checkbox("Explain Confusion Matrices with LLM", value=True, help="Uses the selected OpenRouter/OpenAI model to provide a natural language explanation of confusion matrix results.")

    # --- COST TRACKING UI ---
    st.divider()
    st.subheader("💰 Cost Tracking")

    ct = st.session_state.cost_tracker

    # Display totals
    st.metric("Total Cost", f"${ct.totals['total_cost_usd']:.4f}")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Input Cost", f"${ct.totals['input_cost_usd']:.4f}")
        st.metric("Prompt Tokens", f"{ct.totals['prompt_tokens']:,}")
    with col2:
        st.metric("Output Cost", f"${ct.totals['output_cost_usd']:.4f}")
        st.metric("Completion Tokens", f"{ct.totals['completion_tokens']:,}")

    st.metric("Total Tokens", f"{ct.totals['total_tokens']:,}")

    # Interactive cost visualizations
    render_cost_dashboard()

    # Display breakdown by provider/model
    with st.expander("Cost Breakdown by Model"):
        summary = ct.get_summary()
        if summary:
            for (provider, model), stats in summary.items():
                st.markdown(f"**{provider} / {model}**")
                st.text(f"  Calls: {stats['calls']}")
                st.text(f"  Tokens: {stats['total_tokens']:,} ({stats['prompt_tokens']:,} in / {stats['completion_tokens']:,} out)")
                st.text(f"  Cost: ${stats['total_cost_usd']:.4f} (${stats['input_cost_usd']:.4f} in / ${stats['output_cost_usd']:.4f} out)")
                st.divider()
        else:
            st.info("No API calls tracked yet.")

    # Display recent calls
    with st.expander("Recent API Calls"):
        if ct.by_call:
            recent_calls = ct.by_call[-10:]  # Last 10 calls
            for i, call in enumerate(reversed(recent_calls), 1):
                st.markdown(f"**Call {len(ct.by_call) - i + 1}**: {call['provider']} / {call['model']}")
                st.text(f"  API: {call['api']}")
                st.text(f"  Tokens: {call['total_tokens']:,} ({call['prompt_tokens']:,} in / {call['completion_tokens']:,} out)")
                st.text(f"  Cost: ${call['total_cost_usd']:.4f}")
                st.divider()
        else:
            st.info("No API calls tracked yet.")

    # Reset button
    if st.button("Reset Cost Tracking"):
        st.session_state.cost_tracker.reset()
        st.success("Cost tracking reset!")
        st.rerun()


# --- PATCH 5: Update Tab Definitions ---
tabs = st.tabs([
    "Preparation: Data Generation", # New Tab 0
    "Test 1: Classify, F1, Latency & Analysis",
    "Test 2: Advanced Ensembling",
    "Test 3: LLM as Judge",
    "Test 4: Quantitative Pruning",
    "Test 5: Agent Self-Refinement (Code Ex.)", # Tab 5
    "Agent Dashboard" # NEW Tab 6
])
# -------------------------------------

# NEW: Data Generation Function
async def generate_synthetic_data(task_prompt: str, data_size: int, data_type: str, generation_model: str) -> pd.DataFrame:

    # --- PATCH 16: Define the schema for a single item based on data type ---
    if data_type == "Classification":
        item_schema = SyntheticDataItem.model_json_schema()
    elif data_type == "Tool/Agent Sequence":
        item_schema = ToolCallSequenceItem.model_json_schema()
    elif data_type == "Context Pruning":
        item_schema = PruningDataItem.model_json_schema()
    else:
        raise ValueError(f"Invalid data type: {data_type}")
    # -----------------------------------------------------------------------

    # Define a schema wrapper to ensure the output is a single dict with a list inside,
    # which is safer for structured JSON APIs.
    list_wrapper_schema = {
        "type": "object",
        "properties": {"items": {"type": "array", "items": item_schema}},
        "required": ["items"]
    }

    # User payload includes instructions for the LLM
    user_input = {
        "required_output_count": data_size,
        "task_description": task_prompt,
        "instructions": f"Generate exactly {data_size} distinct data items, ensuring all output is wrapped in a JSON object under the key 'items'.",
        "target_schema": list_wrapper_schema
    }

    st.info(f"Generating {data_size} items using {generation_model}...")

    try:
        raw_result = {}

        # --- UPDATED: Flexible model routing for data generation ---
        provider = _get_provider_from_model_id(generation_model)
        system = "You are a synthetic data generator. Respond ONLY with a single JSON object matching the requested schema, containing a list of objects under the key 'items'."

        if API_ROUTING_MODE == "openrouter" or provider in ["mistralai", "anthropic", "meta-llama", "deepseek"]:
            # Use OpenRouter
            if not OPENROUTER_API_KEY:
                raise ValueError("OPENROUTER_API_KEY not set for OpenRouter mode.")
            raw_result = await openrouter_json(_to_openrouter_model_id(generation_model), system, user_input, "synthetic_dataset_list", list_wrapper_schema)

        elif provider == "openai":
            # Use native OpenAI
            if not OPENAI_API_KEY:
                raise ValueError("OPENAI_API_KEY not set for OpenAI model.")
            client = AsyncOpenAI(api_key=OPENAI_API_KEY)
            native_model = _to_native_model_id(generation_model)
            raw_result = await openai_structured_json(client, native_model, system, user_input)

        elif provider == "google":
            # For Gemini, route through OpenRouter (native Gemini doesn't support structured JSON well)
            if not OPENROUTER_API_KEY:
                raise ValueError("OPENROUTER_API_KEY required for Gemini models in data generation.")
            raw_result = await openrouter_json(_to_openrouter_model_id(generation_model), system, user_input, "synthetic_dataset_list", list_wrapper_schema)

        else:
            # Default to OpenRouter
            if not OPENROUTER_API_KEY:
                raise ValueError("OPENROUTER_API_KEY not set.")
            raw_result = await openrouter_json(_to_openrouter_model_id(generation_model), system, user_input, "synthetic_dataset_list", list_wrapper_schema)

        # --- Robust Extraction ---
        data_list = []
        if isinstance(raw_result, dict):
            # Explicitly extract from the 'items' key enforced by the schema
            data_list = raw_result.get('items', [])

        if not isinstance(data_list, list) or not data_list:
             # Fallback: If 'items' didn't work, try to find a list anywhere else in the top level
             if isinstance(raw_result, dict):
                 for key, value in raw_result.items():
                     if isinstance(value, list) and all(isinstance(i, dict) for i in value):
                         data_list = value
                         break

        if not data_list:
             raise ValueError(f"Could not extract a list of structured items from the LLM response: {raw_result}")

        # Final check and DataFrame creation
        df = pd.DataFrame(data_list)
        return df

    except Exception as e:
        st.error(f"Data generation failed: {e}")
        return pd.DataFrame()
# --------------------------------------------------------------------------


with tabs[0]:
    st.header("Preparation: Synthetic Data Generation")

    # --- NEW: Display dataset status ---
    st.subheader("📊 Dataset Status")
    col1, col2, col3 = st.columns(3)

    with col1:
        if os.path.exists(CLASSIFICATION_DATASET_PATH):
            df_class = pd.read_csv(CLASSIFICATION_DATASET_PATH)
            st.success(f"✅ Classification\n({len(df_class)} rows)")
        else:
            st.warning("⚠️ Classification\n(Missing)")

    with col2:
        if os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
            df_tool = pd.read_csv(TOOL_SEQUENCE_DATASET_PATH)
            st.success(f"✅ Tool/Agent Seq.\n({len(df_tool)} rows)")
        else:
            st.warning("⚠️ Tool/Agent Seq.\n(Missing)")

    with col3:
        if os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
            df_prune = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH)
            st.success(f"✅ Context Pruning\n({len(df_prune)} rows)")
        else:
            st.warning("⚠️ Context Pruning\n(Missing)")

    st.divider()

    # --- NEW: Dataset Composition Visualization ---
    st.subheader("📊 Dataset Composition Analysis")

    # Create tabs for each dataset type
    dataset_viz_tabs = st.tabs(["Classification", "Tool/Agent Sequence", "Context Pruning"])

    with dataset_viz_tabs[0]:
        if os.path.exists(CLASSIFICATION_DATASET_PATH):
            df_class = pd.read_csv(CLASSIFICATION_DATASET_PATH)
            visualize_dataset_composition(df_class, dataset_type="Classification")
        else:
            st.info("Generate the Classification dataset to see composition analysis.")

    with dataset_viz_tabs[1]:
        if os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
            df_tool = pd.read_csv(TOOL_SEQUENCE_DATASET_PATH)
            visualize_dataset_composition(df_tool, dataset_type="Tool/Agent Sequence")
        else:
            st.info("Generate the Tool/Agent Sequence dataset to see composition analysis.")

    with dataset_viz_tabs[2]:
        if os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
            df_prune = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH)
            visualize_dataset_composition(df_prune, dataset_type="Context Pruning")
        else:
            st.info("Generate the Context Pruning dataset to see composition analysis.")

    st.divider()

    # --- NEW: Cross-Dataset Analytics Section ---
    # Only display if at least one dataset exists
    if any([os.path.exists(p) for p in [CLASSIFICATION_DATASET_PATH, TOOL_SEQUENCE_DATASET_PATH, CONTEXT_PRUNING_DATASET_PATH]]):
        st.subheader("📊 Cross-Dataset Analytics")

        analytics_tabs = st.tabs(["Dataset Comparison", "Generation History", "Quality Metrics", "Token Economics"])

        # Tab 1: Dataset Comparison Matrix
        with analytics_tabs[0]:
            st.markdown("### Dataset Comparison Matrix")

            # Collect statistics for all datasets
            dataset_stats = []

            # Classification dataset
            if os.path.exists(CLASSIFICATION_DATASET_PATH):
                try:
                    df_class = pd.read_csv(CLASSIFICATION_DATASET_PATH)
                    unique_labels = df_class['classification'].nunique() if 'classification' in df_class.columns else 0
                    avg_query_len = df_class['query'].astype(str).str.len().mean() if 'query' in df_class.columns else 0
                    missing_pct = (df_class.isnull().sum().sum() / df_class.size * 100) if df_class.size > 0 else 0
                    file_size_kb = os.path.getsize(CLASSIFICATION_DATASET_PATH) / 1024

                    dataset_stats.append({
                        'Dataset': 'Classification',
                        'Total Items': len(df_class),
                        'Unique Labels/Actions': unique_labels,
                        'Avg Query Length (chars)': round(avg_query_len, 1),
                        'Missing Values (%)': round(missing_pct, 2),
                        'File Size (KB)': round(file_size_kb, 2)
                    })
                except Exception as e:
                    st.warning(f"Error reading Classification dataset: {e}")
            else:
                dataset_stats.append({
                    'Dataset': 'Classification',
                    'Total Items': 'N/A',
                    'Unique Labels/Actions': 'N/A',
                    'Avg Query Length (chars)': 'N/A',
                    'Missing Values (%)': 'N/A',
                    'File Size (KB)': 'N/A'
                })

            # Tool/Agent Sequence dataset
            if os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
                try:
                    df_tool = pd.read_csv(TOOL_SEQUENCE_DATASET_PATH)
                    unique_seqs = df_tool['expected_sequence'].nunique() if 'expected_sequence' in df_tool.columns else 0
                    avg_query_len = df_tool['query'].astype(str).str.len().mean() if 'query' in df_tool.columns else 0
                    missing_pct = (df_tool.isnull().sum().sum() / df_tool.size * 100) if df_tool.size > 0 else 0
                    file_size_kb = os.path.getsize(TOOL_SEQUENCE_DATASET_PATH) / 1024

                    dataset_stats.append({
                        'Dataset': 'Tool/Agent Sequence',
                        'Total Items': len(df_tool),
                        'Unique Labels/Actions': unique_seqs,
                        'Avg Query Length (chars)': round(avg_query_len, 1),
                        'Missing Values (%)': round(missing_pct, 2),
                        'File Size (KB)': round(file_size_kb, 2)
                    })
                except Exception as e:
                    st.warning(f"Error reading Tool/Agent Sequence dataset: {e}")
            else:
                dataset_stats.append({
                    'Dataset': 'Tool/Agent Sequence',
                    'Total Items': 'N/A',
                    'Unique Labels/Actions': 'N/A',
                    'Avg Query Length (chars)': 'N/A',
                    'Missing Values (%)': 'N/A',
                    'File Size (KB)': 'N/A'
                })

            # Context Pruning dataset
            if os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
                try:
                    df_prune = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH)
                    unique_actions = df_prune['expected_action'].nunique() if 'expected_action' in df_prune.columns else 0
                    avg_query_len = df_prune['new_question'].astype(str).str.len().mean() if 'new_question' in df_prune.columns else 0
                    missing_pct = (df_prune.isnull().sum().sum() / df_prune.size * 100) if df_prune.size > 0 else 0
                    file_size_kb = os.path.getsize(CONTEXT_PRUNING_DATASET_PATH) / 1024

                    dataset_stats.append({
                        'Dataset': 'Context Pruning',
                        'Total Items': len(df_prune),
                        'Unique Labels/Actions': unique_actions,
                        'Avg Query Length (chars)': round(avg_query_len, 1),
                        'Missing Values (%)': round(missing_pct, 2),
                        'File Size (KB)': round(file_size_kb, 2)
                    })
                except Exception as e:
                    st.warning(f"Error reading Context Pruning dataset: {e}")
            else:
                dataset_stats.append({
                    'Dataset': 'Context Pruning',
                    'Total Items': 'N/A',
                    'Unique Labels/Actions': 'N/A',
                    'Avg Query Length (chars)': 'N/A',
                    'Missing Values (%)': 'N/A',
                    'File Size (KB)': 'N/A'
                })

            # Create DataFrame and display with styling
            stats_df = pd.DataFrame(dataset_stats)

            # Apply gradient styling only to numeric columns
            def style_stats(df):
                styled = df.style
                numeric_cols = ['Total Items', 'Avg Query Length (chars)', 'File Size (KB)']
                for col in numeric_cols:
                    if col in df.columns:
                        # Only apply gradient to numeric values (not 'N/A')
                        try:
                            styled = styled.background_gradient(subset=[col], cmap='Blues', vmin=0)
                        except:
                            pass
                return styled

            st.dataframe(style_stats(stats_df), use_container_width=True)

            # Comparison Charts
            st.markdown("### Visual Comparison")
            comp_col1, comp_col2 = st.columns(2)

            with comp_col1:
                # Bar chart: Total items
                valid_stats = [s for s in dataset_stats if s['Total Items'] != 'N/A']
                if valid_stats:
                    fig_items = go.Figure(data=[go.Bar(
                        x=[s['Dataset'] for s in valid_stats],
                        y=[s['Total Items'] for s in valid_stats],
                        text=[s['Total Items'] for s in valid_stats],
                        textposition='auto',
                        marker_color='#1f77b4',
                        hovertemplate='<b>%{x}</b><br>Items: %{y}<extra></extra>'
                    )])
                    fig_items.update_layout(
                        title="Total Items Comparison",
                        xaxis_title="Dataset",
                        yaxis_title="Total Items",
                        height=300,
                        margin=dict(l=20, r=20, t=40, b=40)
                    )
                    st.plotly_chart(fig_items, use_container_width=True, config=PLOTLY_CONFIG)
                else:
                    st.info("No datasets available for comparison")

            with comp_col2:
                # Bar chart: Average query complexity
                valid_stats = [s for s in dataset_stats if s['Avg Query Length (chars)'] != 'N/A']
                if valid_stats:
                    fig_complexity = go.Figure(data=[go.Bar(
                        x=[s['Dataset'] for s in valid_stats],
                        y=[s['Avg Query Length (chars)'] for s in valid_stats],
                        text=[f"{s['Avg Query Length (chars)']:.1f}" for s in valid_stats],
                        textposition='auto',
                        marker_color='#ff7f0e',
                        hovertemplate='<b>%{x}</b><br>Avg Length: %{y:.1f} chars<extra></extra>'
                    )])
                    fig_complexity.update_layout(
                        title="Average Query Complexity",
                        xaxis_title="Dataset",
                        yaxis_title="Avg Character Length",
                        height=300,
                        margin=dict(l=20, r=20, t=40, b=40)
                    )
                    st.plotly_chart(fig_complexity, use_container_width=True, config=PLOTLY_CONFIG)
                else:
                    st.info("No datasets available for comparison")

        # Tab 2: Generation History & Cost Trends
        with analytics_tabs[1]:
            st.markdown("### Generation History & Cost Trends")

            # Check if generation history exists
            gen_history = st.session_state.get("generation_history", [])

            if not gen_history:
                st.info("No generation history available. Generate datasets to see cost trends.")
            else:
                # Convert to DataFrame for easier manipulation
                hist_df = pd.DataFrame(gen_history)

                # Add run number
                hist_df['run_number'] = range(1, len(hist_df) + 1)

                # Calculate cumulative cost
                hist_df['cumulative_cost'] = hist_df['cost_usd'].cumsum()

                # 1. Dual-Axis Cost Trend Chart
                st.markdown("#### Cost Trends Over Time")

                fig_cost_trend = make_subplots(specs=[[{"secondary_y": True}]])

                # Primary axis: Cost per generation run
                fig_cost_trend.add_trace(
                    go.Scatter(
                        x=hist_df['run_number'],
                        y=hist_df['cost_usd'],
                        mode='lines+markers',
                        name='Cost per Run',
                        line=dict(color='#2ca02c', width=2),
                        marker=dict(size=8),
                        hovertemplate='Run #%{x}<br>Cost: $%{y:.4f}<extra></extra>'
                    ),
                    secondary_y=False
                )

                # Secondary axis: Cumulative cost
                fig_cost_trend.add_trace(
                    go.Scatter(
                        x=hist_df['run_number'],
                        y=hist_df['cumulative_cost'],
                        mode='lines',
                        name='Cumulative Cost',
                        line=dict(color='#d62728', width=2, dash='dash'),
                        hovertemplate='Run #%{x}<br>Total: $%{y:.4f}<extra></extra>'
                    ),
                    secondary_y=True
                )

                # Update axes
                fig_cost_trend.update_xaxes(title_text="Generation Run Number")
                fig_cost_trend.update_yaxes(title_text="Cost per Run (USD)", secondary_y=False)
                fig_cost_trend.update_yaxes(title_text="Cumulative Cost (USD)", secondary_y=True)

                fig_cost_trend.update_layout(
                    height=400,
                    hovermode='x unified',
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                )

                st.plotly_chart(fig_cost_trend, use_container_width=True, config=PLOTLY_CONFIG)

                # 2. Token Usage Trends
                st.markdown("#### Token Usage Trends")

                fig_tokens = go.Figure()

                fig_tokens.add_trace(go.Scatter(
                    x=hist_df['run_number'],
                    y=hist_df['tokens'],
                    mode='lines+markers',
                    name='Tokens Used',
                    fill='tozeroy',
                    fillcolor='rgba(31, 119, 180, 0.3)',
                    line=dict(color='#1f77b4', width=2),
                    marker=dict(size=6),
                    hovertemplate='Run #%{x}<br>Tokens: %{y:,}<extra></extra>'
                ))

                fig_tokens.update_layout(
                    title="Token Usage Over Time",
                    xaxis_title="Generation Run Number",
                    yaxis_title="Total Tokens Used",
                    height=300,
                    margin=dict(l=20, r=20, t=40, b=40)
                )

                st.plotly_chart(fig_tokens, use_container_width=True, config=PLOTLY_CONFIG)

                # 3. Model Efficiency Comparison (if multiple models used)
                unique_models = hist_df['model'].nunique()

                if unique_models >= 2:
                    st.markdown("#### Model Efficiency Comparison")

                    # Calculate efficiency metrics per model
                    model_efficiency = hist_df.groupby('model').agg({
                        'cost_usd': 'sum',
                        'tokens': 'sum',
                        'total_items': 'sum'
                    }).reset_index()

                    model_efficiency['cost_per_item'] = model_efficiency['cost_usd'] / model_efficiency['total_items']
                    model_efficiency['tokens_per_item'] = model_efficiency['tokens'] / model_efficiency['total_items']

                    eff_col1, eff_col2 = st.columns(2)

                    with eff_col1:
                        # Cost per item by model
                        fig_cost_eff = go.Figure(data=[go.Bar(
                            x=model_efficiency['model'],
                            y=model_efficiency['cost_per_item'],
                            text=[f"${v:.4f}" for v in model_efficiency['cost_per_item']],
                            textposition='auto',
                            marker_color='#2ca02c',
                            hovertemplate='<b>%{x}</b><br>Cost/Item: $%{y:.4f}<extra></extra>'
                        )])
                        fig_cost_eff.update_layout(
                            title="Cost per Item by Model",
                            xaxis_title="Model",
                            yaxis_title="Cost per Item (USD)",
                            height=300,
                            margin=dict(l=20, r=20, t=40, b=40)
                        )
                        st.plotly_chart(fig_cost_eff, use_container_width=True, config=PLOTLY_CONFIG)

                    with eff_col2:
                        # Tokens per item by model
                        fig_tokens_eff = go.Figure(data=[go.Bar(
                            x=model_efficiency['model'],
                            y=model_efficiency['tokens_per_item'],
                            text=[f"{v:.0f}" for v in model_efficiency['tokens_per_item']],
                            textposition='auto',
                            marker_color='#1f77b4',
                            hovertemplate='<b>%{x}</b><br>Tokens/Item: %{y:.0f}<extra></extra>'
                        )])
                        fig_tokens_eff.update_layout(
                            title="Tokens per Item by Model",
                            xaxis_title="Model",
                            yaxis_title="Tokens per Item",
                            height=300,
                            margin=dict(l=20, r=20, t=40, b=40)
                        )
                        st.plotly_chart(fig_tokens_eff, use_container_width=True, config=PLOTLY_CONFIG)

        # Tab 3: Quality Metrics Dashboard
        with analytics_tabs[2]:
            st.markdown("### Quality Metrics Dashboard")

            # 3-column layout for each dataset
            qm_col1, qm_col2, qm_col3 = st.columns(3)

            # --- Column 1: Classification Dataset Quality ---
            with qm_col1:
                st.markdown("#### 📋 Classification Dataset")

                if os.path.exists(CLASSIFICATION_DATASET_PATH):
                    try:
                        df_class = pd.read_csv(CLASSIFICATION_DATASET_PATH)

                        if not df_class.empty and 'classification' in df_class.columns:
                            # Calculate balance score (min/max class ratio)
                            class_counts = df_class['classification'].value_counts()
                            if len(class_counts) > 1:
                                balance_score = (class_counts.min() / class_counts.max() * 100) if class_counts.max() > 0 else 0
                            else:
                                balance_score = 100  # Perfect balance if only one class

                            # Gauge chart for balance
                            fig_gauge_class = go.Figure(go.Indicator(
                                mode="gauge+number",
                                value=balance_score,
                                title={'text': "Balance Score"},
                                gauge={
                                    'axis': {'range': [0, 100]},
                                    'bar': {'color': "darkblue"},
                                    'steps': [
                                        {'range': [0, 50], 'color': "lightgray"},
                                        {'range': [50, 80], 'color': "lightyellow"},
                                        {'range': [80, 100], 'color': "lightgreen"}
                                    ],
                                    'threshold': {
                                        'line': {'color': "red", 'width': 4},
                                        'thickness': 0.75,
                                        'value': 90
                                    }
                                }
                            ))
                            fig_gauge_class.update_layout(height=250, margin=dict(l=20, r=20, t=40, b=20))
                            st.plotly_chart(fig_gauge_class, use_container_width=True, config=PLOTLY_CONFIG)

                            # Metrics below gauge
                            metric_col1, metric_col2 = st.columns(2)
                            with metric_col1:
                                diversity = len(class_counts)
                                st.metric("Diversity", f"{diversity} classes")
                            with metric_col2:
                                completeness = (1 - df_class.isnull().sum().sum() / df_class.size) * 100 if df_class.size > 0 else 0
                                st.metric("Completeness", f"{completeness:.1f}%")

                            # Keyword analysis for classification
                            st.markdown("**Top Keywords by Label**")

                            # Simple stop words list
                            stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                                        'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',
                                        'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
                                        'would', 'should', 'could', 'may', 'might', 'must', 'can', 'this',
                                        'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'}

                            if 'query' in df_class.columns:
                                # Get top label (most common)
                                top_label = class_counts.index[0]
                                label_queries = df_class[df_class['classification'] == top_label]['query'].astype(str)

                                # Extract words
                                all_words = []
                                for query in label_queries:
                                    words = re.findall(r'\b[a-z]{3,}\b', query.lower())
                                    all_words.extend([w for w in words if w not in stop_words])

                                if all_words:
                                    word_counts = Counter(all_words).most_common(5)

                                    # Horizontal bar chart
                                    fig_keywords = go.Figure(data=[go.Bar(
                                        y=[w[0] for w in word_counts],
                                        x=[w[1] for w in word_counts],
                                        orientation='h',
                                        marker_color='darkblue',
                                        text=[w[1] for w in word_counts],
                                        textposition='auto'
                                    )])
                                    fig_keywords.update_layout(
                                        title=f"Top 5 Keywords: {top_label}",
                                        xaxis_title="Frequency",
                                        yaxis_title="",
                                        height=200,
                                        margin=dict(l=20, r=20, t=40, b=20)
                                    )
                                    st.plotly_chart(fig_keywords, use_container_width=True, config=PLOTLY_CONFIG)
                                else:
                                    st.info("No keywords extracted")

                        else:
                            st.info("Dataset is empty or missing required columns")
                    except Exception as e:
                        st.warning(f"Error analyzing Classification dataset: {e}")
                else:
                    st.info("Classification dataset not found")

            # --- Column 2: Tool/Agent Sequence Dataset Quality ---
            with qm_col2:
                st.markdown("#### 🔧 Tool/Agent Sequence")

                if os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
                    try:
                        df_tool = pd.read_csv(TOOL_SEQUENCE_DATASET_PATH)

                        if not df_tool.empty and 'expected_sequence' in df_tool.columns:
                            # Calculate average sequence length (0-10 scale)
                            avg_seq_length = df_tool['expected_sequence'].astype(str).str.split(',').str.len().mean()
                            # Normalize to 0-100 scale (assume max 10 steps)
                            balance_score = min((avg_seq_length / 10) * 100, 100) if avg_seq_length > 0 else 0

                            # Gauge chart
                            fig_gauge_tool = go.Figure(go.Indicator(
                                mode="gauge+number",
                                value=balance_score,
                                title={'text': "Complexity Score"},
                                gauge={
                                    'axis': {'range': [0, 100]},
                                    'bar': {'color': "darkgreen"},
                                    'steps': [
                                        {'range': [0, 50], 'color': "lightgray"},
                                        {'range': [50, 80], 'color': "lightyellow"},
                                        {'range': [80, 100], 'color': "lightgreen"}
                                    ],
                                    'threshold': {
                                        'line': {'color': "red", 'width': 4},
                                        'thickness': 0.75,
                                        'value': 90
                                    }
                                }
                            ))
                            fig_gauge_tool.update_layout(height=250, margin=dict(l=20, r=20, t=40, b=20))
                            st.plotly_chart(fig_gauge_tool, use_container_width=True, config=PLOTLY_CONFIG)

                            # Metrics below gauge
                            metric_col1, metric_col2 = st.columns(2)
                            with metric_col1:
                                diversity = df_tool['expected_sequence'].nunique()
                                st.metric("Unique Sequences", f"{diversity}")
                            with metric_col2:
                                completeness = (1 - df_tool.isnull().sum().sum() / df_tool.size) * 100 if df_tool.size > 0 else 0
                                st.metric("Completeness", f"{completeness:.1f}%")

                            # Sequence length distribution
                            st.markdown("**Sequence Length Distribution**")
                            seq_lengths = df_tool['expected_sequence'].astype(str).str.split(',').str.len()

                            fig_seq_dist = go.Figure(data=[go.Histogram(
                                x=seq_lengths,
                                marker_color='darkgreen',
                                nbinsx=10
                            )])
                            fig_seq_dist.update_layout(
                                title="Steps per Sequence",
                                xaxis_title="Number of Steps",
                                yaxis_title="Frequency",
                                height=200,
                                margin=dict(l=20, r=20, t=40, b=20)
                            )
                            st.plotly_chart(fig_seq_dist, use_container_width=True, config=PLOTLY_CONFIG)

                        else:
                            st.info("Dataset is empty or missing required columns")
                    except Exception as e:
                        st.warning(f"Error analyzing Tool/Agent dataset: {e}")
                else:
                    st.info("Tool/Agent dataset not found")

            # --- Column 3: Context Pruning Dataset Quality ---
            with qm_col3:
                st.markdown("#### ✂️ Context Pruning")

                if os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
                    try:
                        df_prune = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH)

                        if not df_prune.empty and 'expected_action' in df_prune.columns:
                            # Calculate action balance ratio
                            action_counts = df_prune['expected_action'].value_counts()
                            if len(action_counts) > 1:
                                balance_score = (action_counts.min() / action_counts.max() * 100) if action_counts.max() > 0 else 0
                            else:
                                balance_score = 100

                            # Gauge chart
                            fig_gauge_prune = go.Figure(go.Indicator(
                                mode="gauge+number",
                                value=balance_score,
                                title={'text': "Action Balance"},
                                gauge={
                                    'axis': {'range': [0, 100]},
                                    'bar': {'color': "darkorange"},
                                    'steps': [
                                        {'range': [0, 50], 'color': "lightgray"},
                                        {'range': [50, 80], 'color': "lightyellow"},
                                        {'range': [80, 100], 'color': "lightgreen"}
                                    ],
                                    'threshold': {
                                        'line': {'color': "red", 'width': 4},
                                        'thickness': 0.75,
                                        'value': 90
                                    }
                                }
                            ))
                            fig_gauge_prune.update_layout(height=250, margin=dict(l=20, r=20, t=40, b=20))
                            st.plotly_chart(fig_gauge_prune, use_container_width=True, config=PLOTLY_CONFIG)

                            # Metrics below gauge
                            metric_col1, metric_col2 = st.columns(2)
                            with metric_col1:
                                diversity = len(action_counts)
                                st.metric("Action Types", f"{diversity}")
                            with metric_col2:
                                completeness = (1 - df_prune.isnull().sum().sum() / df_prune.size) * 100 if df_prune.size > 0 else 0
                                st.metric("Completeness", f"{completeness:.1f}%")

                            # Action distribution
                            st.markdown("**Action Distribution**")

                            fig_actions = go.Figure(data=[go.Bar(
                                y=action_counts.index.tolist(),
                                x=action_counts.values.tolist(),
                                orientation='h',
                                marker_color='darkorange',
                                text=action_counts.values.tolist(),
                                textposition='auto'
                            )])
                            fig_actions.update_layout(
                                title="Actions by Type",
                                xaxis_title="Count",
                                yaxis_title="",
                                height=200,
                                margin=dict(l=20, r=20, t=40, b=20)
                            )
                            st.plotly_chart(fig_actions, use_container_width=True, config=PLOTLY_CONFIG)

                        else:
                            st.info("Dataset is empty or missing required columns")
                    except Exception as e:
                        st.warning(f"Error analyzing Context Pruning dataset: {e}")
                else:
                    st.info("Context Pruning dataset not found")

        # Tab 4: Token Economics
        with analytics_tabs[3]:
            st.markdown("### Token Economics Dashboard")

            # Model pricing reference (per 1M tokens)
            model_prices = {
                'gpt-5-mini': {'input': 0.25, 'output': 2.0, 'display': 'GPT-5 Mini'},
                'gpt-5': {'input': 1.25, 'output': 10.0, 'display': 'GPT-5'},
                'gemini-2.5-flash': {'input': 0.30, 'output': 2.50, 'display': 'Gemini 2.5 Flash'}
            }

            # Collect token data from all datasets
            dataset_token_data = []

            # Helper function to estimate tokens (rough approximation: chars / 4)
            def estimate_tokens(text_series):
                return (text_series.astype(str).str.len() / 4).sum()

            # Classification dataset
            if os.path.exists(CLASSIFICATION_DATASET_PATH):
                try:
                    df_class = pd.read_csv(CLASSIFICATION_DATASET_PATH)
                    if not df_class.empty and 'query' in df_class.columns:
                        input_tokens = estimate_tokens(df_class['query'])
                        # Assume output is classification label (small)
                        output_tokens = len(df_class) * 5  # ~5 tokens per classification
                        dataset_token_data.append({
                            'dataset': 'Classification',
                            'input_tokens': input_tokens,
                            'output_tokens': output_tokens,
                            'total_tokens': input_tokens + output_tokens,
                            'items': len(df_class)
                        })
                except Exception as e:
                    st.warning(f"Error processing Classification dataset: {e}")

            # Tool/Agent Sequence dataset
            if os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
                try:
                    df_tool = pd.read_csv(TOOL_SEQUENCE_DATASET_PATH)
                    if not df_tool.empty and 'query' in df_tool.columns:
                        input_tokens = estimate_tokens(df_tool['query'])
                        # Assume output is sequence (medium)
                        if 'expected_sequence' in df_tool.columns:
                            output_tokens = estimate_tokens(df_tool['expected_sequence'])
                        else:
                            output_tokens = len(df_tool) * 20  # ~20 tokens per sequence
                        dataset_token_data.append({
                            'dataset': 'Tool/Agent Sequence',
                            'input_tokens': input_tokens,
                            'output_tokens': output_tokens,
                            'total_tokens': input_tokens + output_tokens,
                            'items': len(df_tool)
                        })
                except Exception as e:
                    st.warning(f"Error processing Tool/Agent dataset: {e}")

            # Context Pruning dataset
            if os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
                try:
                    df_prune = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH)
                    if not df_prune.empty:
                        # Input includes conversation history + new question
                        input_tokens = 0
                        if 'conversation_history' in df_prune.columns:
                            input_tokens += estimate_tokens(df_prune['conversation_history'])
                        if 'new_question' in df_prune.columns:
                            input_tokens += estimate_tokens(df_prune['new_question'])
                        # Output is action decision (small)
                        output_tokens = len(df_prune) * 10  # ~10 tokens per action
                        dataset_token_data.append({
                            'dataset': 'Context Pruning',
                            'input_tokens': input_tokens,
                            'output_tokens': output_tokens,
                            'total_tokens': input_tokens + output_tokens,
                            'items': len(df_prune)
                        })
                except Exception as e:
                    st.warning(f"Error processing Context Pruning dataset: {e}")

            if dataset_token_data:
                # 1. Violin plot: Token distribution comparison
                st.markdown("#### Token Distribution Comparison")

                # Prepare data for violin plot
                violin_data = []
                for data in dataset_token_data:
                    # Create distribution (approximate per-item tokens)
                    tokens_per_item = data['total_tokens'] / data['items'] if data['items'] > 0 else 0
                    # Simulate distribution around mean (for visualization)
                    for _ in range(min(int(data['items']), 100)):  # Cap at 100 for performance
                        violin_data.append({
                            'Dataset': data['dataset'],
                            'Tokens': tokens_per_item
                        })

                if violin_data:
                    df_violin = pd.DataFrame(violin_data)

                    fig_violin = go.Figure()

                    for dataset_name in df_violin['Dataset'].unique():
                        dataset_tokens = df_violin[df_violin['Dataset'] == dataset_name]['Tokens']
                        fig_violin.add_trace(go.Violin(
                            y=dataset_tokens,
                            name=dataset_name,
                            box_visible=True,
                            meanline_visible=True
                        ))

                    fig_violin.update_layout(
                        title="Token Distribution by Dataset",
                        yaxis_title="Tokens per Item",
                        height=350,
                        margin=dict(l=20, r=20, t=40, b=40)
                    )
                    st.plotly_chart(fig_violin, use_container_width=True, config=PLOTLY_CONFIG)

                # 2. Cost Calculator
                st.markdown("#### Cost Calculator")

                selected_model = st.selectbox(
                    "Select Model for Cost Estimation",
                    options=list(model_prices.keys()),
                    format_func=lambda x: model_prices[x]['display']
                )

                if selected_model:
                    pricing = model_prices[selected_model]

                    # Calculate costs for each dataset
                    cost_data = []
                    for data in dataset_token_data:
                        input_cost = (data['input_tokens'] / 1_000_000) * pricing['input']
                        output_cost = (data['output_tokens'] / 1_000_000) * pricing['output']
                        total_cost = input_cost + output_cost

                        cost_data.append({
                            'Dataset': data['dataset'],
                            'Input Tokens': f"{data['input_tokens']:,.0f}",
                            'Output Tokens': f"{data['output_tokens']:,.0f}",
                            'Input Cost': f"${input_cost:.4f}",
                            'Output Cost': f"${output_cost:.4f}",
                            'Total Cost': f"${total_cost:.4f}",
                            'total_cost_numeric': total_cost
                        })

                    # Display cost table
                    cost_df = pd.DataFrame(cost_data)
                    display_df = cost_df.drop(columns=['total_cost_numeric'])

                    st.dataframe(display_df, use_container_width=True)

                    # Total summary
                    total_cost_all = sum([c['total_cost_numeric'] for c in cost_data])
                    st.metric("**Total Estimated Cost**", f"${total_cost_all:.4f}",
                             help=f"Total cost for all datasets using {model_prices[selected_model]['display']}")

                    # 3. Bar chart: Total estimated costs by dataset
                    st.markdown("#### Cost Breakdown by Dataset")

                    fig_costs = go.Figure(data=[go.Bar(
                        x=[c['Dataset'] for c in cost_data],
                        y=[c['total_cost_numeric'] for c in cost_data],
                        text=[f"${c['total_cost_numeric']:.4f}" for c in cost_data],
                        textposition='auto',
                        marker_color=['#1f77b4', '#ff7f0e', '#2ca02c'][:len(cost_data)],
                        hovertemplate='<b>%{x}</b><br>Total Cost: $%{y:.4f}<extra></extra>'
                    )])

                    fig_costs.update_layout(
                        title=f"Estimated Costs by Dataset ({model_prices[selected_model]['display']})",
                        xaxis_title="Dataset",
                        yaxis_title="Total Cost (USD)",
                        height=350,
                        margin=dict(l=20, r=20, t=40, b=40)
                    )
                    st.plotly_chart(fig_costs, use_container_width=True, config=PLOTLY_CONFIG)

                    # 4. Input vs Output cost comparison
                    st.markdown("#### Input vs Output Cost Analysis")

                    fig_io_costs = go.Figure()

                    datasets = [c['Dataset'] for c in cost_data]
                    input_costs = [(float(c['Input Cost'].replace('$', ''))) for c in cost_data]
                    output_costs = [(float(c['Output Cost'].replace('$', ''))) for c in cost_data]

                    fig_io_costs.add_trace(go.Bar(
                        name='Input Cost',
                        x=datasets,
                        y=input_costs,
                        marker_color='lightblue',
                        text=[f"${v:.4f}" for v in input_costs],
                        textposition='auto'
                    ))

                    fig_io_costs.add_trace(go.Bar(
                        name='Output Cost',
                        x=datasets,
                        y=output_costs,
                        marker_color='lightcoral',
                        text=[f"${v:.4f}" for v in output_costs],
                        textposition='auto'
                    ))

                    fig_io_costs.update_layout(
                        title="Input vs Output Costs",
                        xaxis_title="Dataset",
                        yaxis_title="Cost (USD)",
                        barmode='group',
                        height=300,
                        margin=dict(l=20, r=20, t=40, b=40)
                    )
                    st.plotly_chart(fig_io_costs, use_container_width=True, config=PLOTLY_CONFIG)

            else:
                st.info("No datasets available for token economics analysis. Generate datasets to see cost estimates.")

    # Define use case configurations
    USE_CASE_CONFIGS = {
        "🚀 Development (Fast)": {
            "description": "Quick iteration, prompt engineering, initial testing",
            "total_items": 200,
            "time_estimate": "2-5 minutes",
            "cost_estimate": "$0.10-0.30",
            "statistical_power": "Low-Medium",
            "config": [
                ("Classification", 100, CLASSIFICATION_DATASET_PATH),
                ("Tool/Agent Sequence", 50, TOOL_SEQUENCE_DATASET_PATH),
                ("Context Pruning", 50, CONTEXT_PRUNING_DATASET_PATH)
            ]
        },
        "🧪 Testing (Recommended)": {
            "description": "Pre-production validation, reliable metrics, stakeholder demos",
            "total_items": 550,
            "time_estimate": "10-15 minutes",
            "cost_estimate": "$0.25-0.75",
            "statistical_power": "Medium-High",
            "config": [
                ("Classification", 300, CLASSIFICATION_DATASET_PATH),
                ("Tool/Agent Sequence", 150, TOOL_SEQUENCE_DATASET_PATH),
                ("Context Pruning", 100, CONTEXT_PRUNING_DATASET_PATH)
            ]
        },
        "🏭 Production (Robust)": {
            "description": "Final validation, regulatory compliance, production deployment",
            "total_items": 950,
            "time_estimate": "20-40 minutes",
            "cost_estimate": "$0.50-1.50",
            "statistical_power": "High",
            "config": [
                ("Classification", 500, CLASSIFICATION_DATASET_PATH),
                ("Tool/Agent Sequence", 250, TOOL_SEQUENCE_DATASET_PATH),
                ("Context Pruning", 200, CONTEXT_PRUNING_DATASET_PATH)
            ]
        }
    }

    # Use case selector
    selected_use_case = st.selectbox(
        "Select Use Case",
        options=list(USE_CASE_CONFIGS.keys()),
        index=0,  # Default to Development
        help="Choose based on your testing needs and time constraints"
    )

    # Display use case details
    use_case_info = USE_CASE_CONFIGS[selected_use_case]

    col_info1, col_info2, col_info3, col_info4 = st.columns(4)
    with col_info1:
        st.metric("Total Items", use_case_info["total_items"])
    with col_info2:
        st.metric("Time Estimate", use_case_info["time_estimate"])
    with col_info3:
        st.metric("Est. Cost", use_case_info["cost_estimate"])
    with col_info4:
        st.metric("Statistical Power", use_case_info["statistical_power"])

    st.info(f"**Use for:** {use_case_info['description']}")

    # Show breakdown
    with st.expander("📊 Dataset Breakdown"):
        for data_type, size, _ in use_case_info["config"]:
            st.write(f"- **{data_type}**: {size} items")

    # --- Quick Generate: Choose model used for all datasets ---
    default_quick_model = "openai/gpt-5-mini"
    if default_quick_model not in AVAILABLE_MODELS:
        default_quick_model = AVAILABLE_MODELS[0] if AVAILABLE_MODELS else OPENAI_MODEL
    default_quick_index = AVAILABLE_MODELS.index(default_quick_model) if default_quick_model in AVAILABLE_MODELS else 0

    quick_gen_model = st.selectbox(
        "Generation Model (Quick Generate)",
        options=AVAILABLE_MODELS,
        index=default_quick_index,
        help="Model used for Quick Generate across all selected datasets."
    )
    st.session_state["quick_gen_model"] = quick_gen_model
    st.caption(f"Using model: {quick_gen_model} via {API_ROUTING_MODE}")

    # Generate button
    if st.button(f"🔄 Generate All Datasets ({selected_use_case})", use_container_width=True, type="primary"):
        try:
            datasets_config = use_case_info["config"]
            quick_gen_model = st.session_state.get("quick_gen_model", quick_gen_model)
            # Snapshot totals for whole batch and prepare meta collection
            ct0 = st.session_state.cost_tracker if "cost_tracker" in st.session_state else None
            batch_pre_totals = dict(ct0.totals) if ct0 else {}
            batch_pre_calls = len(ct0.by_call) if ct0 else 0
            meta_files: List[str] = []

            all_success = True

            # Show progress
            progress_bar = st.progress(0)
            status_text = st.empty()

            total_datasets = len(datasets_config)

            for idx, (data_type, size, path) in enumerate(datasets_config):
                # Snapshot cost totals before this dataset
                pre_totals = dict(st.session_state.cost_tracker.totals) if "cost_tracker" in st.session_state else {}
                pre_calls = len(st.session_state.cost_tracker.by_call) if "cost_tracker" in st.session_state else 0

                status_text.text(f"Generating {data_type} dataset ({size} items)... [{idx+1}/{total_datasets}]")
                progress_bar.progress((idx) / total_datasets)

                try:
                    prompt = DEFAULT_DATASET_PROMPTS[data_type]
                    df = asyncio.run(generate_synthetic_data(prompt, size, data_type, quick_gen_model))

                    if not df.empty:
                        df.to_csv(path, index=False)
                        # Write sidecar meta for traceability
                        # Compute cost deltas for this dataset
                        try:
                            ct = st.session_state.cost_tracker
                            delta_usage = {
                                "prompt_tokens": max(ct.totals.get("prompt_tokens", 0) - pre_totals.get("prompt_tokens", 0), 0),
                                "completion_tokens": max(ct.totals.get("completion_tokens", 0) - pre_totals.get("completion_tokens", 0), 0),
                                "total_tokens": max(ct.totals.get("total_tokens", 0) - pre_totals.get("total_tokens", 0), 0),
                            }
                            delta_cost = {
                                "input_cost_usd": round(max(ct.totals.get("input_cost_usd", 0.0) - pre_totals.get("input_cost_usd", 0.0), 0.0), 4),
                                "output_cost_usd": round(max(ct.totals.get("output_cost_usd", 0.0) - pre_totals.get("output_cost_usd", 0.0), 0.0), 4),
                                "total_cost_usd": round(max(ct.totals.get("total_cost_usd", 0.0) - pre_totals.get("total_cost_usd", 0.0), 0.0), 4),
                            }
                            delta_calls = max(len(ct.by_call) - pre_calls, 0)
                        except Exception:
                            delta_usage, delta_cost, delta_calls = {}, {}, 0

                        try:
                            meta_path = path.replace(".csv", ".meta.json")
                            with open(meta_path, "w", encoding="utf-8") as f:
                                json.dump({
                                    "model": quick_gen_model,
                                    "routing_mode": API_ROUTING_MODE,
                                    "when": pd.Timestamp.utcnow().isoformat(),
                                    "dataset_type": data_type,
                                    "size": size,
                                    "use_case": selected_use_case,
                                    "calls": delta_calls,
                                    "usage": delta_usage,
                                    "cost": delta_cost,
                                }, f, indent=2)
                            meta_files.append(meta_path)
                        except Exception:
                            pass
                        st.success(f"✅ Generated {data_type} dataset ({len(df)} rows) using {quick_gen_model} [{API_ROUTING_MODE}]")
                    else:
                        st.warning(f"⚠️ Failed to generate {data_type} dataset")
                        all_success = False
                except Exception as e:
                    st.error(f"Error generating {data_type} dataset: {e}")
                    all_success = False

            progress_bar.progress(1.0)
            status_text.text("Generation complete!")

            if all_success:
                # Reload datasets into session state
                st.session_state.df = load_classification_dataset()
                st.session_state.agent_df = load_tool_sequence_dataset()
                st.session_state.pruning_df = load_context_pruning_dataset()

                # Compute batch deltas for tokens, cost, and calls
                try:
                    ct = st.session_state.cost_tracker
                    d_tokens = max(ct.totals.get("total_tokens", 0) - batch_pre_totals.get("total_tokens", 0), 0)
                    d_cost = round(max(ct.totals.get("total_cost_usd", 0.0) - batch_pre_totals.get("total_cost_usd", 0.0), 0.0), 4)
                    d_calls = max(len(ct.by_call) - batch_pre_calls, 0)
                except Exception:
                    d_tokens, d_cost, d_calls = 0, 0.0, 0

                # Persist meta file list and summary for the expander/history
                st.session_state["last_generated_meta_files"] = meta_files
                summary = {
                    "when": pd.Timestamp.utcnow().isoformat(),
                    "use_case": selected_use_case,
                    "model": quick_gen_model,
                    "routing": API_ROUTING_MODE,
                    "total_items": use_case_info['total_items'],
                    "tokens": d_tokens,
                    "cost_usd": d_cost,
                    "calls": d_calls,
                    "meta_files": meta_files,
                }
                st.session_state["last_generation_summary"] = summary
                if "generation_history" not in st.session_state:
                    st.session_state["generation_history"] = []
                st.session_state["generation_history"].append(summary)

                st.success(f"✅ All datasets generated successfully! Total: {use_case_info['total_items']} items using {quick_gen_model} [{API_ROUTING_MODE}] • Tokens: +{d_tokens:,} • Cost: +${d_cost:.4f} • Calls: +{d_calls}")
                st.balloons()
                st.rerun()
        except Exception as e:
            st.error(f"Failed to generate datasets: {e}")

    # Compact expander to view last generation metadata
    if st.session_state.get("last_generated_meta_files"):
        with st.expander("🧾 View last generation metadata"):
            files = st.session_state.get("last_generated_meta_files", [])
            # Compact summary table
            rows = []
            for p in files:
                try:
                    with open(p, "r", encoding="utf-8") as f:
                        meta = json.load(f)
                        rows.append({
                            "dataset_type": meta.get("dataset_type"),
                            "size": meta.get("size"),
                            "total_tokens": ((meta.get("usage") or {}).get("total_tokens")),
                            "total_cost_usd": ((meta.get("cost") or {}).get("total_cost_usd")),
                        })
                except Exception:
                    pass
            if rows:
                try:
                    st.dataframe(pd.DataFrame(rows), use_container_width=True, height=min(300, 60+28*len(rows)))
                except Exception:
                    st.write(rows)
            # Raw JSON entries
            for p in files:
                try:
                    st.markdown(f"**{os.path.basename(p)}**")
                    with open(p, "r", encoding="utf-8") as f:
                        st.json(json.load(f))
                except Exception as e:
                    st.text(f"{p}: {e}")

    # History expander
    if st.session_state.get("generation_history"):
        with st.expander("🗂️ Generation history"):
            hist = st.session_state.get("generation_history", [])
            try:
                dfh = pd.DataFrame(hist)
                st.dataframe(dfh[["when","use_case","model","routing","total_items","tokens","cost_usd","calls"]].sort_values("when", ascending=False), use_container_width=True)
            except Exception:
                st.write(hist)

    st.divider()

    # --- PATCH 20: Restructure Tab 0 UI for dynamic prompt generation ---
    # Initialize prompt key
    if 'generation_prompt_text' not in st.session_state:
        st.session_state['generation_prompt_text'] = SUGGESTED_PROMPTS["Classification"][0]

    st.subheader("🎯 Generate Individual Dataset")
    c_model, c_size, c_type = st.columns(3)

    with c_model:
        # --- UPDATED: Flexible model selection from all available models ---
        default_gen_model = "openai/gpt-5-mini"
        if default_gen_model not in AVAILABLE_MODELS:
            default_gen_model = AVAILABLE_MODELS[0] if AVAILABLE_MODELS else OPENAI_MODEL

        default_index = AVAILABLE_MODELS.index(default_gen_model) if default_gen_model in AVAILABLE_MODELS else 0

        gen_model = st.selectbox(
            "Generation Model",
            options=AVAILABLE_MODELS,
            index=default_index,
            help="Select any model to generate synthetic data. Defaults to gpt-5-mini."
        )

    with c_size:
        size_options = {5: "5 Pairs", 25: "25 Pairs", 100: "100 Pairs", 1000: "1000 Pairs"}
        data_size_label = st.selectbox("Dataset Size", options=list(size_options.values()), index=2)
        data_size = list(size_options.keys())[list(size_options.values()).index(data_size_label)]

    with c_type:
        # --- PATCH 17: Added "Context Pruning" option ---
        data_type = st.selectbox("Data Type", ["Classification", "Tool/Agent Sequence", "Context Pruning"])

    # Define the helper function for random prompt selection (user-input-aware)
    def set_random_prompt(current_data_type):
        """Generate random prompt that's aware of user's existing input."""
        import random

        prompts = SUGGESTED_PROMPTS.get(current_data_type, [])
        if not prompts:
            st.session_state['generation_prompt_text'] = f"Generate synthetic data for the '{current_data_type}' task."
            return

        # Get current user input
        current_input = st.session_state.get('generation_prompt_text', '')

        # Select base prompt
        base_prompt = random.choice(prompts)

        # Build context
        context = {
            'dataset_size': st.session_state.get('data_size', 100),
            'columns': ['query', 'classification'] if current_data_type == 'Classification' else ['query', 'expected_sequence'],
            'suggested_approach': base_prompt.split('\n')[0] if '\n' in base_prompt else base_prompt[:100],
            'success_criteria': '85%+ accuracy on test set'
        }

        # Enhance with user input
        enhanced = enhance_prompt_with_user_input(base_prompt, current_input, context)

        st.session_state['generation_prompt_text'] = enhanced

    # 1. Random Prompt Generator Button
    if st.button(f"🎲 Randomly Generate Prompt for {data_type}", use_container_width=True):
        set_random_prompt(data_type)
        st.rerun() # Rerun to update the text area immediately

    # 2. Text Area (linked to session state)
    st.text_area(
        "Describe the dataset requirements:",
        height=150,
        key='generation_prompt_text',
        help="Provide a detailed description of the type of queries, the required complexity, and the target labels/sequences."
    )

    # 3. Suggestion Pills
    st.markdown("##### Suggested Starting Points (Click to use):")
    selected_prompts = SUGGESTED_PROMPTS.get(data_type, [])

    if selected_prompts:
        cols = st.columns(len(selected_prompts))
        for i, prompt in enumerate(selected_prompts):
            with cols[i]:
                if st.button(prompt[:30] + "...", key=f"pill_{data_type}_{i}", help=prompt, use_container_width=True):
                    st.session_state['generation_prompt_text'] = prompt
                    st.rerun() # Rerun to update the text area
    else:
        st.info("No suggestions available for this data type.")

    # 4. Execution Button (reads from session state)
    if st.button("Generate & Save Dataset", use_container_width=True):
        generation_prompt_used = st.session_state.get('generation_prompt_text', "").strip()
        if not generation_prompt_used:
            st.warning("Please provide a dataset description prompt.")
        else:
            # Need to run generation synchronously in Streamlit context
            with st.spinner(f"Generating {data_size} {data_type} items..."):
                new_df = asyncio.run(generate_synthetic_data(generation_prompt_used, data_size, data_type, gen_model))

            if not new_df.empty:

                # Check if we are generating for the classification tests (Test 1-3)
                if data_type == "Classification" and "classification" in new_df.columns:
                    # Re-initialize the dataframe with all necessary classification skeleton columns
                    st.session_state.df = new_df[["query", "classification"]].copy()
                    for col in SKELETON_COLUMNS:
                        if col not in st.session_state.df.columns:
                            st.session_state.df[col] = None

                    # Save to file
                    save_dataset_to_file(new_df[["query", "classification"]], data_type, model_used=gen_model, routing_mode=API_ROUTING_MODE)
                    st.success(f"Generated {len(new_df)} classification items using {gen_model} [{API_ROUTING_MODE}] and updated the main dataset (Tests 1-3).")

                    # Visualize generated dataset
                    st.subheader("📊 Generated Dataset Analysis")
                    visualize_dataset_composition(new_df, dataset_type=data_type)
                    st.dataframe(new_df.head(10), use_container_width=True)

                # If it's an Agent test set, save it separately/use a different structure
                elif data_type == "Tool/Agent Sequence" and "expected_sequence" in new_df.columns:
                    # Save agent data to a specific session state key
                    st.session_state.agent_df = new_df[["query", "expected_sequence"]].copy()

                    # Save to file
                    save_dataset_to_file(new_df, data_type, model_used=gen_model, routing_mode=API_ROUTING_MODE)
                    st.success(f"Generated {len(new_df)} agent sequence items using {gen_model} [{API_ROUTING_MODE}]. Ready for Test 5.")

                    # Visualize generated dataset
                    st.subheader("📊 Generated Dataset Analysis")
                    visualize_dataset_composition(new_df, dataset_type=data_type)
                    st.dataframe(new_df.head(10), use_container_width=True)

                # --- PATCH 17: Save Pruning Data to session state ---
                elif data_type == "Context Pruning" and "new_question" in new_df.columns:
                    # Test 4 requires specific columns defined in PruningDataItem
                    required_cols = list(PruningDataItem.model_fields.keys())
                    st.session_state.pruning_df = new_df[[c for c in required_cols if c in new_df.columns]].copy()

                    # Save to file
                    save_dataset_to_file(new_df, data_type, model_used=gen_model, routing_mode=API_ROUTING_MODE)
                    st.success(f"Generated {len(new_df)} context pruning cases using {gen_model} [{API_ROUTING_MODE}]. Ready for Test 4.")

                    # Visualize generated dataset
                    st.subheader("📊 Generated Dataset Analysis")
                    visualize_dataset_composition(new_df, dataset_type=data_type)
                    st.dataframe(new_df.head(10), use_container_width=True)
                # ----------------------------------------------------

                st.dataframe(new_df, use_container_width=True)

                # --- Force rerun to update the Dataset Status display ---
                st.rerun()
            else:
                st.error("Data generation failed to produce valid output.")
    # -------------------------------------------------------------


# ---------- Test 1 ----------
with tabs[1]:
    # Dashboard header
    render_test_flow_diagram(1, "Test 1: Two-Model Classification + F1/Latency Analysis")

    # Dataset preview for classification tests (Tests 1-3)
    st.subheader("Dataset Preview (Classification)")
    _df_preview = st.session_state.df if "df" in st.session_state else pd.DataFrame(columns=SKELETON_COLUMNS)

    # Show dataset source
    if os.path.exists(CLASSIFICATION_DATASET_PATH):
        dataset_source = f"📁 Source: `{CLASSIFICATION_DATASET_PATH}` ({len(_df_preview)} rows)"
    else:
        dataset_source = "⚠️ No dataset loaded. Generate datasets in the Preparation tab."

    st.caption(dataset_source)
    st.dataframe(_style_selected_rows(_df_preview, ROW_LIMIT_N), use_container_width=True, height=250)
    st.caption(f"Highlighted {len(_subset_for_run(_df_preview, ROW_LIMIT_N))} of {len(_df_preview)} rows will be used in Tests 1-3.")

    st.divider()

    # --- Per-Test Model Selection (Test 1) ---
    st.subheader("Model Selection (Test 1)")
    t1_col1, t1_col2 = st.columns(2)
    with t1_col1:
        t1_use_ollama = st.checkbox("Use OpenRouter (Test 1)", value=True, key="t1_use_ollama")
        _t1_or_ids = list(OPENROUTER_MODEL_METADATA.keys())
        _t1_or_default = _t1_or_ids.index(OPENROUTER_MODEL) if OPENROUTER_MODEL in _t1_or_ids else 0
        t1_openrouter_model = st.selectbox("OpenRouter model (Test 1)", options=_t1_or_ids, index=_t1_or_default, key="t1_or_model")
    with t1_col2:
        t1_use_openai = st.checkbox("Use OpenAI (Test 1)", value=True, key="t1_use_openai")
        _t1_oai_ids = list(OPENAI_MODEL_METADATA.keys())
        _t1_oai_default = _t1_oai_ids.index(OPENAI_MODEL) if OPENAI_MODEL in _t1_oai_ids else 0
        t1_openai_model = st.selectbox("OpenAI model (Test 1)", options=_t1_oai_ids, index=_t1_oai_default, key="t1_oai_model")

    # Collapsible configuration section
    with st.expander("⚙️ Test Configuration", expanded=False):
        row_limit_display = [k for k, v in ROW_LIMIT_OPTIONS.items() if v == ROW_LIMIT_N]
        row_limit_str = row_limit_display[0] if row_limit_display else f"{ROW_LIMIT_N} rows"
        st.markdown(f"""
        Models: OpenRouter={t1_openrouter_model if t1_use_ollama else '—'}; OpenAI={t1_openai_model if t1_use_openai else '—'}
        Row Limit: {row_limit_str}
        Providers Enabled: OpenRouter={t1_use_ollama}, OpenAI={t1_use_openai}
        Explain Confusion Matrix: {explain_cm}
        """)

    st.divider()

    up = st.file_uploader("Upload CSV (query, classification)", type=["csv"], key="t1_up")
    if st.button("Load uploaded (replace DF)", key="t1_load") and up:
        # ... (loading logic unchanged) ...
        st.rerun()

    if st.button("▶️ Run Test 1", type="primary", use_container_width=True):
        # Persist per-test OpenAI model override for this run
        st.session_state['openai_model_override'] = t1_openai_model

        overrides = {
            'use_openai': t1_use_openai,
            'openai_model': t1_openai_model,
            'use_ollama': t1_use_ollama,
            'openrouter_model': t1_openrouter_model,
        }
        capture_run_config("Test 1", overrides)  # CAPTURE

        run_classification_flow(
            include_third_model=False,
            use_openai_override=t1_use_openai,
            use_ollama_override=t1_use_ollama,
            openrouter_model_override=t1_openrouter_model,
        )

        # The analysis code below remains the same, but now runs on fresh data
        df = _subset_for_run(st.session_state.df, ROW_LIMIT_N)

        if len(df) and "classification" in df.columns:
            # 1. FIRST: Progress replay (what just happened)
            st.subheader("📈 Processing Timeline")
            render_progress_replay("classification")

            st.divider()

            # 2. THEN: Organized results with subtabs
            st.subheader("📊 Test Results")
            render_organized_results(
                df,
                test_type="classification",
                model_cols=["openrouter_mistral", "openai"],
                model_names=["Mistral (OpenRouter)", "OpenAI"]
            )

            st.divider()

        # --- Save results at the end of the test run ---
        save_results_df(st.session_state.df, "Test 1", ROW_LIMIT_N)
        # --- PATCH 7: Structured Summary Call for Test 1 ---
        if len(df) and "classification" in df.columns:
            report_text = f"Test 1 Results (N={len(df)}): Mistral Avg Latency: {df['latency_openrouter_mistral'].mean():.2f}s, OpenAI Avg Latency: {df['latency_openai'].mean():.2f}s. Performance reports calculated."
            loop = asyncio.get_event_loop()
            loop.run_until_complete(display_final_summary_for_test("Test 1 Classification", report_text))
        # ----------------------------------------------------

# ---------- Test 2 ----------
with tabs[2]:
    # Dashboard header
    render_test_flow_diagram(2, "Test 2: Advanced Ensembling with Per-Class F1 Weighting")


    # --- Per-Test Model Selection (Test 2) ---
    st.subheader("Model Selection (Test 2)")
    t2_c1, t2_c2 = st.columns(2)
    with t2_c1:
        t2_use_ollama = st.checkbox("Use OpenRouter (Test 2)", value=True, key="t2_use_ollama")
        _t2_or_ids = list(OPENROUTER_MODEL_METADATA.keys())
        _t2_or_default = _t2_or_ids.index(OPENROUTER_MODEL) if OPENROUTER_MODEL in _t2_or_ids else 0
        t2_openrouter_model = st.selectbox("OpenRouter model (Test 2)", options=_t2_or_ids, index=_t2_or_default, key="t2_or_model")
    with t2_c2:
        t2_use_openai = st.checkbox("Use OpenAI (Test 2)", value=True, key="t2_use_openai")
        _t2_oai_ids = list(OPENAI_MODEL_METADATA.keys())
        _t2_oai_default = _t2_oai_ids.index(OPENAI_MODEL) if OPENAI_MODEL in _t2_oai_ids else 0
        t2_openai_model = st.selectbox("OpenAI model (Test 2)", options=_t2_oai_ids, index=_t2_oai_default, key="t2_oai_model")

    t2_third_kind = st.selectbox("Third model provider (Test 2)", ["None", "OpenRouter", "OpenAI"], index=["None","OpenRouter","OpenAI"].index(THIRD_KIND if THIRD_KIND in ["None","OpenRouter","OpenAI"] else "None"), key="t2_third_kind")
    t2_third_model = ""
    if t2_third_kind == "OpenRouter":
        t2_third_model = st.selectbox("Third model (OpenRouter)", options=_t2_or_ids, key="t2_third_model_or")
    elif t2_third_kind == "OpenAI":
        t2_third_model = st.selectbox("Third model (OpenAI)", options=_t2_oai_ids, key="t2_third_model_oai")
    # Collapsible configuration section
    with st.expander("⚙️ Test Configuration", expanded=False):
        row_limit_display = [k for k, v in ROW_LIMIT_OPTIONS.items() if v == ROW_LIMIT_N]
        row_limit_str = row_limit_display[0] if row_limit_display else f"{ROW_LIMIT_N} rows"
        st.markdown(f"""
        Models: OpenRouter={t2_openrouter_model if t2_use_ollama else '—'}; OpenAI={t2_openai_model if t2_use_openai else '—'}; Third={t2_third_model if t2_third_kind != 'None' else '—'}
        Row Limit: {row_limit_str}
        Weighting Strategy: Confidence × Per-Class F1 Score
        Explain Confusion Matrix: {explain_cm}
        """)

    st.divider()
    st.info("This test uses a smarter weighting: score = confidence * F1-score-of-the-predicted-class.")


    if st.button("▶️ Run Test 2", type="primary", use_container_width=True):
        # Persist per-test OpenAI model override for this run
        st.session_state['openai_model_override'] = t2_openai_model

        overrides = {
            'use_openai': t2_use_openai,
            'openai_model': t2_openai_model,
            'use_ollama': t2_use_ollama,
            'openrouter_model': t2_openrouter_model,
            'third_kind': t2_third_kind,
            'third_model': t2_third_model,
        }
        capture_run_config("Test 2", overrides)  # CAPTURE
        # First, run classification for configured models
        run_classification_flow(
            include_third_model=(t2_third_kind != "None"),
            use_openai_override=t2_use_openai,
            use_ollama_override=t2_use_ollama,
            openrouter_model_override=t2_openrouter_model,
            third_kind_override=t2_third_kind,
            third_model_override=t2_third_model,
        )



        # First, ensure classifications exist by running Test 1 logic
        # ... (classification run logic as in Test 1) ...
        df = _subset_for_run(st.session_state.df, ROW_LIMIT_N)
        if len(df):
            # --- PATCH 13: Dynamic name for reporting ---
            third_model_name = get_third_model_display_name()

            y_true = df["classification"].tolist()
            # Generate reports including the third model (for ensemble calculation)
            report_m = generate_classification_report(y_true, df["classification_result_openrouter_mistral"].tolist(), "Mistral", explain=False)
            report_g = generate_classification_report(y_true, df["classification_result_openai"].tolist(), "OpenAI", explain=False)
            report_t = generate_classification_report(y_true, df["classification_result_third"].tolist(), third_model_name, explain=False) if "classification_result_third" in df else None
            # ---------------------------------------------

            # Create a map from class to its F1 score for each model
            f1_maps = {
                "mistral": {label: data['f1-score'] for label, data in report_m.items() if isinstance(data, dict)} if report_m else {},
                "gpt5": {label: data['f1-score'] for label, data in report_g.items() if isinstance(data, dict)} if report_g else {},
                "third": {label: data['f1-score'] for label, data in report_t.items() if isinstance(data, dict)} if report_t else {},
            }

            picks = df.apply(lambda row: _smarter_weighted_pick_row(row, f1_maps), axis=1)
            df['weighted_pick_model'], df['weighted_pick_label'] = zip(*picks)
            st.session_state.df.loc[df.index, ['weighted_pick_model', 'weighted_pick_label']] = df[['weighted_pick_model', 'weighted_pick_label']]

            # 1. FIRST: Progress replay
            st.subheader("📈 Processing Timeline")
            render_progress_replay("classification")

            st.divider()

            # 2. THEN: Organized results with subtabs
            st.subheader("📊 Test Results")
            render_organized_results(
                df,
                test_type="classification",
                model_cols=["openrouter_mistral", "openai", "third"],
                model_names=["Mistral (OpenRouter)", "OpenAI", third_model_name]
            )

            st.divider()

            # 2.5. Confidence Distribution Analysis
            st.subheader("📊 Confidence Distribution Analysis")

            conf_cols = [
                ('classification_result_openrouter_mistral_confidence', 'Mistral'),
                ('classification_result_openai_confidence', 'OpenAI'),
                ('classification_result_third_confidence', third_model_name)
            ]

            fig_conf = go.Figure()

            for col, name in conf_cols:
                if col in df.columns:
                    confidences = df[col].dropna()
                    fig_conf.add_trace(go.Violin(
                        y=confidences,
                        name=name,
                        box_visible=True,
                        meanline_visible=True,
                        hovertemplate=f'<b>{name}</b><br>Confidence: %{{y:.3f}}<extra></extra>'
                    ))

            fig_conf.update_layout(
                title="Confidence Score Distribution by Model",
                yaxis_title="Confidence Score",
                height=400,
                showlegend=True
            )

            st.plotly_chart(fig_conf, use_container_width=True, config=PLOTLY_CONFIG)

            st.divider()

            # 3. Ensemble results
            with st.expander("🎯 Ensemble Performance (Weighted Pick)", expanded=True):
                st.info("This ensemble uses: score = confidence × F1-score-of-the-predicted-class")
                generate_classification_report(y_true, df["weighted_pick_label"].tolist(), "Smarter Weighted Pick", explain=explain_cm)

            # --- Save results at the end of the test run ---
            save_results_df(st.session_state.df, "Test 2", ROW_LIMIT_N)
            # --- PATCH 7: Structured Summary Call for Test 2 ---
            report_text = f"Test 2 Ensemble Results (N={len(df)}): Weighted Pick Macro F1: {report_m.get('weighted avg', {}).get('f1-score', 0.0):.4f}. Focus analysis on model disagreement."
            loop = asyncio.get_event_loop()
            loop.run_until_complete(display_final_summary_for_test("Test 2 Advanced Ensembling", report_text))
            # ----------------------------------------------------

# ---------- Test 3 ----------
with tabs[3]:
    # Dashboard header
    render_test_flow_diagram(3, "Test 3: LLM as Judge")

    # --- Classification Models (Test 3) ---
    st.subheader("Classification Models (Test 3)")
    t3_c1, t3_c2 = st.columns(2)
    with t3_c1:
        t3_use_ollama = st.checkbox("Use OpenRouter (Test 3)", value=True, key="t3_use_ollama")
        _t3_or_ids = list(OPENROUTER_MODEL_METADATA.keys())
        _t3_or_default = _t3_or_ids.index(OPENROUTER_MODEL) if OPENROUTER_MODEL in _t3_or_ids else 0
        t3_openrouter_model = st.selectbox("OpenRouter model (Test 3)", options=_t3_or_ids, index=_t3_or_default, key="t3_or_model")
    with t3_c2:
        t3_use_openai = st.checkbox("Use OpenAI (Test 3)", value=True, key="t3_use_openai")
        _t3_oai_ids = list(OPENAI_MODEL_METADATA.keys())
        _t3_oai_default = _t3_oai_ids.index(OPENAI_MODEL) if OPENAI_MODEL in _t3_oai_ids else 0
        t3_openai_model = st.selectbox("OpenAI model (Test 3)", options=_t3_oai_ids, index=_t3_oai_default, key="t3_oai_model")

    t3_third_kind = st.selectbox("Third model provider (Test 3)", ["None", "OpenRouter", "OpenAI"], index=["None","OpenRouter","OpenAI"].index(THIRD_KIND if THIRD_KIND in ["None","OpenRouter","OpenAI"] else "None"), key="t3_third_kind")
    t3_third_model = ""
    if t3_third_kind == "OpenRouter":
        t3_third_model = st.selectbox("Third model (OpenRouter)", options=_t3_or_ids, key="t3_third_model_or")
    elif t3_third_kind == "OpenAI":
        t3_third_model = st.selectbox("Third model (OpenAI)", options=_t3_oai_ids, key="t3_third_model_oai")

    # Collapsible configuration section
    with st.expander("⚙️ Test Configuration", expanded=False):
        row_limit_display = [k for k, v in ROW_LIMIT_OPTIONS.items() if v == ROW_LIMIT_N]
        row_limit_str = row_limit_display[0] if row_limit_display else f"{ROW_LIMIT_N} rows"
        st.markdown(f"""
        Models: OpenRouter={t3_openrouter_model if t3_use_ollama else '—'}; OpenAI={t3_openai_model if t3_use_openai else '—'}; Third={t3_third_model if t3_third_kind != 'None' else '—'}
        Judge Model: {st.session_state.get('judge_model', 'openai/gpt-5-mini')}
        Row Limit: {row_limit_str}
        Judging Strategy: Weighted F1 scores + confidence
        Explain Confusion Matrix: {explain_cm}
        """)

    st.divider()

    # --- NEW: Judge Model Selector ---
    col1, col2 = st.columns([3, 1])
    with col1:
        default_judge_model = "openai/gpt-5-mini"
        if default_judge_model not in AVAILABLE_MODELS:
            default_judge_model = AVAILABLE_MODELS[0] if AVAILABLE_MODELS else OPENAI_MODEL

        default_judge_index = AVAILABLE_MODELS.index(default_judge_model) if default_judge_model in AVAILABLE_MODELS else 0

        judge_model = st.selectbox(
            "Judge Model",
            options=AVAILABLE_MODELS,
            index=default_judge_index,
            key='judge_model',
            help="Select the model to act as the judge. Defaults to gpt-5-mini for cost-effectiveness."
        )

    with col2:
        st.metric("Judge Model", _to_native_model_id(judge_model))

    if st.button("▶️ Run Test 3 (Judge)", type="primary", use_container_width=True):
        # Persist per-test OpenAI model override for this run
        st.session_state['openai_model_override'] = t3_openai_model

        overrides = {
            'use_openai': t3_use_openai,
            'openai_model': t3_openai_model,
            'use_ollama': t3_use_ollama,
            'openrouter_model': t3_openrouter_model,
            'third_kind': t3_third_kind,
            'third_model': t3_third_model,
        }
        capture_run_config("Test 3", overrides)  # CAPTURE
        # Always ensure fresh classification data is available for all models
        run_classification_flow(
            include_third_model=(t3_third_kind != "None"),
            use_openai_override=t3_use_openai,
            use_ollama_override=t3_use_ollama,
            openrouter_model_override=t3_openrouter_model,
            third_kind_override=t3_third_kind,
            third_model_override=t3_third_model,
        )

        df = _subset_for_run(st.session_state.df, ROW_LIMIT_N)

        if not len(df):
            st.info("No rows to judge.")
        else:
            with st.spinner("Running Judge on classified rows..."):
                # --- 1. Compute global F1s for weights (null-safe + filtered) ---
                y_true_all = df["classification"].fillna("").map(_normalize_label).tolist()

                # Helper to get F1 scores
                def get_f1_report_dict(pred_series):
                    pred_list = pred_series.fillna("").map(_normalize_label).tolist()
                    valid_indices = [i for i, (t, p) in enumerate(zip(y_true_all, pred_list)) if t and p]
                    if not valid_indices: return None
                    y_true_f = [y_true_all[i] for i in valid_indices]
                    y_pred_f = [pred_list[i] for i in valid_indices]
                    return classification_report(y_true_f, y_pred_f, output_dict=True, zero_division=0)

                report_m = get_f1_report_dict(df.get("classification_result_openrouter_mistral", pd.Series(dtype='str')))
                report_g = get_f1_report_dict(df.get("classification_result_openai", pd.Series(dtype='str')))
                report_t = get_f1_report_dict(df.get("classification_result_third", pd.Series(dtype='str')))

                global_f1s = {
                    "mistral": report_m['macro avg']['f1-score'] if report_m else 0.0,
                    "gpt5": report_g['macro avg']['f1-score'] if report_g else 0.0,
                    "third": report_t['macro avg']['f1-score'] if report_t else 0.0,
                }

                # --- 2. Define and run the async judge worker ---
                async def _judge_all_rows_async(df_to_judge, f1_scores):

                    async def judge_one_row(idx, row):
                        payload = {
                            "query": row.get("query"),
                            "candidates": {
                                "mistral": {"label": row.get("classification_result_openrouter_mistral"), "confidence": row.get("classification_result_openrouter_mistral_confidence"), "rationale": row.get("classification_result_openrouter_mistral_rationale")},
                                "gpt5": {"label": row.get("classification_result_openai"), "confidence": row.get("classification_result_openai_confidence"), "rationale": row.get("classification_result_openai_rationale")},
                                "third": {"label": row.get("classification_result_third"), "confidence": row.get("classification_result_third_confidence"), "rationale": row.get("classification_result_third_rationale")} if pd.notna(row.get("classification_result_third")) else None
                            },
                            "weighted_scores": {k: v for k, v in [
                                ("mistral", (f1_scores.get("mistral", 0) * float(row.get("classification_result_openrouter_mistral_confidence") or 0))),
                                ("gpt5", (f1_scores.get("gpt5", 0) * float(row.get("classification_result_openai_confidence") or 0))),
                                ("third", (f1_scores.get("third", 0) * float(row.get("classification_result_third_confidence") or 0)))
                            ] if v > 0}
                        }
                        try:
                            # Using run_judge_ollama as the primary (which calls OpenRouter)
                            result = await run_judge_ollama(payload)
                        except Exception as e:
                            result = {"final_choice_model": None, "final_label": None, "judge_rationale": f"Judge Error: {e}"}

                        # Return index with the result for safe mapping
                        return idx, result

                    tasks = [judge_one_row(idx, row) for idx, row in df_to_judge.iterrows() if str(row.get("query", "")).strip()]
                    return await asyncio.gather(*tasks)

                # --- 3. Execute the async runner and write back results safely ---
                loop = asyncio.get_event_loop()
                judge_results = loop.run_until_complete(_judge_all_rows_async(df, global_f1s))

                for idx, res in judge_results:
                    st.session_state.df.at[idx, "judge_choice_model"] = res.get("final_choice_model")
                    st.session_state.df.at[idx, "judge_choice_label"] = _normalize_label(res.get("final_label"))
                    st.session_state.df.at[idx, "judge_rationale"] = res.get("judge_rationale")

                st.success(f"Judging complete for {len(judge_results)} rows.")

                # Get fresh data with judge results
                final_df = _subset_for_run(st.session_state.df, ROW_LIMIT_N)
                third_model_name = get_third_model_display_name()

                # 1. FIRST: Progress replay
                st.subheader("📈 Processing Timeline")
                render_progress_replay("classification")

                st.divider()

                # 2. THEN: Organized results with subtabs
                st.subheader("📊 Test Results (Individual Models)")
                render_organized_results(
                    final_df,
                    test_type="classification",
                    model_cols=["openrouter_mistral", "openai", "third"],
                    model_names=["Mistral (OpenRouter)", "OpenAI", third_model_name]
                )

                st.divider()

                # 2.5. Judge Decision Flow Visualization
                st.subheader("👨‍⚖️ Judge Decision Flow")

                # Create Sankey diagram showing which models were chosen
                judge_choices = final_df['judge_choice_model'].value_counts()

                # Build flow data
                labels = ['Mistral', 'OpenAI', third_model_name, 'Judge Decision']
                source = [0, 1, 2]  # Models
                target = [3, 3, 3]  # All flow to judge
                values = [
                    judge_choices.get('mistral', 0),
                    judge_choices.get('gpt5', 0),
                    judge_choices.get('third', 0)
                ]

                fig_sankey = go.Figure(data=[go.Sankey(
                    node=dict(
                        pad=15,
                        thickness=20,
                        label=labels,
                        color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
                    ),
                    link=dict(
                        source=source,
                        target=target,
                        value=values,
                        color=['rgba(31,119,180,0.4)', 'rgba(255,127,14,0.4)', 'rgba(44,160,44,0.4)']
                    )
                )])

                fig_sankey.update_layout(
                    title="Model Selection by Judge",
                    height=300
                )

                st.plotly_chart(fig_sankey, use_container_width=True, config=PLOTLY_CONFIG)

                st.divider()

                # 3. Judge-specific performance
                with st.expander("👨‍⚖️ Judge Performance (Detailed)", expanded=True):
                    st.info("The judge selects the best answer from the 3 models based on weighted F1 scores and confidence.")
                    generate_classification_report(
                        final_df["classification"].tolist(),
                        final_df["judge_choice_label"].tolist(),
                        "LLM Judge",
                        explain=explain_cm
                    )

                    # Show judge decisions
                    st.subheader("Judge Decisions")
                    display_cols = [
                        "query", "classification",
                        "classification_result_openrouter_mistral", "classification_result_openai", "classification_result_third",
                        "judge_choice_model", "judge_choice_label", "judge_rationale"
                    ]
                    existing_cols = [col for col in display_cols if col in final_df.columns]

                    # Rename third column for clarity
                    display_df = final_df.copy()
                    if "classification_result_third" in existing_cols:
                        display_df = display_df.rename(columns={"classification_result_third": f"classification_result_{third_model_name}"})
                        existing_cols = [col if col != "classification_result_third" else f"classification_result_{third_model_name}" for col in existing_cols]

                    # Truncate judge rationale for display to prevent MemoryError
                    judge_display_df = display_df[existing_cols].copy()
                    if "judge_rationale" in judge_display_df.columns:
                        judge_display_df["judge_rationale"] = judge_display_df["judge_rationale"].astype(str).str.slice(0, 150) + "..."
                    st.dataframe(judge_display_df, use_container_width=True)

                    # Download option
                    csv = display_df[existing_cols].to_csv(index=False).encode('utf-8')
                    st.download_button(
                        "⬇️ Download Judge Results",
                        data=csv,
                        file_name="test3_judge_results.csv",
                        mime="text/csv",
                        use_container_width=True
                    )

                # --- Save results at the end of the test run ---
                save_results_df(st.session_state.df, "Test 3", ROW_LIMIT_N)
                # --- PATCH 7: Structured Summary Call for Test 3 ---
                report_text = f"Test 3 LLM Judge Results (N={len(final_df)}): Judge selection finalized. Focus analysis on optimizing judge prompt or candidate presentation."
                loop = asyncio.get_event_loop()
                loop.run_until_complete(display_final_summary_for_test("Test 3 LLM as Judge", report_text, JUDGE_INSTRUCTIONS))
                # ----------------------------------------------------


# ---------- Test 4 ----------
with tabs[4]:
    show_main_df_previews = False

    # Dashboard header
    render_test_flow_diagram(4, "Test 4: Quantitative Context Pruning & Action")

    # Collapsible configuration section
    with st.expander("⚙️ Test Configuration", expanded=False):
        st.markdown(f"""
        **Pruner Model:** Configurable (default: gpt-5-mini)
        **Test Data Source:** Generated data or {CONTEXT_PRUNING_DATASET_PATH}
        **Metrics:** Action Accuracy + Jaccard Similarity for kept keys
        **Expected Columns:** instruction, summary, user_msgs, agent_resps, tool_logs, new_question, expected_action, expected_kept_keys
        """)

    st.divider()

    # --- NEW: Pruner Model Selector ---
    col1, col2 = st.columns([3, 1])
    with col1:
        default_pruner_model = "openai/gpt-5-mini"
        if default_pruner_model not in AVAILABLE_MODELS:
            default_pruner_model = AVAILABLE_MODELS[0] if AVAILABLE_MODELS else OPENAI_MODEL

        default_pruner_index = AVAILABLE_MODELS.index(default_pruner_model) if default_pruner_model in AVAILABLE_MODELS else 0

        pruner_model = st.selectbox(
            "Pruner Model",
            options=AVAILABLE_MODELS,
            index=default_pruner_index,
            key='pruner_model',
            help="Select the model to perform context pruning. Defaults to gpt-5-mini for cost-effectiveness."
        )

    with col2:
        st.metric("Pruner Model", _to_native_model_id(pruner_model))

    st.info(f"This test runs the pruner against either generated data or `{CONTEXT_PRUNING_DATASET_PATH}`.")
    st.code("Expected columns: instruction, summary, user_msgs, agent_resps, tool_logs, new_question, expected_action, expected_kept_keys (comma-separated)")

    # --- PATCH 18: Check for session state data first ---
    if not st.session_state.pruning_df.empty:
        pruning_df = st.session_state.pruning_df
        st.subheader(f"Pruning Testset Preview (Generated Data, N={len(pruning_df)})")
        st.dataframe(pruning_df, use_container_width=True)
        loaded_from_file = False
    else:
        # Fallback to file if session state is empty
        try:
            pruning_df = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH).fillna("")
            st.subheader(f"Pruning Testset Preview (Loaded from {CONTEXT_PRUNING_DATASET_PATH})")
            st.dataframe(pruning_df, use_container_width=True)
            loaded_from_file = True
        except FileNotFoundError:
            st.error(f"Create `{CONTEXT_PRUNING_DATASET_PATH}` or generate data in the Preparation tab to run this test.")
            pruning_df = None # Ensure df is None if not found
            loaded_from_file = True # Treat as file error for messaging
    # -------------------------------------------------------

    if st.button("▶️ Run Test 4 (Pruning)", type="primary", use_container_width=True) and pruning_df is not None:
        capture_run_config("Test 4") # CAPTURE
        async def run_all_pruners():
            results = []
            # Note: The logic below assumes the column names match the PruningDataItem schema,
            # which is guaranteed if loaded from session state (PATCH 17) or if the CSV is compliant.
            for _, row in pruning_df.iterrows():
                # The columns in the dataframe are already assumed to be strings/pipe-separated.
                context_items = {
                    "instruction": row.get("instruction", ""),
                    "summary": row.get("summary", ""),
                    "user_messages": str(row.get("user_msgs", "")).split("||"),
                    "agent_responses": str(row.get("agent_resps", "")).split("||"),
                    "tool_logs": str(row.get("tool_logs", "")).split("||"),
                }
                payload = { "context": context_items, "new_question": row.get("new_question", "") }
                results.append(run_pruner(payload))
            return await asyncio.gather(*results)

        with st.spinner(f"Running pruner on {len(pruning_df)} test cases..."):
            pruner_outputs = asyncio.run(run_all_pruners())

        correct_actions, key_scores = 0, []
        results_data = [] # For the detailed results table

        for i, row in pruning_df.iterrows():
            output = pruner_outputs[i]

            # --- Action Comparison ---
            model_action = output.get('action')
            expected_action = row['expected_action']
            action_correct = (model_action == expected_action)
            if action_correct:
                correct_actions += 1

            # --- Kept Keys Comparison (Jaccard) ---
            # Ensure expected_kept_keys is treated as a string before splitting
            expected_keys = set(str(row.get('expected_kept_keys', '')).split(','))
            expected_keys.discard('') # Remove empty string if split results in one
            model_keys_raw = output.get('kept_context_keys', [])
            model_keys = set(model_keys_raw) if isinstance(model_keys_raw, list) else set()

            intersection = len(expected_keys.intersection(model_keys))
            union = len(expected_keys.union(model_keys))
            jaccard_score = intersection / union if union > 0 else 0
            key_scores.append(jaccard_score)

            results_data.append({
                "Question": row['new_question'],
                "Expected Action": expected_action,
                "Model Action": model_action,
                "Action Correct": "✅" if action_correct else "❌",
                "Expected Keys": ", ".join(sorted(expected_keys)),
                "Model Keys": ", ".join(sorted(model_keys)),
                "Key Score (Jaccard)": jaccard_score
            })

        action_accuracy = correct_actions / len(pruning_df) if len(pruning_df) > 0 else 0
        avg_key_score = sum(key_scores) / len(key_scores) if key_scores else 0

        # Create results DataFrame
        results_df = pd.DataFrame(results_data)

        # Use organized rendering
        st.subheader("📊 Test Results")
        render_kpi_metrics(results_df, test_type="pruning")

        st.divider()
        st.subheader("📊 Pruning Analysis Visualizations")

        viz_col1, viz_col2 = st.columns(2)

        with viz_col1:
            # Heatmap: Action vs. Count of Kept Keys
            st.markdown("**Heatmap: Action vs. Number of Kept Keys**")

            # Prepare data
            heatmap_data = []
            for _, row in results_df.iterrows():
                num_keys = len(row['Model Keys'].split(', ')) if row['Model Keys'] else 0
                heatmap_data.append({
                    'Action': row['Model Action'],
                    'Num_Keys': num_keys
                })

            heatmap_df = pd.DataFrame(heatmap_data)
            pivot = heatmap_df.groupby(['Action', 'Num_Keys']).size().reset_index(name='Count')
            pivot_matrix = pivot.pivot_table(index='Action', columns='Num_Keys', values='Count', fill_value=0)

            fig_heatmap = go.Figure(data=go.Heatmap(
                z=pivot_matrix.values,
                x=pivot_matrix.columns,
                y=pivot_matrix.index,
                colorscale='Blues',
                text=pivot_matrix.values,
                texttemplate='%{text}',
                textfont={"size": 14},
                hovertemplate='Action: %{y}<br>Keys: %{x}<br>Count: %{z}<extra></extra>',
                colorbar=dict(title="Frequency")
            ))

            fig_heatmap.update_layout(
                xaxis_title="Number of Keys Kept",
                yaxis_title="Action Type",
                height=300
            )

            st.plotly_chart(fig_heatmap, use_container_width=True, config=PLOTLY_CONFIG)

        with viz_col2:
            # Stacked bar: Specific kept keys by action
            st.markdown("**Key Distribution by Action (Stacked)**")

            # Parse all kept keys by action
            key_action_data = []
            for _, row in results_df.iterrows():
                action = row['Model Action']
                keys = [k.strip() for k in row['Model Keys'].split(',') if k.strip()]
                for key in keys:
                    key_action_data.append({'Action': action, 'Key': key})

            if key_action_data:
                ka_df = pd.DataFrame(key_action_data)
                key_counts = ka_df.groupby(['Action', 'Key']).size().reset_index(name='Count')

                fig_stacked = go.Figure()

                actions = key_counts['Action'].unique()
                keys = key_counts['Key'].unique()

                for key in keys:
                    key_data = key_counts[key_counts['Key'] == key]
                    fig_stacked.add_trace(go.Bar(
                        name=key,
                        x=key_data['Action'],
                        y=key_data['Count'],
                        text=key_data['Count'],
                        textposition='inside',
                        hovertemplate='<b>%{fullData.name}</b><br>Action: %{x}<br>Count: %{y}<extra></extra>'
                    ))

                fig_stacked.update_layout(
                    barmode='stack',
                    xaxis_title="Action Type",
                    yaxis_title="Key Count",
                    height=300,
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=1.02,
                        xanchor="right",
                        x=1
                    )
                )

                st.plotly_chart(fig_stacked, use_container_width=True, config=PLOTLY_CONFIG)
            else:
                st.info("No key data available for visualization.")

        st.divider()

        # Show detailed results in organized format
        with st.expander("📋 Detailed Row-by-Row Results", expanded=True):
            # Truncate long rationale columns for display
            display_df = results_df.copy()
            rationale_cols = [c for c in display_df.columns if c.endswith("_rationale") or c == "judge_rationale"]
            for col in rationale_cols:
                if col in display_df.columns:
                    display_df[col] = display_df[col].astype(str).str.slice(0, 150) + "..."
            st.dataframe(display_df, use_container_width=True)

            # Download option
            csv = results_df.to_csv(index=False).encode('utf-8')
            st.download_button(
                "⬇️ Download Pruning Results",
                data=csv,
                file_name="test4_pruning_results.csv",
                mime="text/csv",
                use_container_width=True
            )

        # --- Save results at the end of the test run ---
        save_results_df(results_df, "Test 4", len(pruning_df), is_pruning_test=True)
        # --- PATCH 7: Structured Summary Call for Test 4 ---
        report_text = f"Test 4 Pruning Results (N={len(pruning_df)}): Action Accuracy: {action_accuracy:.2%}, Avg Key Similarity: {avg_key_score:.3f}. Focus analysis on context key pruning efficiency."
        loop = asyncio.get_event_loop()
        loop.run_until_complete(display_final_summary_for_test("Test 4 Quantitative Context Pruning", report_text, PRUNER_INSTRUCTIONS))
        # ----------------------------------------------------


# --- PATCH 8: Test 5: Agent Self-Refinement (Code Execution) (tabs[5]) ---

if "agent_df" not in st.session_state:
    st.session_state.agent_df = pd.DataFrame(columns=["query", "expected_sequence"])

AGENT_SYSTEM_PROMPT = """
You are a self-refining Agent Builder using the Gemini Code Execution tool. Your task is to design a Python class 'AgentFramework'
that successfully processes a list of user queries, identifies the necessary tool calls, and returns
the exact sequence of tools used.

The queries are provided in JSON format, and your goal is to write a script that defines the AgentFramework,
runs the queries using an 'evaluate' function (which you must also define), and prints the accuracy of the predicted tool sequences
against the expected sequences.

You MUST derive the final accuracy metric using the format 'Accuracy: X.XXXX' in the stdout of your executable code block
for the next turn to process your performance.

Your response in each turn must ONLY contain your reasoning followed by the full executable Python code block containing the refined AgentFramework and evaluation logic.
"""

with tabs[5]:
    # Dashboard header
    render_test_flow_diagram(5, "Test 5: Unified Orchestrator (Three Modes)")

    # Collapsible configuration section
    with st.expander("⚙️ Test Configuration", expanded=False):
        st.markdown("""
        **Architecture:** Unified orchestrator with three execution modes

        **Execution Modes:**
        1. **🎯 Direct Inference:** Pattern matching (classification, prediction)
           - Each turn: Generate → Evaluate → Analyze Failures → Refine
           - Best for: Prediction tasks, classification, tool sequence prediction
           - Example: "Predict tool sequences for user queries"

        2. **📊 Computational Analysis:** Statistics, simulations, optimization
           - Uses code execution for computational tasks
           - Best for: Data analysis, statistical computations, optimization
           - Example: "Analyze performance metrics and compute statistics"

        3. **🔍 Research Tasks:** Multi-source information gathering
           - Each turn: Decompose → Execute in Parallel → Synthesize
           - Best for: Open-ended research, multi-source information gathering
           - Example: "Research George Morgan, Symbolica AI, and their fundraising"

        **Features:**
        - ✅ Auto-mode detection based on goal
        - ✅ Parallel task execution (asyncio.gather)
        - ✅ Smart caching with deduplication
        - ✅ Convergence detection (mode-specific)

        **Budget Modes:**
        - **Fixed Turns:** Run exactly N iterations (predictable, simpler)
        - **Cost/Token Limit:** Run until budget exhausted or converged (efficient, dynamic)

        **Stopping Conditions:**
        - Turn mode: Max turns reached OR no improvement in last 3 turns
        - Cost mode: Budget exhausted OR marginal value < 1%

        **Execution Model:** Gemini 2.5 Flash with Code Execution
        **Test Data:** Tool/Agent Sequence dataset (for inference mode)
        **Evaluation Metric:** Exact sequence match accuracy (inference mode)
        **Turn Tracking:** Per-turn metrics with improvement trajectory visualization
        """)

    st.divider()

    # ============================================================
    # DEMO LAUNCHER UI COMPONENT
    # ============================================================
    st.markdown("### 🎬 Quick Demonstration Scenarios")
    st.caption("Pre-configured scenarios showcasing Leaf Agent Scaffold versatility across domains")

    col1, col2 = st.columns(2)

    with col1:
        if st.button("🤖 Demo 1: PI Agent (Laundry Folding)", use_container_width=True, help="Autonomous robotics with vision-to-motor control and adaptive learning"):
            st.session_state['demo_scenario'] = 'pi_agent'
            st.session_state['demo_goal'] = PI_AGENT_GOAL_PROMPT
            st.session_state['demo_agents'] = ["web_researcher", "code_executor", "validator", "content_generator"]
            st.session_state['demo_memory_policy'] = FOLDING_POLICY_BLOCK
            st.session_state['demo_mode'] = 'research'
            st.session_state['demo_coordination'] = 'leaf_scaffold'
            st.session_state['auto_run_demo'] = True  # Flag to auto-run
            st.success("✅ PI Agent demo starting...")
            st.rerun()

    with col2:
        if st.button("🔒 Demo 2: Cybersecurity (Phishing Analysis)", use_container_width=True, help="Threat detection with reasoning, risk scoring, and policy enforcement"):
            st.session_state['demo_scenario'] = 'cybersecurity'
            st.session_state['demo_goal'] = CYBERSECURITY_GOAL_PROMPT
            st.session_state['demo_agents'] = ["web_researcher", "validator", "code_executor", "content_generator"]
            st.session_state['demo_memory_policy'] = THREAT_POLICY_BLOCK
            st.session_state['demo_mode'] = 'research'
            st.session_state['demo_coordination'] = 'leaf_scaffold'
            st.session_state['auto_run_demo'] = True  # Flag to auto-run
            st.success("✅ Cybersecurity demo starting...")
            st.rerun()

    # Display active scenario indicator
    if 'demo_scenario' in st.session_state:
        scenario_name = "PI Agent (Laundry Folding)" if st.session_state['demo_scenario'] == 'pi_agent' else "Cybersecurity (Phishing Analysis)"
        st.info(f"📌 Active Demo: **{scenario_name}**")

        # Add clear demo button
        if st.button("🔄 Clear Demo Configuration", help="Reset to manual configuration"):
            for key in ['demo_scenario', 'demo_goal', 'demo_agents', 'demo_memory_policy', 'demo_mode', 'demo_coordination']:
                if key in st.session_state:
                    del st.session_state[key]
            st.success("Demo configuration cleared!")
            st.rerun()

    st.divider()

    # Mode selection
    st.markdown("### Task Type")
    default_mode = st.session_state.get('demo_mode', 'auto')
    mode_index = ["auto", "inference", "research", "analysis"].index(default_mode) if default_mode in ["auto", "inference", "research", "analysis"] else 0
    mode_option = st.radio(
        "Select Mode",
        options=["auto", "inference", "research", "analysis"],
        index=mode_index,
        format_func=lambda x: {
            "auto": "🤖 Auto-detect",
            "inference": "🎯 Direct Inference",
            "research": "🔍 Research Tasks",
            "analysis": "📊 Computational Analysis"
        }.get(x, x),
        horizontal=True,
        help="""
        - **Auto-detect:** Automatically choose mode based on goal
        - **Direct Inference:** Prediction/classification (e.g., tool sequences)
        - **Research Tasks:** Open-ended multi-source questions
        - **Analysis:** Computational statistics and optimization
        """
    )

    # Mode-specific info (hide for auto mode)
    if mode_option == "inference":
        st.info("💡 **Inference Mode**: Optimizes prompts through self-evaluation and failure analysis")
        st.caption("Best for: Classification, prediction, pattern matching")

    elif mode_option == "analysis":
        st.info("💡 **Analysis Mode**: Generates Python code for computational tasks")
        st.caption("Best for: Statistics, data analysis, optimization, simulations")

    elif mode_option == "research":
        st.info("💡 **Research Mode**: Hybrid strategy - decomposes into subtasks, uses code generation for computational parts")
        st.caption("Best for: Multi-source research with computational analysis requirements")

    # Auto mode: no info box (mode not yet determined)

    st.divider()

    # Coordination Pattern selection
    st.markdown("### Coordination Pattern")
    default_coordination = st.session_state.get('demo_coordination', 'auto')
    coordination_index = ["auto", "solo", "subagent", "multi_agent", "leaf_scaffold"].index(default_coordination) if default_coordination in ["auto", "solo", "subagent", "multi_agent", "leaf_scaffold"] else 0
    coordination_option = st.radio(
        "Select Coordination Pattern",
        options=["auto", "solo", "subagent", "multi_agent", "leaf_scaffold"],
        index=coordination_index,
        format_func=lambda x: {
            "auto": "🤖 Auto-detect",
            "solo": "👤 Solo Agent",
            "subagent": "🏗️ Subagent Orchestration",
            "multi_agent": "👥 Multi-Agent Collaboration",
            "leaf_scaffold": "🌳 Leaf Agent Scaffold"
        }.get(x, x),
        horizontal=True,
        help="""
        - **Auto-detect:** Automatically choose pattern based on task complexity
        - **Solo Agent:** Single agent executes independently (fastest)
        - **Subagent Orchestration:** Hierarchical delegation with specialized subagents
        - **Multi-Agent Collaboration:** Peer agents propose, review, and build consensus
        - **Leaf Agent Scaffold:** Hierarchical multi-agent with supervisor and specialized leaf agents
        """
    )

    # Pattern-specific info (hide simple info for multi-agent and auto)
    if coordination_option == "solo":
        st.info("👤 **Solo Agent**: Single agent executes the task independently")
        st.caption("Best for: Simple, straightforward tasks")

    elif coordination_option == "subagent":
        st.info("🏗️ **Subagent Orchestration**: Hierarchical delegation (Decomposer → Generator → Evaluator → Analyzer → Synthesizer)")
        st.caption("Best for: Complex tasks requiring specialized expertise at each stage")

        # Optional: Show subagent workflow diagram
        with st.expander("🏗️ Subagent Workflow Details", expanded=False):
            st.markdown("""
            **Hierarchical Pipeline**:
            1. **Decomposer** → Breaks goal into subtasks
            2. **Generator** → Creates solutions for each subtask
            3. **Evaluator** → Tests and scores solutions
            4. **Analyzer** → Identifies failures and suggests improvements
            5. **Synthesizer** → Combines results into final solution

            Each subagent specializes in one stage of the pipeline.
            """)

    elif coordination_option == "multi_agent":
        st.divider()
        st.markdown("## 👥 Multi-Agent Collaboration")
        st.info("**How it works**: Peer agents independently propose solutions, cross-review each other's work, and build consensus through collaborative synthesis")
        st.caption("✨ Best for: Tasks requiring diverse perspectives, validation, or creative problem-solving")

        # Multi-agent configuration
        st.markdown("### 🎭 Configure Your Agent Team")

        # Get default roles based on mode
        default_roles = {
            "inference": ["Pattern Analyst", "Rule Designer", "Evaluator"],
            "analysis": ["Data Scientist", "Algorithm Designer", "Code Reviewer"],
            "research": ["Market Analyst", "Technical Expert", "Domain Specialist"]
        }

        current_mode = mode_option if mode_option != "auto" else "inference"
        default_role_list = default_roles.get(current_mode, ["Agent 1", "Agent 2", "Agent 3"])

        peer_roles_input = st.text_input(
            "Peer Agent Roles (comma-separated)",
            value=", ".join(default_role_list),
            help="Define the roles for peer agents. Each role brings a different perspective."
        )

        peer_agent_roles = [role.strip() for role in peer_roles_input.split(",") if role.strip()]

        if len(peer_agent_roles) < 2:
            st.warning("⚠️ Multi-agent mode requires at least 2 peer agents")
        else:
            st.success(f"✅ {len(peer_agent_roles)} peer agents configured: {', '.join(peer_agent_roles)}")

        # Show workflow preview
        with st.expander("📋 Multi-Agent Workflow Preview", expanded=True):
            st.markdown(f"""
            **Round 1: Independent Proposals** 💡
            - Each of the {len(peer_agent_roles)} agents proposes their solution independently
            - Agents: {', '.join(f'**{role}**' for role in peer_agent_roles)}

            **Round 2: Cross-Review** 🔍
            - Each agent reviews proposals from other agents
            - Provides constructive feedback and identifies concerns

            **Round 3: Consensus Building** 🤝
            - Synthesizer combines all proposals and reviews
            - Creates unified solution incorporating best ideas

            **Round 4: Joint Evaluation** ✅
            - All agents jointly evaluate the consensus
            - Assess quality, agreement, and completeness

            **View Results**: Check the **Agent Dashboard** (Tab 6) to see the full interaction timeline!
            """)

        # Quick demo button
        st.markdown("#### 🎯 Quick Demo")
        demo_col1, demo_col2 = st.columns([3, 1])
        with demo_col1:
            st.caption("Try a pre-configured multi-agent collaboration to see how it works")
        with demo_col2:
            if st.button("🎬 Load Demo", type="secondary", use_container_width=True):
                # Set demo values
                st.session_state['test5_prompt'] = "Design a robust tool sequence predictor with 90%+ accuracy"
                st.rerun()

        # Show message if demo was just loaded
        if st.session_state.get('test5_prompt') == "Design a robust tool sequence predictor with 90%+ accuracy":
            st.success("✅ Demo prompt loaded! Scroll down and click the '👥 Run Multi-Agent Collaboration' button to execute.")

    elif coordination_option == "leaf_scaffold":
        st.divider()
        st.markdown("## 🌳 Leaf Agent Scaffold")
        st.info("**Hierarchical multi-agent system** with supervisor orchestration and specialized leaf agents")
        st.caption("✨ Best for: Complex tasks requiring multiple specialized capabilities (research, computation, writing, validation)")

        # Show architecture diagram
        with st.expander("🏗️ Architecture Overview", expanded=True):
            st.markdown("""
            **Hierarchical Structure**:

            ```
            🧠 Supervisor Agent
                ├── 📋 Task Planning (decomposes complex task)
                ├── 🎯 Delegation (routes to specialists)
                └── 🔄 Synthesis (combines results)

            👥 Specialized Leaf Agents
                ├── 🔍 Web Researcher (information gathering)
                ├── 💻 Code Executor (computation & analysis)
                ├── ✍️ Content Generator (writing & formatting)
                └── ✅ Validator (quality assurance)
            ```

            **How it works**:
            1. **Supervisor** analyzes your goal and breaks it into specialized sub-tasks
            2. **Leaf Agents** execute their assigned sub-tasks independently
            3. **Supervisor** synthesizes all results into final answer

            **Check Agent Dashboard (Tab 6)** to see the full execution hierarchy!
            """)

        # Agent selection
        st.markdown("### 🎭 Select Leaf Agents")

        available_agents = {
            "web_researcher": "🔍 Web Researcher (information gathering from web sources)",
            "code_executor": "💻 Code Executor (Python computation & data analysis)",
            "content_generator": "✍️ Content Generator (writing & formatting)",
            "validator": "✅ Validator (quality assurance & validation)"
        }

        # Use demo agents if available, otherwise default
        default_agents = st.session_state.get('demo_agents', ["web_researcher", "validator", "content_generator"])
        selected_agent_types = st.multiselect(
            "Choose specialized agents for this task",
            options=list(available_agents.keys()),
            default=default_agents,
            format_func=lambda x: available_agents[x],
            help="Select agents based on task requirements. The supervisor will automatically delegate sub-tasks to appropriate agents. For research tasks, include 'validator' to prevent hallucinations."
        )

        # Store in session state for orchestrator
        st.session_state['selected_agent_types'] = selected_agent_types

        if len(selected_agent_types) < 1:
            st.warning("⚠️ Select at least one leaf agent")
        else:
            st.success(f"✅ {len(selected_agent_types)} specialized agents configured")

            # Show selected agents
            st.markdown("**Your Agent Team**:")
            for agent_type in selected_agent_types:
                st.write(f"  • {available_agents[agent_type]}")

        # Quick demo button
        st.markdown("#### 🎯 Quick Demo")
        demo_col1, demo_col2 = st.columns([3, 1])
        with demo_col1:
            st.caption("Try a pre-configured leaf agent scaffold to see hierarchical orchestration in action")
        with demo_col2:
            if st.button("🎬 Load Demo", key="leaf_demo", type="secondary", use_container_width=True):
                # Set demo values
                st.session_state['test5_prompt'] = "Research and analyze the latest trends in multi-agent AI systems, then write a comprehensive report"
                st.rerun()

        # Show message if demo was just loaded
        if st.session_state.get('test5_prompt') == "Research and analyze the latest trends in multi-agent AI systems, then write a comprehensive report":
            st.success("✅ Demo prompt loaded! Scroll down and click the '🌳 Run Leaf Agent Scaffold' button to execute.")

        peer_agent_roles = None

    else:
        peer_agent_roles = None
        selected_agent_types = None

    # Dataset-aware suggested prompts
    TEST5_SUGGESTED_PROMPTS = {
        "inference": [
            "Design an agent that predicts tool sequences from user queries with 90%+ accuracy through iterative refinement.",
            "Build a pattern-matching agent using keyword extraction and rule-based mapping for tool sequence prediction.",
            "Create a rule-based tool sequence predictor with learning capability that analyzes failures and adds new rules."
        ],
        "research": [
            "Research [Company/Person Name] for due diligence: background, financial health, market position, recent news, and key stakeholders.",
            "Competitive analysis for [Industry/Market]: identify top players, compare features/pricing, analyze strengths/weaknesses.",
            "Investment thesis research: fundamentals, market opportunity, competitive moat, team execution, and risk assessment."
        ],
        "analysis": [
            "Analyze test dataset performance metrics: sequence length distribution, common tool combinations, coverage analysis.",
            "Optimize prediction accuracy through data analysis: baseline accuracy, hard vs easy examples, feature importance, error analysis.",
            "Compute statistical summary of dataset: complexity scores, optimal training split, correlation analysis."
        ]
    }

    # Initialize prompt in session state
    if 'test5_prompt' not in st.session_state:
        st.session_state['test5_prompt'] = ""

    # Prompt generator function
    def generate_test5_prompt():
        """Generate a dataset-aware prompt based on current mode AND user input."""
        mode = mode_option if mode_option != "auto" else "inference"

        # Get current user input
        current_input = st.session_state.get('test5_prompt', '')

        # Select base prompt
        import random
        prompts = TEST5_SUGGESTED_PROMPTS.get(mode, TEST5_SUGGESTED_PROMPTS["inference"])
        base_prompt = random.choice(prompts) if isinstance(prompts, list) else prompts

        # Build context from dataset
        context = {
            'dataset_size': len(st.session_state.agent_df) if not st.session_state.agent_df.empty else 0,
            'columns': list(st.session_state.agent_df.columns) if not st.session_state.agent_df.empty else [],
            'mode': mode
        }

        if mode == "inference" and not st.session_state.agent_df.empty:
            # Extract actual tools from dataset
            all_tools = set()
            for seq in st.session_state.agent_df['expected_sequence']:
                if isinstance(seq, list):
                    all_tools.update(seq)
                elif isinstance(seq, str):
                    try:
                        tools = json.loads(seq.replace("'", '"'))
                        all_tools.update(tools)
                    except:
                        all_tools.update([t.strip() for t in seq.split(',') if t.strip()])

            # Sample queries
            sample_queries = st.session_state.agent_df['query'].head(3).tolist()

            context['tools'] = sorted(all_tools)[:15]
            context['sample_queries'] = sample_queries
            context['success_criteria'] = '85%+ exact sequence match accuracy'

            # Add dataset-specific approach
            context['suggested_approach'] = f"""Pattern matching + entity extraction:
1. Keyword dictionaries for common patterns
2. Regex for structured data (IDs, IPs)
3. Sequence logic for multi-step operations
4. Confidence scoring for ambiguous cases"""

            # Create dataset-aware base prompt
            base_prompt = f"""Design a tool sequence predictor for IT operations queries.

DATASET: {len(st.session_state.agent_df)} examples
TOOLS: {', '.join(sorted(all_tools)[:12])}...

SAMPLE QUERIES:
{chr(10).join([f'- "{q}"' for q in sample_queries])}

APPROACH:
1. Pattern matching (keywords → tools)
2. Entity extraction (IDs, IPs, device names)
3. Sequence logic (auth before query, check before order)
4. Error handling (unknown patterns → default)

TARGET: 85%+ exact match accuracy"""

        # Enhance with user input
        enhanced = enhance_prompt_with_user_input(base_prompt, current_input, context)

        return enhanced

    # Different inputs based on mode
    if mode_option in ["auto", "research"]:
        use_test_data = False

        # Prompt management section (NO test data preview for research/auto)
        st.markdown("### 🎯 Research Goal")

        # Use demo goal if available, otherwise default
        if 'demo_goal' in st.session_state:
            st.session_state['test5_prompt'] = st.session_state['demo_goal']
            # Clear demo_goal after using it to prevent re-applying on every rerun
            del st.session_state['demo_goal']
        elif not st.session_state['test5_prompt']:
            st.session_state['test5_prompt'] = "Help me research George Morgan, Symbolica AI, their position on AI Engineering, and their next fundraising round"

        col1, col2 = st.columns([3, 1])
        with col1:
            pass  # Placeholder for layout
        with col2:
            if st.button("🎲 Generate Prompt", width='content'):
                st.session_state['test5_prompt'] = generate_test5_prompt()
                st.rerun()

        goal = st.text_area(
            "Research Goal / Task Description",
            value=st.session_state['test5_prompt'],
            height=200,  # Increased height for demo prompts
            help="Describe what you want to research or accomplish",
            key="test5_goal_input"
        )

        # Update session state
        st.session_state['test5_prompt'] = goal

        if mode_option == "research":
            st.info("💡 **Research Mode:** Decomposes goal into parallel subtasks for multi-source information gathering.")
    else:
        # For inference and analysis modes: SHOW test data preview
        if st.session_state.agent_df.empty:
            st.warning("⚠️ Please generate a Tool/Agent Sequence dataset in the 'Preparation' tab first.")
            use_test_data = False
            goal = "Predict tool sequences"
        else:
            st.subheader("📊 Agent Test Data Preview")
            st.dataframe(st.session_state.agent_df.head(5), use_container_width=True)
            st.caption(f"Total: {len(st.session_state.agent_df)} examples")

            use_test_data = True

            # Prompt management section
            st.markdown("### 🎯 Task Description")

            # Use demo goal if available, otherwise generate
            if 'demo_goal' in st.session_state:
                st.session_state['test5_prompt'] = st.session_state['demo_goal']
                # Clear demo_goal after using it
                del st.session_state['demo_goal']
            elif not st.session_state['test5_prompt']:
                st.session_state['test5_prompt'] = generate_test5_prompt()

            col1, col2 = st.columns([3, 1])
            with col1:
                pass  # Placeholder for layout
            with col2:
                if st.button("🎲 Generate Prompt", width='content'):
                    st.session_state['test5_prompt'] = generate_test5_prompt()
                    st.rerun()

            goal = st.text_area(
                "Agent Goal / Task Description",
                value=st.session_state['test5_prompt'],
                height=200,
                help="Describe the task in detail. Be specific about inputs, outputs, and success criteria.",
                key="test5_goal_input"
            )

            # Update session state
            st.session_state['test5_prompt'] = goal

            # Quick Start Templates (ONLY for inference/analysis, NOT research)
            st.markdown("##### 💡 Quick Start Templates")
            mode_key = mode_option if mode_option != "auto" else "inference"
            prompts = TEST5_SUGGESTED_PROMPTS.get(mode_key, TEST5_SUGGESTED_PROMPTS["inference"])

            cols = st.columns(len(prompts))
            for i, prompt in enumerate(prompts):
                with cols[i]:
                    label = f"Template {i+1}"
                    if st.button(label, key=f"test5_pill_{i}", help=prompt, use_container_width=True):
                        st.session_state['test5_prompt'] = prompt
                        st.rerun()

            # Mode-specific tips (only for inference/analysis in this section)
            if mode_option == "inference":
                st.info("💡 **Inference Mode:** Each turn generates code, evaluates it, and refines based on failures. Best for prediction tasks.")
            elif mode_option == "analysis":
                st.info("💡 **Analysis Mode:** Uses code execution for computational analysis and optimization tasks.")

    st.divider()

    # Budget mode selection
    budget_mode = st.radio(
        "Budget Mode",
        options=["turns", "cost"],
        format_func=lambda x: "Fixed Turns" if x == "turns" else "Cost/Token Limit",
        horizontal=True,
        help="Fixed Turns: Run exactly N iterations. Cost/Token: Run until budget exhausted or converged."
    )

    col1, col2 = st.columns(2)

    if budget_mode == "turns":
        with col1:
            max_turns = st.number_input("Max Turns", min_value=1, max_value=20, value=3)
        with col2:
            st.info("Runs for exactly N turns (tracking cost for reporting)")
    else:  # cost mode
        with col1:
            max_cost = st.number_input("Budget (USD)", min_value=0.5, max_value=20.0, value=2.0, step=0.5)
        with col2:
            max_tokens = st.number_input("Token Limit", min_value=100_000, max_value=5_000_000, value=500_000, step=100_000)

    # Run button with pattern-specific messaging
    st.divider()

    if coordination_option == "multi_agent":
        st.markdown("### 🎬 Run Multi-Agent Collaboration")
        st.info("💡 **Tip**: After running, switch to **Tab 6 (Agent Dashboard)** to see the full agent interaction timeline!")
        button_label = "👥 Run Multi-Agent Collaboration"
    elif coordination_option == "subagent":
        st.markdown("### 🎬 Run Subagent Orchestration")
        button_label = "🏗️ Run Subagent Orchestration"
    elif coordination_option == "leaf_scaffold":
        st.markdown("### 🎬 Run Leaf Agent Scaffold")
        st.info("💡 **Tip**: After running, switch to **Tab 6 (Agent Dashboard)** to see the hierarchical execution flow!")
        button_label = "🌳 Run Leaf Agent Scaffold"
    else:
        st.markdown("### 🎬 Run Orchestrator")
        button_label = "🚀 Run Unified Orchestrator"

    # Check if demo should auto-run
    auto_run = st.session_state.get('auto_run_demo', False)
    if auto_run:
        # Clear the flag to prevent infinite loop
        st.session_state['auto_run_demo'] = False
        # Show demo is running
        demo_name = "PI Agent (Laundry Folding)" if st.session_state.get('demo_scenario') == 'pi_agent' else "Cybersecurity (Phishing Analysis)"
        st.info(f"🎬 **Auto-running demo:** {demo_name}")

    if st.button(button_label, type="primary", use_container_width=True) or auto_run:
        if not GEMINI_API_KEY:
            st.error("GEMINI_API_KEY required")
        else:
            # Reset tracker for fresh run
            tracker = st.session_state.get('execution_tracker')
            if tracker:
                tracker.reset()

            # Create budget based on mode
            if budget_mode == "turns":
                budget = Budget(mode="turns", max_turns=max_turns)
            else:
                budget = Budget(mode="cost", max_cost_usd=max_cost, max_tokens=max_tokens)

            # Prepare test data if needed
            test_data = None
            if use_test_data and not st.session_state.agent_df.empty:
                # CRITICAL: Parse expected_sequence from string to list with robust error handling
                test_data = st.session_state.agent_df.to_dict('records')

                parse_errors = []
                for idx, item in enumerate(test_data):
                    seq = item.get('expected_sequence', '')
                    original_seq = seq  # Keep for error reporting

                    try:
                        if isinstance(seq, str):
                            # Handle various formats: "tool1,tool2" or "['tool1','tool2']" or "tool1|tool2"
                            if seq.startswith('['):
                                # JSON-like format
                                try:
                                    parsed = json.loads(seq.replace("'", '"'))
                                    # Validate it's actually a list
                                    if isinstance(parsed, list):
                                        item['expected_sequence'] = parsed
                                    else:
                                        # Single value wrapped in brackets
                                        item['expected_sequence'] = [parsed]
                                except json.JSONDecodeError as e:
                                    # Fallback to split
                                    item['expected_sequence'] = [s.strip() for s in re.split(r'[,|;]', seq.strip('[]')) if s.strip()]
                                    parse_errors.append(f"Row {idx}: JSON parse failed, used split fallback: {str(e)[:50]}")
                            else:
                                # Comma or pipe separated
                                item['expected_sequence'] = [s.strip() for s in re.split(r'[,|;]', seq) if s.strip()]
                        elif isinstance(seq, list):
                            # Already a list - validate contents
                            item['expected_sequence'] = [str(x).strip() for x in seq]
                        else:
                            # Unexpected type
                            item['expected_sequence'] = []
                            parse_errors.append(f"Row {idx}: Unexpected type {type(seq).__name__}, defaulted to empty list")
                    except Exception as e:
                        # Catch-all for any parsing errors
                        item['expected_sequence'] = []
                        parse_errors.append(f"Row {idx}: Parse error '{str(e)[:50]}', defaulted to empty list")

                # Show parsing warnings if any
                if parse_errors:
                    st.warning(f"⚠️ {len(parse_errors)} data parsing issue(s):")
                    for err in parse_errors[:5]:  # Show first 5
                        st.text(f"  • {err}")
                    if len(parse_errors) > 5:
                        st.text(f"  ... and {len(parse_errors) - 5} more")

                st.write(f"📋 Loaded {len(test_data)} test cases")
                if test_data:
                    st.write(f"📝 Sample: {test_data[0]}")  # Verify format

            # Create orchestrator with auto-detection or explicit mode and coordination pattern
            orchestrator_kwargs = {
                "goal": goal,
                "test_data": test_data,
                "budget": budget
            }

            # Add mode if not auto
            if mode_option != "auto":
                orchestrator_kwargs["mode"] = mode_option

            # Add coordination pattern if not auto
            if coordination_option != "auto":
                orchestrator_kwargs["coordination_pattern"] = coordination_option

            # Add peer agent roles for multi-agent mode
            if coordination_option == "multi_agent" and peer_agent_roles:
                orchestrator_kwargs["peer_agent_roles"] = peer_agent_roles

            orchestrator = UnifiedOrchestrator(**orchestrator_kwargs)

            try:
                results = asyncio.run(orchestrator.run())

                # Display coordination pattern used
                st.info(f"🎯 Mode: **{orchestrator.mode.upper()}** | 🤝 Pattern: **{orchestrator.coordination_pattern.upper()}**")

                # Handle different result types based on coordination pattern and mode
                if orchestrator.coordination_pattern == "multi_agent":
                    # Multi-agent returns a dict with consensus
                    st.success("✅ Multi-agent collaboration completed!")

                    # Prominent dashboard link
                    st.info("📊 **View the full agent interaction timeline in Tab 6 (Agent Dashboard)!**")

                    # Show summary metrics
                    col1, col2, col3 = st.columns(3)

                    if isinstance(results, dict):
                        with col1:
                            st.metric("Final Score", f"{results.get('final_score', 0.0):.3f}")
                        with col2:
                            st.metric("Peer Agents", len(results.get('peer_roles', [])))
                        with col3:
                            st.metric("Consensus Rounds", len(results.get('consensus_history', [])))

                        # Show final consensus in expandable section
                        with st.expander("📋 View Final Consensus", expanded=False):
                            st.json(results.get('final_consensus', {}))

                        # Show consensus history
                        if results.get('consensus_history'):
                            with st.expander("📚 View Consensus History", expanded=False):
                                for idx, consensus_round in enumerate(results['consensus_history'], 1):
                                    st.markdown(f"### Turn {idx}")

                                    # Show proposals
                                    st.markdown("**Proposals:**")
                                    for proposal in consensus_round.get('proposals', []):
                                        role = proposal.get('role', 'Unknown')
                                        prop = proposal.get('proposal', {})
                                        st.markdown(f"- **{role}**: {prop.get('approach', 'N/A')}")

                                    # Show evaluation
                                    eval_data = consensus_round.get('evaluation', {})
                                    st.markdown(f"**Score**: {eval_data.get('score', 0.0):.3f} | **Agreement**: {eval_data.get('agreement', 0.0):.3f}")
                                    st.divider()

                elif orchestrator.coordination_pattern == "leaf_scaffold":
                    # Leaf scaffold returns a dict with hierarchical results
                    st.success("✅ Leaf agent scaffold execution completed!")

                    # Prominent dashboard link
                    st.info("📊 **View the hierarchical execution flow in Tab 6 (Agent Dashboard)!**")

                    # Show summary metrics
                    col1, col2, col3 = st.columns(3)

                    if isinstance(results, dict):
                        with col1:
                            st.metric("Leaf Agents", len(results.get('leaf_agents', [])))
                        with col2:
                            st.metric("Sub-Tasks", results.get('sub_tasks', 0))
                        with col3:
                            successful = results.get('metadata', {}).get('successful_tasks', 0)
                            st.metric("Successful Tasks", successful)

                        # Show final result
                        if results.get('final_result'):
                            st.markdown("### 📊 Final Synthesized Result")
                            st.markdown(results['final_result'])

                        # Show metadata
                        if results.get('metadata'):
                            with st.expander("📈 Execution Metadata", expanded=False):
                                st.json(results['metadata'])

                        # Show contributing agents
                        if results.get('leaf_agents'):
                            with st.expander("👥 Contributing Agents", expanded=False):
                                for agent_name in results['leaf_agents']:
                                    st.write(f"  • {agent_name}")

                elif orchestrator.coordination_pattern == "subagent":
                    # Subagent returns a dict with synthesized results
                    st.success("Subagent orchestration completed!")

                    if isinstance(results, dict):
                        st.metric("Final Score", f"{results.get('score', 0.0):.3f}")
                        st.metric("Best Turn", results.get('best_turn', 0))
                        st.metric("Total Turns", results.get('total_turns', 0))

                        if results.get('solution'):
                            st.subheader("Final Solution")
                            st.json(results['solution'])

                elif orchestrator.mode == "research":
                    # Research mode returns a dict with findings
                    st.success("Research completed!")
                    st.json(results)

                elif orchestrator.mode in ["inference", "analysis"]:
                    # Inference/analysis modes in solo pattern return (code, perf, history)
                    if isinstance(results, dict):
                        # New format from prompt optimization
                        best_code = results.get('best_prompt', '')
                        best_perf = results.get('best_accuracy', 0.0)
                        history = results.get('history', [])
                    else:
                        # Legacy tuple format
                        best_code, best_perf, history = results

                    st.success(f"Final Performance: {best_perf:.4f} (Turn {orchestrator.best_turn})")

                    if best_code:
                        st.subheader("Best Code Generated")
                        st.code(best_code, language='python')

                    # Turn-by-turn breakdown
                    st.subheader("Turn-by-Turn Progress")

                    turns_df = pd.DataFrame([
                        {
                            "Turn": m.turn,
                            "Tasks Attempted": m.tasks_attempted,
                            "Tasks Verified": m.tasks_verified,
                            "Best Accuracy": f"{m.best_accuracy:.4f}",
                            "Improvement": f"{m.improvement:+.4f}",
                            "Cost": f"${m.cost_spent:.3f}",
                            "Tokens": f"{m.tokens_used:,}"
                        }
                        for m in history
                    ])

                    st.dataframe(turns_df, width='content')

                    # Plot improvement trajectory
                    if history:
                        fig = go.Figure()

                        fig.add_trace(go.Scatter(
                            x=[m.turn for m in history],
                            y=[m.best_accuracy for m in history],
                            mode='lines+markers',
                            name='Accuracy',
                            line=dict(color='#1f77b4', width=3),
                            marker=dict(size=8)
                        ))

                        fig.update_layout(
                            title="Performance Improvement Trajectory",
                            xaxis_title="Turn",
                            yaxis_title="Accuracy",
                            height=400
                        )

                        st.plotly_chart(fig, width='content', config=PLOTLY_CONFIG)

                    # NEW: Agent Memory State Expander
                    if orchestrator.memory_manager:
                        with st.expander("🧠 Agent Memory State", expanded=False):
                            st.markdown("### Core Memory Blocks")

                            for block_name, block in orchestrator.memory_manager.core_blocks.items():
                                st.markdown(f"#### {block_name}")
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("Version", block.version)
                                with col2:
                                    st.metric("Last Modified Turn", block.last_modified_turn)
                                with col3:
                                    st.metric("Modifications", block.modification_count)

                                st.json({"content": block.content})
                                st.divider()

                            # Archival Memory Summary
                            st.markdown("### Archival Memory")
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Total Entries", len(orchestrator.memory_manager.archival_entries))
                            with col2:
                                st.metric("Total Retrievals", orchestrator.memory_manager.stats["total_retrievals"])
                            with col3:
                                avg_latency = orchestrator.memory_manager.stats["avg_retrieval_latency_ms"]
                                st.metric("Avg Retrieval Latency", f"{avg_latency:.2f}ms")

                            # Show recent archival entries
                            if orchestrator.memory_manager.archival_entries:
                                st.markdown("#### Recent Entries")
                                recent_entries = orchestrator.memory_manager.archival_entries[-5:]
                                for entry in reversed(recent_entries):
                                    st.text(f"[{entry.source_agent}] {entry.content[:100]}...")
                                    st.caption(f"Tags: {', '.join(entry.tags)} | {entry.timestamp}")

                    # NEW: Rethink Events on Improvement Trajectory
                    if orchestrator.self_correction_manager and orchestrator.self_correction_manager.rethink_history:
                        st.markdown("### 🔄 Self-Correction Events")

                        # Show rethink events as markers on the chart
                        rethink_turns = [r["turn"] for r in orchestrator.self_correction_manager.rethink_history]

                        if rethink_turns:
                            st.info(f"🔄 {len(rethink_turns)} rethink event(s) occurred at turn(s): {', '.join(map(str, rethink_turns))}")

                            # Table of rethink events
                            rethink_df = pd.DataFrame([
                                {
                                    "Turn": r["turn"],
                                    "Block Modified": r["block_name"],
                                    "Trigger": r["trigger"][:50] + "..." if len(r["trigger"]) > 50 else r["trigger"],
                                    "Timestamp": r["timestamp"]
                                }
                                for r in orchestrator.self_correction_manager.rethink_history
                            ])

                            st.dataframe(rethink_df, use_container_width=True)

                            # Analyze effectiveness
                            if history:
                                performance_scores = [m.best_accuracy for m in history]
                                effectiveness = orchestrator.self_correction_manager.analyze_correction_effectiveness(performance_scores)

                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("Total Rethinks", effectiveness["total_rethinks"])
                                with col2:
                                    st.metric("Successful Rethinks", effectiveness["successful_rethinks"])
                                with col3:
                                    success_rate = effectiveness["success_rate"] * 100
                                    st.metric("Success Rate", f"{success_rate:.1f}%")

                    # NEW: Turn-by-Turn Metrics with Memory & Security
                    if orchestrator.dashboard_logger:
                        st.markdown("### 📊 Detailed Turn Metrics")

                        # Load execution log
                        try:
                            run_data = DashboardLogger.load_run(orchestrator.dashboard_logger.run_id)
                            execution_log = run_data.get("execution_log", [])
                            security_audits = run_data.get("security_audits", [])

                            # Group by turn
                            turn_metrics = {}
                            for entry in execution_log:
                                turn = entry.get("turn", 0)
                                if turn not in turn_metrics:
                                    turn_metrics[turn] = {
                                        "memory_ops": 0,
                                        "security_checks": 0,
                                        "events": []
                                    }

                                if entry.get("event_type") == "MEMORY_WRITE":
                                    turn_metrics[turn]["memory_ops"] += 1

                                turn_metrics[turn]["events"].append(entry)

                            # Add security audits
                            for audit in security_audits:
                                turn = audit.get("turn", 0)
                                if turn in turn_metrics:
                                    turn_metrics[turn]["security_checks"] += 1

                            # Create enhanced metrics table
                            enhanced_metrics = []
                            for m in history:
                                turn_data = turn_metrics.get(m.turn, {})
                                enhanced_metrics.append({
                                    "Turn": m.turn,
                                    "Accuracy": f"{m.best_accuracy:.4f}",
                                    "Memory Ops": turn_data.get("memory_ops", 0),
                                    "Security Checks": turn_data.get("security_checks", 0),
                                    "Duration": f"{m.cost_spent:.3f}s",
                                    "Improvement": f"{m.improvement:+.4f}"
                                })

                            st.dataframe(pd.DataFrame(enhanced_metrics), use_container_width=True)

                        except Exception as e:
                            st.warning(f"Could not load detailed metrics: {e}")

                    # Summary
                    st.subheader("Execution Summary")
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("Total Turns", len(history))
                    with col2:
                        st.metric("Verified Tasks", len(orchestrator.cache.verified_tasks))
                    with col3:
                        st.metric("Total Cost", f"${budget.spent_cost:.2f}")
                    with col4:
                        st.metric("Best Turn", orchestrator.best_turn)

                    # Knowledge Index Summary
                    if orchestrator.index.entries:
                        st.subheader("Knowledge Index")
                        verified_count = sum(1 for e in orchestrator.index.entries if e["verdict"] == "verified")
                        partial_count = sum(1 for e in orchestrator.index.entries if e["verdict"] == "partial")
                        failed_count = sum(1 for e in orchestrator.index.entries if e["verdict"] == "failed")

                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Verified", verified_count, delta=None)
                        with col2:
                            st.metric("Partial", partial_count, delta=None)
                        with col3:
                            st.metric("Failed", failed_count, delta=None)

                    # LLM Analysis
                    mode_desc = f"Turn-based ({len(history)} turns)" if budget_mode == "turns" else f"Cost-based (${budget.spent_cost:.2f})"
                    report = f"""Orchestrator Test Report:
Task Type: {orchestrator.mode.upper()}
Mode: {mode_desc}
Final Best Accuracy: {best_perf:.4f} (achieved at turn {orchestrator.best_turn})
Total Turns: {len(history)}
Verified Tasks: {len(orchestrator.cache.verified_tasks)}
Total Cost: ${budget.spent_cost:.2f}
Total Tokens: {budget.spent_tokens:,}

Turn-by-Turn Summary:
{chr(10).join([f"Turn {m.turn}: {m.tasks_verified}/{m.tasks_attempted} verified, accuracy={m.best_accuracy:.4f}, improvement={m.improvement:+.4f}" for m in history])}
"""

                    summary_result = asyncio.run(get_structured_summary_and_refinement(report, best_code))

                    st.subheader("🎯 LLM Analysis & Suggested Improvements")
                    st.markdown(f"**Summary:** {summary_result.findings_summary}")
                    st.markdown("**Key Suggestions:**")
                    st.json(summary_result.key_suggestions)

                    if summary_result.suggested_improvement_code:
                        st.subheader("✨ Refined Code/Prompt for Next Iteration")
                        st.code(summary_result.suggested_improvement_code, language='python')
                    if summary_result.suggested_improvement_prompt_reasoning:
                        st.subheader("💡 Reasoning for Refinement")
                        st.markdown(summary_result.suggested_improvement_prompt_reasoning)

            except Exception as e:
                st.error(f"Orchestrator failed: {e}")
                st.exception(e)


# ---------- Tab 6: Agent Dashboard ----------
with tabs[6]:
    st.header("🎯 Agent Execution Dashboard")
    st.caption("Real-time monitoring and visualization of all test executions")

    # Enhanced dashboard with historical data loading and mock data
    def load_historical_data():
        """Load historical execution data from dashboard logs."""
        try:
            from utils.dashboard_logger import DashboardLogger

            # Load index
            index_data = DashboardLogger.load_index()
            historical_runs = []

            for run_info in index_data.get('runs', []):
                try:
                    run_data = DashboardLogger.load_run(run_info['run_id'])
                    historical_runs.append({
                        'run_info': run_info,
                        'data': run_data
                    })
                except Exception as e:
                    st.warning(f"Could not load run {run_info['run_id']}: {e}")

            return historical_runs
        except Exception as e:
            st.warning(f"Could not load historical data: {e}")
            return []

    def generate_mock_data():
        """Generate mock execution data for demonstration with RELATIVE timestamps."""
        mock_events = []
        base_time = 0  # Start at 0 for relative timing

        # Mock Test 1: Classification Test
        for i in range(3):
            mock_events.append({
                'test_name': 'Classification Test',
                'agent_id': f'classifier_{i+1}',
                'event_type': 'start',
                'timestamp': base_time + i * 60,  # Start at 0s, 60s, 120s
                'data': {
                    'model': ['gpt-4o-mini', 'claude-3-5-sonnet', 'gemini-2.5-flash'][i],
                    'batch_size': 50,
                    'task': f'Classification Batch {i+1}'
                }
            })
            mock_events.append({
                'test_name': 'Classification Test',
                'agent_id': f'classifier_{i+1}',
                'event_type': 'complete',
                'timestamp': base_time + i * 60 + 45,  # Duration: 45s each
                'data': {
                    'accuracy': [0.87, 0.91, 0.89][i],
                    'f1_score': [0.85, 0.90, 0.88][i],
                    'cost_usd': [0.12, 0.18, 0.08][i],
                    'tokens': [2400, 3200, 1800][i]
                }
            })

        # Mock Test 5: Orchestrator
        orchestrator_start = 200  # Start at 200s
        mock_events.append({
            'test_name': 'Test 5',
            'agent_id': 'orchestrator',
            'event_type': 'start',
            'timestamp': orchestrator_start,
            'data': {
                'task': 'Research: TechCorp Fundraising',
                'mode': 'leaf_scaffold',
                'target_person': 'John Smith, CEO'
            }
        })

        # Sub-agents for orchestrator (parallel execution)
        sub_agents = [
            ('web_researcher', 'LinkedIn Profile Analysis', 25, 0.92),
            ('knowledge_retriever', 'Company Background Research', 35, 0.88),
            ('content_generator', 'Executive Summary Generation', 20, 0.95),
            ('validator', 'Fact Verification', 15, 0.85)
        ]

        for i, (agent_type, task, duration, score) in enumerate(sub_agents):
            start_time = orchestrator_start + 5 + i * 10  # Staggered start
            mock_events.extend([
                {
                    'test_name': 'Test 5',
                    'agent_id': f'{agent_type}_{i+1}',
                    'event_type': 'start',
                    'timestamp': start_time,
                    'data': {
                        'agent_type': agent_type,
                        'task': task,
                        'parent': 'orchestrator'
                    }
                },
                {
                    'test_name': 'Test 5',
                    'agent_id': f'{agent_type}_{i+1}',
                    'event_type': 'complete',
                    'timestamp': start_time + duration,
                    'data': {
                        'score': score,
                        'output_size': f'{1.2 + i * 0.3:.1f} KB',
                        'confidence': score * 0.9
                    }
                }
            ])

        mock_events.append({
            'test_name': 'Test 5',
            'agent_id': 'orchestrator',
            'event_type': 'complete',
            'timestamp': orchestrator_start + 80,  # Total duration: 80s
            'data': {
                'final_score': 0.89,
                'total_cost': 0.45,
                'research_quality': 'High',
                'hallucination_risk': 0.12
            }
        })

        return mock_events

    # Get data sources
    tracker = st.session_state.get('execution_tracker')
    historical_data = load_historical_data()

    # Data source selector
    col1, col2, col3 = st.columns([2, 1, 1])

    with col1:
        data_source = st.selectbox(
            "📊 Data Source",
            options=['current_session', 'historical', 'mock_demo'],
            format_func=lambda x: {
                'current_session': '🔄 Current Session Data',
                'historical': '📚 Historical Runs',
                'mock_demo': '🎭 Demo Data (Mock)'
            }[x],
            index=2 if not tracker or not tracker.events else 0
        )

    with col2:
        if st.button("🔄 Refresh", width='content'):
            st.rerun()

    with col3:
        auto_refresh = st.checkbox("Auto-refresh", value=False)

    # Load appropriate data based on selection
    if data_source == 'current_session' and tracker and tracker.events:
        events_data = tracker.events
        st.success(f"✅ Loaded {len(events_data)} events from current session")
    elif data_source == 'historical' and historical_data:
        # Combine all historical events
        events_data = []
        for run in historical_data:
            for event in run['data'].get('execution_log', []):
                events_data.append(event)
        st.success(f"✅ Loaded {len(events_data)} events from {len(historical_data)} historical runs")
    elif data_source == 'mock_demo':
        events_data = generate_mock_data()
        st.info(f"🎭 Generated {len(events_data)} mock events for demonstration")
    else:
        events_data = []
        if data_source == 'current_session':
            st.info("No execution data in current session. Run any test to see live data.")
        elif data_source == 'historical':
            st.info("No historical data found. Historical runs will appear here after tests complete.")
        else:
            st.info("Select a data source to view the dashboard.")

    if events_data:
        # Enhanced Gantt chart renderer for mock/historical/current data
        def render_enhanced_gantt_from_events(events, title="Agent Execution Timeline"):
            """Render enhanced Gantt chart from event data with vibrant, high-contrast colors."""
            # Vibrant, high-contrast color scheme for maximum visual distinction
            AGENT_COLORS_VIBRANT = {
                'orchestrator': '#6366F1',      # Vibrant Indigo
                'classifier': '#10B981',        # Emerald Green
                'web_researcher': '#0EA5E9',    # Sky Blue
                'code_executor': '#F59E0B',     # Amber Orange
                'knowledge_retriever': '#8B5CF6', # Purple
                'content_generator': '#EC4899',  # Hot Pink
                'validator': '#14B8A6',         # Teal
                'editor': '#EF4444',            # Red
                'main_agent': '#22C55E',        # Bright Green
                'sub_agent': '#64748B'          # Slate Gray
            }

            # Build agent timeline
            agents = {}
            min_time = float('inf')

            for event in events:
                agent_id = event.get('agent_id', 'unknown')
                timestamp = event.get('timestamp', 0)

                # Track minimum timestamp for normalization
                min_time = min(min_time, timestamp)

                if agent_id not in agents:
                    agents[agent_id] = {
                        'id': agent_id,
                        'name': event.get('data', {}).get('task', agent_id),
                        'type': event.get('data', {}).get('agent_type', 'task'),
                        'start': timestamp,
                        'end': None,
                        'status': 'running',
                        'data': event.get('data', {})
                    }

                if event.get('event_type') == 'complete':
                    agents[agent_id]['end'] = timestamp
                    agents[agent_id]['status'] = 'complete'
                    agents[agent_id]['data'].update(event.get('data', {}))

            if not agents:
                st.info("No agent data to visualize.")
                return

            # Normalize timestamps to start at 0
            if min_time == float('inf'):
                min_time = 0

            # Create Gantt data with normalized timestamps
            gantt_data = []
            for agent in sorted(agents.values(), key=lambda x: x['start']):
                normalized_start = agent['start'] - min_time
                end_time = agent.get('end') or (agent['start'] + 30)
                normalized_end = end_time - min_time
                duration = normalized_end - normalized_start

                # Determine color (vibrant, high-contrast scheme)
                agent_type = agent.get('type', 'task')
                color = AGENT_COLORS_VIBRANT.get(agent_type, '#94A3B8')

                if agent['status'] == 'error':
                    color = '#DC2626'  # Bright Red for errors
                elif agent['status'] == 'running':
                    color = color + 'CC'  # Slightly transparent for running

                gantt_data.append({
                    'Task': agent['name'],
                    'Start': normalized_start,
                    'Finish': normalized_end,
                    'Duration': duration,
                    'Type': agent_type,
                    'Status': agent['status'],
                    'Color': color
                })

            # Create Plotly figure with enhanced styling
            fig = go.Figure()

            for row in gantt_data:
                fig.add_trace(go.Bar(
                    y=[row['Task']],
                    x=[row['Duration']],
                    base=row['Start'],
                    orientation='h',
                    name=row['Task'],
                    marker=dict(
                        color=row['Color'],
                        line=dict(color='rgba(255,255,255,0.3)', width=2),  # White border for contrast
                        pattern=dict(
                            shape="" if row['Status'] == 'complete' else "/" if row['Status'] == 'running' else "x"
                        )
                    ),
                    text=f"{row['Duration']:.1f}s",
                    textposition='inside',
                    textfont=dict(color='white', size=12, family='Arial Black', weight='bold'),
                    hovertemplate=(
                        f"<b style='font-size:14px'>{row['Task']}</b><br>" +
                        f"<b>Type:</b> {row['Type']}<br>" +
                        f"<b>Start:</b> {row['Start']:.1f}s<br>" +
                        f"<b>Duration:</b> {row['Duration']:.1f}s<br>" +
                        f"<b>Status:</b> {row['Status']}<br>" +
                        "<extra></extra>"
                    ),
                    showlegend=False
                ))

            fig.update_layout(
                title=dict(
                    text=title,
                    font=dict(size=20, weight='bold', color='#1F2937')
                ),
                xaxis=dict(
                    title="Time (seconds)",
                    titlefont=dict(size=14, weight='bold'),
                    gridcolor='#D1D5DB',
                    showgrid=True,
                    zeroline=True,
                    zerolinecolor='#6B7280',
                    zerolinewidth=2
                ),
                yaxis=dict(
                    title="Agent / Task",
                    titlefont=dict(size=14, weight='bold'),
                    gridcolor='#E5E7EB',
                    tickfont=dict(size=11, weight='bold')
                ),
                height=max(450, len(gantt_data) * 50),
                showlegend=False,
                hovermode='closest',
                plot_bgcolor='#FFFFFF',
                paper_bgcolor='#F9FAFB',
                margin=dict(l=220, r=60, t=90, b=70),
                font=dict(family='Arial, sans-serif')
            )

            st.plotly_chart(fig, use_container_width=True, config=PLOTLY_CONFIG)

            # Color Legend
            st.markdown("### 🎨 Agent Type Color Legend")
            legend_cols = st.columns(5)

            legend_items = [
                ("Orchestrator", AGENT_COLORS_VIBRANT['orchestrator']),
                ("Web Researcher", AGENT_COLORS_VIBRANT['web_researcher']),
                ("Code Executor", AGENT_COLORS_VIBRANT['code_executor']),
                ("Knowledge Retriever", AGENT_COLORS_VIBRANT['knowledge_retriever']),
                ("Content Generator", AGENT_COLORS_VIBRANT['content_generator']),
                ("Validator", AGENT_COLORS_VIBRANT['validator']),
                ("Classifier", AGENT_COLORS_VIBRANT['classifier']),
                ("Main Agent", AGENT_COLORS_VIBRANT['main_agent']),
                ("Editor", AGENT_COLORS_VIBRANT['editor']),
                ("Sub Agent", AGENT_COLORS_VIBRANT['sub_agent'])
            ]

            for idx, (name, color) in enumerate(legend_items[:5]):
                with legend_cols[idx]:
                    st.markdown(f"""
                    <div style="display: flex; align-items: center; margin-bottom: 8px;">
                        <div style="width: 20px; height: 20px; background-color: {color};
                                    border: 2px solid white; border-radius: 4px; margin-right: 8px;
                                    box-shadow: 0 2px 4px rgba(0,0,0,0.2);"></div>
                        <span style="font-size: 11px; font-weight: 600;">{name}</span>
                    </div>
                    """, unsafe_allow_html=True)

            if len(legend_items) > 5:
                legend_cols2 = st.columns(5)
                for idx, (name, color) in enumerate(legend_items[5:]):
                    with legend_cols2[idx]:
                        st.markdown(f"""
                        <div style="display: flex; align-items: center; margin-bottom: 8px;">
                            <div style="width: 20px; height: 20px; background-color: {color};
                                        border: 2px solid white; border-radius: 4px; margin-right: 8px;
                                        box-shadow: 0 2px 4px rgba(0,0,0,0.2);"></div>
                            <span style="font-size: 11px; font-weight: 600;">{name}</span>
                        </div>
                        """, unsafe_allow_html=True)

            st.divider()

            # Summary metrics
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Total Agents", len(agents))
            with col2:
                completed = len([a for a in agents.values() if a['status'] == 'complete'])
                st.metric("Completed", completed)
            with col3:
                total_time = max([a.get('end', a['start']) - min_time for a in agents.values()]) if agents else 0
                st.metric("Total Time", f"{total_time:.1f}s")
            with col4:
                avg_duration = sum([(a.get('end', a['start']) - a['start']) for a in agents.values()]) / len(agents) if agents else 0
                st.metric("Avg Duration", f"{avg_duration:.1f}s")

        # Render the enhanced Gantt chart
        st.subheader("📊 Execution Timeline")
        render_enhanced_gantt_from_events(events_data, title=f"{data_source.replace('_', ' ').title()} - Agent Execution Timeline")

        st.divider()

        # Test selector with available tests
        test_name_map = {
            "classification": "Classification Test",
            "pruning": "Pruning Test",
            "test5": "Test 5",
            "smoke_test": "Smoke Test Run"
        }

        col1, col2, col3 = st.columns([2, 1, 1])

        with col1:
            # Build options dynamically based on available data
            if data_source == 'mock_demo':
                test_options = ["classification", "test5"]
            else:
                # Extract from events
                available_tests = set()
                for event in events_data:
                    test_name = event.get('test_name', '')
                    if 'Classification' in test_name:
                        available_tests.add('classification')
                    elif 'Test 5' in test_name or 'Orchestrator' in test_name:
                        available_tests.add('test5')
                    elif 'Pruning' in test_name:
                        available_tests.add('pruning')

                test_options = sorted(list(available_tests)) if available_tests else ["test5"]

            # Use old selector for backward compatibility
            test_selector = st.selectbox(
                "Select Test to Visualize",
                options=test_options,
                format_func=lambda x: {
                    "classification": "Classification Tests (1-3)",
                    "pruning": "Context Pruning (Test 4)",
                    "test5": "Agent Self-Refinement (Test 5)",
                    "smoke_test": "🔍 Smoke Test Run"
                }.get(x, x),
                index=test_options.index(st.session_state.get('selected_test_in_dashboard_selector', test_options[0])) if st.session_state.get('selected_test_in_dashboard_selector') in test_options else 0,
                key='dashboard_test_selector'
            )

            # Map to actual test name
            selected_test = test_name_map.get(test_selector, "Classification Test")

            # Store selection for persistence
            st.session_state['selected_test_in_dashboard_selector'] = test_selector

        with col2:
            if st.button("🔄 Refresh", key='dashboard_refresh_btn_2', width='content'):
                st.rerun()

        with col3:
            auto_refresh = st.checkbox("Auto-refresh", key='dashboard_auto_refresh_2', value=False)

        st.divider()

        # Add historical run selector for Test 5
        if test_selector == "test5":
            st.markdown("### 📂 Load Past Run (Historical Replay)")

            all_runs = DashboardLogger.list_all_runs()
            test5_runs = [r for r in all_runs if r.get("test_type") == "orchestrator"]

            if test5_runs:
                run_options = ["Live Execution"] + [
                    f"{r['run_id']} - {r.get('model', 'unknown')} - {r.get('timestamp', '')[:19]} ({r.get('summary_metrics', {}).get('final_score', 0.0):.2%} accuracy)"
                    for r in test5_runs
                ]

                selected_run = st.selectbox(
                    "Select Run to View",
                    options=run_options,
                    help="View historical test runs without re-execution"
                )

                # Load historical data if selected
                if selected_run != "Live Execution":
                    run_id = selected_run.split(" - ")[0]
                    try:
                        historical_data = DashboardLogger.load_run(run_id)
                        st.session_state['historical_run_data'] = historical_data
                        st.session_state['view_mode'] = 'historical'
                        st.success(f"✅ Loaded historical run: {run_id}")
                    except Exception as e:
                        st.error(f"Failed to load run: {e}")
                        st.session_state['view_mode'] = 'live'
                else:
                    st.session_state['view_mode'] = 'live'
            else:
                st.info("No historical Test 5 runs available yet. Run Test 5 to create logs.")
                st.session_state['view_mode'] = 'live'

            st.divider()

        # Create enhanced dashboard tabs
        dashboard_tabs = st.tabs([
            "📊 Overview",
            "📈 Gantt Timeline",
            "📋 Task Cards",
            "📜 Event Log",
            "📥 Export",
            "🧠 Memory Inspector",
            "🔒 Security Audit Log",
            "📜 Rethink History",
            "🔍 Interactive Tools"
        ])

        # --- TAB 1: OVERVIEW ---
        with dashboard_tabs[0]:
            st.subheader("Execution Overview")

            test_events = tracker.get_test_events(selected_test)

            if not test_events:
                st.info(f"No execution data for {selected_test}. Run the test first.")
            else:
                # KPIs
                col1, col2, col3, col4 = st.columns(4)

                with col1:
                    total_agents = len(set([e.agent_id for e in test_events]))
                    st.metric("Total Agents", total_agents)

                with col2:
                    completed = len([e for e in test_events if e.status == "complete"])
                    st.metric("Completed", completed)

                with col3:
                    running = len(tracker.active_agents)
                    st.metric("Running", running)

                with col4:
                    errors = len([e for e in test_events if e.status == "error"])
                    st.metric("Errors", errors)

                st.divider()

                # Timeline summary
                st.subheader("Timeline Summary")

                start_time = min([e.timestamp for e in test_events])
                end_time = max([e.timestamp for e in test_events])
                total_duration = end_time - start_time

                col1, col2 = st.columns(2)

                with col1:
                    st.metric("Total Duration", f"{total_duration:.1f}s")

                with col2:
                    durations = [e.duration for e in test_events if e.duration]
                    avg_agent_duration = sum(durations) / max(len(durations), 1) if durations else 0
                    st.metric("Avg Agent Duration", f"{avg_agent_duration:.2f}s")

        # --- TAB 2: GANTT TIMELINE ---
        with dashboard_tabs[1]:
            st.subheader("Execution Timeline (Gantt Chart)")

            test_events = tracker.get_test_events(selected_test)

            if not test_events:
                st.info("No events recorded for this test.")
            else:
                # Build Gantt data
                gantt_df = generate_gantt_data(selected_test, tracker)

                if gantt_df.empty:
                    st.info("No timeline data available.")
                else:
                    # Color scale based on status
                    color_map = {
                        'complete': '#10B981',
                        'running': '#F59E0B',
                        'error': '#EF4444',
                        'pending': '#94A3B8'
                    }

                    colors = [color_map.get(status, '#94A3B8') for status in gantt_df['Status']]

                    fig = go.Figure()

                    for idx, row in gantt_df.iterrows():
                        # Add hover text with metadata
                        hover_text = f"<b>{row['Task']}</b><br>"
                        hover_text += f"Start: {row['Start']:.1f}s<br>"
                        hover_text += f"Duration: {row['Duration']:.2f}s<br>"
                        hover_text += f"Status: {row['Status']}<br>"

                        if 'Metadata' in row and row['Metadata']:
                            metadata = row['Metadata']
                            if isinstance(metadata, dict):
                                for key, value in list(metadata.items())[:3]:  # Show first 3 metadata items
                                    if key != 'code':  # Skip code in hover
                                        hover_text += f"{key}: {value}<br>"

                        fig.add_trace(go.Bar(
                            y=[row['Task']],
                            x=[row['Duration']],
                            base=row['Start'],
                            orientation='h',
                            name=row['Task'],
                            marker=dict(color=colors[idx]),
                            text=f"{row['Progress']:.0f}%",
                            textposition='inside',
                            hovertemplate=hover_text + "<extra></extra>",
                            showlegend=False
                        ))

                    fig.update_layout(
                        title=f"{selected_test}: Execution Timeline",
                        xaxis_title="Time (seconds)",
                        yaxis_title="Agent / Task",
                        height=max(400, len(gantt_df) * 40),
                        showlegend=False,
                        hovermode='closest',
                        plot_bgcolor='#F7F7FB',
                        paper_bgcolor='white'
                    )

                    st.plotly_chart(fig, width='content', config=PLOTLY_CONFIG)

                    # Summary metrics below chart
                    col1, col2, col3, col4 = st.columns(4)

                    with col1:
                        st.metric("Total Tasks", len(gantt_df))

                    with col2:
                        completed = len(gantt_df[gantt_df['Status'] == 'complete'])
                        st.metric("Completed", completed)

                    with col3:
                        total_duration = gantt_df['Start'].max() + gantt_df.loc[gantt_df['Start'].idxmax(), 'Duration']
                        st.metric("Total Time", f"{total_duration:.1f}s")

                    with col4:
                        avg_progress = gantt_df['Progress'].mean()
                        st.metric("Avg Progress", f"{avg_progress:.0f}%")

        # --- TAB 3: TASK CARDS ---
        with dashboard_tabs[2]:
            render_task_cards(selected_test, tracker)

        # --- TAB 4: EVENT LOG ---
        with dashboard_tabs[3]:
            st.subheader("Event Log")

            # Check if we have historical Test 5 data with enhanced logging
            view_mode = st.session_state.get('view_mode', 'live')

            if view_mode == 'historical' and 'historical_run_data' in st.session_state and test_selector == "test5":
                # Enhanced Event Log for Test 5 with structured entries
                run_data = st.session_state['historical_run_data']
                execution_log = run_data.get('execution_log', [])

                if not execution_log:
                    st.info("No execution log data available for this run.")
                else:
                    st.markdown("### Structured Execution Log")

                    # Enhanced filter controls
                    col1, col2, col3, col4 = st.columns(4)

                    with col1:
                        all_event_types = set(entry.get('event_type', 'UNKNOWN') for entry in execution_log)
                        event_type_filter = st.multiselect(
                            "Event Type",
                            options=sorted(all_event_types),
                            default=list(all_event_types),
                            help="Filter by event type: TOOL_RULE_ENFORCED, MEMORY_WRITE, SECURITY_AUDIT, RETHINK_TRIGGERED"
                        )

                    with col2:
                        all_severities = set(entry.get('severity', 'INFO') for entry in execution_log)
                        severity_filter = st.multiselect(
                            "Severity",
                            options=sorted(all_severities),
                            default=list(all_severities),
                            help="Filter by severity: INFO, WARNING, ERROR, SECURITY_ALERT"
                        )

                    with col3:
                        all_agents = set(entry.get('agent', 'Unknown') for entry in execution_log)
                        agent_filter = st.multiselect(
                            "Agent",
                            options=sorted(all_agents),
                            default=list(all_agents)
                        )

                    with col4:
                        # Turn range filter
                        max_turn = max((entry.get('turn', 0) for entry in execution_log), default=0)
                        turn_range = st.slider(
                            "Turn Range",
                            min_value=0,
                            max_value=max_turn,
                            value=(0, max_turn),
                            help="Filter events by turn number"
                        )

                    # Apply filters
                    filtered_log = [
                        entry for entry in execution_log
                        if entry.get('event_type') in event_type_filter
                        and entry.get('severity') in severity_filter
                        and entry.get('agent') in agent_filter
                        and turn_range[0] <= entry.get('turn', 0) <= turn_range[1]
                    ]

                    # Display with visual indicators
                    if filtered_log:
                        st.caption(f"Showing {len(filtered_log)} of {len(execution_log)} events")

                        # Event type icons and colors
                        event_icons = {
                            'TOOL_RULE_ENFORCED': '⚙️',
                            'MEMORY_WRITE': '💾',
                            'SECURITY_AUDIT': '🔒',
                            'RETHINK_TRIGGERED': '🔄',
                            'CODE_GENERATED': '📝',
                            'VALIDATION': '✅'
                        }

                        severity_colors = {
                            'INFO': '🔵',
                            'WARNING': '🟡',
                            'ERROR': '🔴',
                            'SECURITY_ALERT': '🔴'
                        }

                        # Create enhanced table
                        log_data = []
                        for entry in filtered_log:
                            event_type = entry.get('event_type', 'UNKNOWN')
                            severity = entry.get('severity', 'INFO')

                            icon = event_icons.get(event_type, '📋')
                            color = severity_colors.get(severity, '⚪')

                            log_data.append({
                                'Turn': entry.get('turn', 0),
                                'Event': f"{icon} {event_type}",
                                'Severity': f"{color} {severity}",
                                'Agent': entry.get('agent', 'Unknown'),
                                'Message': entry.get('message', '')[:80] + "..." if len(entry.get('message', '')) > 80 else entry.get('message', ''),
                                'Timestamp': entry.get('timestamp', '')[:19]
                            })

                        df_log = pd.DataFrame(log_data)
                        st.dataframe(df_log, use_container_width=True, height=500)

                        # Event type distribution
                        st.divider()
                        st.markdown("### Event Distribution")

                        col1, col2 = st.columns(2)

                        with col1:
                            # Event type counts
                            from collections import Counter
                            event_counts = Counter([entry.get('event_type', 'UNKNOWN') for entry in filtered_log])

                            st.markdown("**Event Types:**")
                            for event_type, count in event_counts.most_common():
                                icon = event_icons.get(event_type, '📋')
                                st.text(f"{icon} {event_type}: {count}")

                        with col2:
                            # Severity counts
                            severity_counts = Counter([entry.get('severity', 'INFO') for entry in filtered_log])

                            st.markdown("**Severity Levels:**")
                            for severity, count in severity_counts.most_common():
                                color = severity_colors.get(severity, '⚪')
                                st.text(f"{color} {severity}: {count}")
                    else:
                        st.info("No events match the selected filters.")
            else:
                # Standard Event Log for other tests
                test_events = tracker.get_test_events(selected_test)

                if not test_events:
                    st.info("No events to display.")
                else:
                    # Filter controls
                    col1, col2 = st.columns([1, 1])

                    with col1:
                        event_type_filter = st.multiselect(
                            "Event Type",
                            options=["start", "progress", "complete", "error"],
                            default=["start", "complete", "error"]
                        )

                    with col2:
                        agent_type_filter = st.multiselect(
                            "Agent Type",
                            options=["orchestrator", "main_agent", "sub_agent", "batch"],
                            default=["orchestrator", "main_agent", "sub_agent", "batch"]
                        )

                    # Filter events
                    filtered_events = [
                        e for e in test_events
                        if e.event_type in event_type_filter and e.agent_type in agent_type_filter
                    ]

                    # Display as table
                    if filtered_events:
                        df_events = pd.DataFrame([
                            {
                                'Timestamp': f"{e.timestamp:.2f}s",
                                'Event': e.event_type,
                                'Agent': e.agent_name,
                                'Type': e.agent_type,
                                'Status': e.status,
                                'Progress': f"{e.progress:.0f}%",
                                'Duration': f"{e.duration:.2f}s" if e.duration else "—",
                                'Metadata': str(e.metadata)[:50] + "..." if e.metadata else ""
                            }
                            for e in filtered_events
                        ])

                        st.dataframe(df_events, width='content', height=400)
                    else:
                        st.info("No events match the filters.")

        # --- TAB 5: EXPORT ---
        with dashboard_tabs[4]:
            st.subheader("Export Execution Data")

            test_events = tracker.get_test_events(selected_test)

            if not test_events:
                st.info("No data to export.")
            else:
                # Export options
                col1, col2 = st.columns(2)

                with col1:
                    st.markdown("**Export Timeline Data**")

                    df_export = tracker.export_timeline(selected_test)

                    csv_bytes = df_export.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        "📥 Download CSV",
                        data=csv_bytes,
                        file_name=f"{selected_test.lower().replace(' ', '_')}_timeline.csv",
                        mime="text/csv",
                        width='content'
                    )

                    json_str = df_export.to_json(orient='records', indent=2)
                    st.download_button(
                        "📥 Download JSON",
                        data=json_str,
                        file_name=f"{selected_test.lower().replace(' ', '_')}_timeline.json",
                        mime="application/json",
                        width='content'
                    )

                with col2:
                    st.markdown("**Export Gantt Chart**")

                    # Generate Gantt data
                    gantt_df = generate_gantt_data(selected_test, tracker)

                    if not gantt_df.empty:
                        csv_gantt = gantt_df.to_csv(index=False).encode('utf-8')
                        st.download_button(
                            "📥 Download Gantt CSV",
                            data=csv_gantt,
                            file_name=f"{selected_test.lower().replace(' ', '_')}_gantt.csv",
                            mime="text/csv",
                            width='content'
                        )

                # Preview
                st.markdown("**Data Preview**")
                st.dataframe(df_export.head(20), width='content')

        # --- TAB 6: MEMORY INSPECTOR ---
        with dashboard_tabs[5]:
            st.subheader("🧠 Memory Inspector")

            # Get historical data if in historical mode
            view_mode = st.session_state.get('view_mode', 'live')

            if view_mode == 'historical' and 'historical_run_data' in st.session_state:
                run_data = st.session_state['historical_run_data']
                memory_snapshots = run_data.get('memory_snapshots', [])

                if not memory_snapshots:
                    st.info("No memory data available for this run.")
                else:
                    # Get latest snapshot
                    latest_snapshot = memory_snapshots[-1]

                    st.markdown("### Current Memory Blocks")
                    core_blocks = latest_snapshot.get('core_blocks', {})

                    for block_name, block_data in core_blocks.items():
                        with st.expander(f"📝 {block_name}", expanded=False):
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Version", block_data.get('version', 1))
                            with col2:
                                st.metric("Last Modified Turn", block_data.get('last_modified_turn', 0))
                            with col3:
                                st.metric("Modifications", block_data.get('modification_count', 0))

                            st.code(block_data.get('content', ''), language='text')

                    st.divider()

                    st.markdown("### Archival Memory Index")
                    archival_entries = latest_snapshot.get('archival_entries', [])

                    if archival_entries:
                        # Pagination
                        page_size = 50
                        total_pages = (len(archival_entries) + page_size - 1) // page_size

                        page = st.number_input("Page", min_value=1, max_value=total_pages, value=1)
                        start_idx = (page - 1) * page_size
                        end_idx = min(start_idx + page_size, len(archival_entries))

                        # Filters
                        col1, col2 = st.columns(2)
                        with col1:
                            all_tags = set()
                            for entry in archival_entries:
                                all_tags.update(entry.get('tags', []))

                            tag_filter = st.multiselect("Filter by Tags", options=sorted(all_tags))

                        with col2:
                            all_agents = set(entry.get('source_agent', 'Unknown') for entry in archival_entries)
                            agent_filter = st.multiselect("Filter by Source Agent", options=sorted(all_agents))

                        # Apply filters
                        filtered_entries = archival_entries
                        if tag_filter:
                            filtered_entries = [e for e in filtered_entries if any(tag in e.get('tags', []) for tag in tag_filter)]
                        if agent_filter:
                            filtered_entries = [e for e in filtered_entries if e.get('source_agent') in agent_filter]

                        # Display entries
                        st.caption(f"Showing {start_idx + 1}-{end_idx} of {len(filtered_entries)} entries")

                        for entry in filtered_entries[start_idx:end_idx]:
                            with st.expander(f"[{entry.get('source_agent', 'Unknown')}] {entry.get('content', '')[:100]}...", expanded=False):
                                st.text(entry.get('content', ''))
                                st.caption(f"Tags: {', '.join(entry.get('tags', []))} | {entry.get('timestamp', '')}")
                    else:
                        st.info("No archival memory entries.")

                    st.divider()

                    st.markdown("### Memory Statistics")
                    stats = latest_snapshot.get('statistics', {})

                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("Total Entries", len(archival_entries))
                    with col2:
                        st.metric("Total Retrievals", stats.get('total_retrievals', 0))
                    with col3:
                        st.metric("Cache Hits", stats.get('cache_hits', 0))
                    with col4:
                        avg_latency = stats.get('avg_retrieval_latency_ms', 0.0)
                        st.metric("Avg Retrieval Latency", f"{avg_latency:.2f}ms")
            else:
                st.info("Memory Inspector is only available for Test 5 historical runs. Run Test 5 and then load a past run to view memory data.")

        # --- TAB 7: SECURITY AUDIT LOG ---
        with dashboard_tabs[6]:
            st.subheader("🔒 Security Audit Log")

            view_mode = st.session_state.get('view_mode', 'live')

            if view_mode == 'historical' and 'historical_run_data' in st.session_state:
                run_data = st.session_state['historical_run_data']
                security_audits = run_data.get('security_audits', [])

                if not security_audits:
                    st.info("No security audit data available for this run.")
                else:
                    st.markdown("### Audit Events Table")

                    # Filters
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        all_operations = set(audit.get('operation_type', 'Unknown') for audit in security_audits)
                        operation_filter = st.multiselect("Operation Type", options=sorted(all_operations), default=list(all_operations))

                    with col2:
                        all_risk_levels = set(audit.get('risk_level', 'UNKNOWN') for audit in security_audits)
                        risk_filter = st.multiselect("Risk Level", options=sorted(all_risk_levels), default=list(all_risk_levels))

                    with col3:
                        all_statuses = set(audit.get('status', 'Unknown') for audit in security_audits)
                        status_filter = st.multiselect("Status", options=sorted(all_statuses), default=list(all_statuses))

                    # Apply filters
                    filtered_audits = [
                        audit for audit in security_audits
                        if audit.get('operation_type') in operation_filter
                        and audit.get('risk_level') in risk_filter
                        and audit.get('status') in status_filter
                    ]

                    # Display table
                    audit_data = []
                    for audit in filtered_audits:
                        status_icon = {
                            'Pass': '🟢',
                            'Blocked': '🔴',
                            'Warning': '🟡'
                        }.get(audit.get('status', ''), '⚪')

                        audit_data.append({
                            'Turn': audit.get('turn', 0),
                            'Agent': audit.get('agent', 'Unknown'),
                            'Operation': audit.get('operation_type', 'Unknown'),
                            'Risk': audit.get('risk_level', 'UNKNOWN'),
                            'Status': f"{status_icon} {audit.get('status', 'Unknown')}",
                            'Details': str(audit.get('details', {}))[:50] + "..."
                        })

                    if audit_data:
                        st.dataframe(pd.DataFrame(audit_data), use_container_width=True, height=400)

                        # Security Metrics Dashboard
                        st.divider()
                        st.markdown("### Security Metrics")

                        col1, col2, col3, col4 = st.columns(4)

                        with col1:
                            st.metric("Total Audits", len(security_audits))

                        with col2:
                            blocked = len([a for a in security_audits if a.get('status') == 'Blocked'])
                            st.metric("Blocked Operations", blocked)

                        with col3:
                            warnings = len([a for a in security_audits if a.get('status') == 'Warning'])
                            st.metric("Warnings", warnings)

                        with col4:
                            passed = len([a for a in security_audits if a.get('status') == 'Pass'])
                            st.metric("Passed", passed)

                        # Pie chart: Distribution of operation types
                        operation_counts = {}
                        for audit in security_audits:
                            op_type = audit.get('operation_type', 'Unknown')
                            operation_counts[op_type] = operation_counts.get(op_type, 0) + 1

                        if operation_counts:
                            fig = go.Figure(data=[go.Pie(
                                labels=list(operation_counts.keys()),
                                values=list(operation_counts.values()),
                                hole=0.3
                            )])

                            fig.update_layout(
                                title="Distribution of Operation Types",
                                height=300
                            )

                            st.plotly_chart(fig, use_container_width=True, config=PLOTLY_CONFIG)
                    else:
                        st.info("No audits match the selected filters.")
            else:
                st.info("Security Audit Log is only available for Test 5 historical runs. Run Test 5 and then load a past run to view security data.")

        # --- TAB 8: RETHINK HISTORY ---
        with dashboard_tabs[7]:
            st.subheader("📜 Rethink History")

            view_mode = st.session_state.get('view_mode', 'live')

            if view_mode == 'historical' and 'historical_run_data' in st.session_state:
                run_data = st.session_state['historical_run_data']
                rethink_history = run_data.get('rethink_history', [])

                if not rethink_history:
                    st.info("No rethink events recorded for this run.")
                else:
                    st.markdown("### Policy Modification Log")

                    # Display rethink events
                    for idx, event in enumerate(rethink_history, 1):
                        with st.expander(f"🔄 Rethink #{idx} - Turn {event.get('turn', 0)} - {event.get('block_name', 'Unknown')}", expanded=False):
                            col1, col2 = st.columns(2)

                            with col1:
                                st.markdown("**Trigger:**")
                                st.text(event.get('trigger', 'N/A'))

                                st.markdown("**Change Summary:**")
                                st.text(event.get('change_summary', 'N/A'))

                            with col2:
                                st.markdown("**Revert Point ID:**")
                                st.code(event.get('revert_point_id', 'N/A'))

                                st.markdown("**Timestamp:**")
                                st.text(event.get('timestamp', 'N/A'))

                            st.divider()

                            # Side-by-side diff
                            st.markdown("**Content Diff:**")
                            col_before, col_after = st.columns(2)

                            with col_before:
                                st.markdown("*Before:*")
                                st.code(event.get('old_content', ''), language='text')

                            with col_after:
                                st.markdown("*After:*")
                                st.code(event.get('new_content', ''), language='text')

                    st.divider()

                    # Self-Correction Analytics
                    st.markdown("### Self-Correction Analytics")

                    # Most frequently modified blocks
                    from collections import Counter
                    block_counts = Counter([event.get('block_name', 'Unknown') for event in rethink_history])
                    most_common = block_counts.most_common(3)

                    if most_common:
                        st.markdown("**Most Frequently Modified Blocks:**")
                        for block_name, count in most_common:
                            st.text(f"  • {block_name}: {count} modification(s)")

                    # Average turns between rethinks
                    if len(rethink_history) > 1:
                        turns = [event.get('turn', 0) for event in rethink_history]
                        avg_gap = sum(turns[i+1] - turns[i] for i in range(len(turns)-1)) / (len(turns) - 1)
                        st.metric("Average Turns Between Rethinks", f"{avg_gap:.1f}")
            else:
                st.info("Rethink History is only available for Test 5 historical runs. Run Test 5 and then load a past run to view rethink data.")

        # --- TAB 9: INTERACTIVE TOOLS ---
        with dashboard_tabs[8]:
            st.subheader("🔍 Interactive Tools")

            view_mode = st.session_state.get('view_mode', 'live')

            if view_mode == 'historical' and 'historical_run_data' in st.session_state:
                run_data = st.session_state['historical_run_data']
                memory_snapshots = run_data.get('memory_snapshots', [])

                if not memory_snapshots:
                    st.info("No memory data available for interactive tools.")
                else:
                    latest_snapshot = memory_snapshots[-1]
                    archival_entries = latest_snapshot.get('archival_entries', [])

                    # Memory Search Simulator
                    st.markdown("### 🔍 Memory Search Simulator")
                    st.caption("Demonstrate RAG mechanism interactively")

                    col1, col2 = st.columns([3, 1])

                    with col1:
                        search_query = st.text_input("Search Query", placeholder="Enter keywords to search archival memory...")

                    with col2:
                        # Tag filters
                        all_tags = set()
                        for entry in archival_entries:
                            all_tags.update(entry.get('tags', []))

                        tag_filters = st.multiselect("Filter by Tags", options=sorted(all_tags))

                    if st.button("🔍 Search Archival Memory", type="primary"):
                        if not search_query:
                            st.warning("Please enter a search query.")
                        else:
                            # Simple keyword search
                            results = []
                            query_lower = search_query.lower()

                            for entry in archival_entries:
                                # Tag filter
                                if tag_filters and not any(tag in entry.get('tags', []) for tag in tag_filters):
                                    continue

                                # Keyword matching
                                content = entry.get('content', '').lower()
                                if query_lower in content:
                                    # Calculate simple relevance score (keyword frequency)
                                    relevance = content.count(query_lower) / max(len(content.split()), 1)
                                    results.append((entry, relevance))

                            # Sort by relevance
                            results.sort(key=lambda x: x[1], reverse=True)
                            top_results = results[:5]

                            if top_results:
                                st.success(f"Found {len(results)} matching entries. Showing top 5:")

                                for idx, (entry, relevance) in enumerate(top_results, 1):
                                    with st.expander(f"Result #{idx} (Relevance: {relevance:.4f})", expanded=idx==1):
                                        st.text(entry.get('content', ''))
                                        st.caption(f"Source: {entry.get('source_agent', 'Unknown')} | Tags: {', '.join(entry.get('tags', []))}")
                            else:
                                st.info("No matching entries found.")

                    st.divider()

                    # Tool Rule Tester
                    st.markdown("### ⚙️ Tool Rule Tester")
                    st.caption("Test tool usage rules from policy blocks")

                    core_blocks = latest_snapshot.get('core_blocks', {})
                    tool_guidelines = core_blocks.get('Tool Guidelines', {})

                    if tool_guidelines:
                        st.markdown("**Current Tool Guidelines:**")
                        st.code(tool_guidelines.get('content', 'No guidelines available'), language='text')

                        st.markdown("**Test a Task:**")
                        test_task = st.text_area("Sample Task Description", placeholder="Enter a task description to test against tool rules...")

                        if st.button("🧪 Test Rule Enforcement", type="secondary"):
                            if not test_task:
                                st.warning("Please enter a task description.")
                            else:
                                # Simple rule matching (can be enhanced)
                                guidelines_text = tool_guidelines.get('content', '').lower()
                                task_lower = test_task.lower()

                                # Check for common rule keywords
                                triggered_rules = []

                                if 'verify' in guidelines_text and 'verify' not in task_lower:
                                    triggered_rules.append("⚠️ Guideline suggests verification, but task doesn't mention it")

                                if 'test' in guidelines_text and 'test' not in task_lower:
                                    triggered_rules.append("⚠️ Guideline suggests testing, but task doesn't mention it")

                                if 'cautious' in guidelines_text or 'careful' in guidelines_text:
                                    triggered_rules.append("ℹ️ Guidelines emphasize caution - ensure proper validation")

                                if triggered_rules:
                                    st.warning("**Rule Enforcement Suggestions:**")
                                    for rule in triggered_rules:
                                        st.text(f"  • {rule}")
                                else:
                                    st.success("✅ Task appears to align with tool guidelines")
                    else:
                        st.info("No Tool Guidelines available in this run.")

            # --- LIVE SMOKE TEST UTILITY (Always Available) ---
            st.divider()
            st.subheader("🔍 Live Smoke Test Utility")
            st.caption("Rapidly validate core planning, policy RAG, and execution across key scenarios.")

            smoke_scenario = st.selectbox(
                "Select Scenario for Live Smoke Test",
                options=list(SMOKE_TEST_SCENARIOS.keys()),
                index=0,
                key='smoke_test_select_final'
            )

            st.markdown(f"**Goal:** {SMOKE_TEST_SCENARIOS[smoke_scenario]['goal'][:150]}...")

            # Display memory policy if applicable
            policy = SMOKE_TEST_SCENARIOS[smoke_scenario]['policy']
            if policy:
                with st.expander(f"Policy Loaded (RAG Source): {policy.splitlines()[0]}", expanded=False):
                    st.code(policy, language='text')

            if st.button("▶️ Run Live Smoke Test (1-Turn Execution)", type="primary", use_container_width=True):
                if not GEMINI_API_KEY:
                    st.error("GEMINI_API_KEY is required to run the smoke test.")
                else:
                    # Run test synchronously and update dashboard selection
                    with st.spinner(f"Running **{smoke_scenario}**... (1 turn execution)"):
                        test_result = asyncio.run(run_live_smoke_test(smoke_scenario))

                    # Immediate Display and Logging
                    st.markdown("---")
                    if test_result['success']:
                        st.success("✅ Smoke Test PASSED: Core Orchestration Path Verified")
                        st.markdown("### Final Answer")
                        st.markdown(test_result['final_answer'])
                        st.markdown("### Specialized Output")
                        st.code(test_result['code_output'], language='python')

                        st.info(f"📊 Log available in **Event Log** and **Gantt Timeline** tabs. Select 'Smoke Test Run' from the test selector above.")
                    else:
                        st.error("❌ Smoke Test FAILED: Orchestrator Encountered an Error")
                        st.text(f"Error: {test_result['error']}")
                        st.warning("Check API Keys and ensure GEMINI_API_KEY is set correctly.")

                    # Clean up temporary policy setting
                    if 'demo_memory_policy' in st.session_state:
                        del st.session_state['demo_memory_policy']
                    if 'demo_scenario' in st.session_state:
                        del st.session_state['demo_scenario']

                    # Force dashboard selector to point to the smoke test to show log immediately
                    st.session_state['selected_test_in_dashboard'] = 'Smoke Test Run'
                    st.session_state['selected_test_in_dashboard_selector'] = 'smoke_test'
                    st.rerun()

        # Auto-refresh
        if auto_refresh:
            time.sleep(2)
            st.rerun()


# ---------- Footer: download + run cmd ----------
# Conditionally display the main DataFrame state and download button for Tests 1-3
if show_main_df_previews:
    st.divider()
    st.subheader("Current DataFrame State")
    # Truncate long rationale columns for display in footer preview
    display_df = st.session_state.df.copy()
    rationale_cols = [c for c in display_df.columns if c.endswith("_rationale") or c == "judge_rationale"]
    for col in rationale_cols:
        if col in display_df.columns:
            display_df[col] = display_df[col].astype(str).str.slice(0, 150) + "..."
    st.dataframe(display_df, use_container_width=True)

    col1, col2 = st.columns([1, 3]) # Give more space to the button
    with col1:
        csv_bytes = st.session_state.df.to_csv(index=False).encode("utf-8")
        st.download_button("⬇️ Download Results as CSV", data=csv_bytes, file_name="classification_results.csv", mime="text/csv", use_container_width=True)
    with col2:
        pass # Keep layout clean

st.divider()
st.code("streamlit run streamlit_app.py", language="bash")