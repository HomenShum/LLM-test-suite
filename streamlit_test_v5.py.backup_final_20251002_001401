# streamlit_app.py
# ============================================================
# Structured classification + evaluation suite (Enhanced Version)
# - Uses your file context, env ingestion, and async style
# - Providers: OpenRouter + OpenAI
# - Tests:
#   1) CSV classify + F1/Latency (two models) + Error Analysis
#   2) Add third model + weighted pick by (per-class F1 * row confidence)
#   3) Mistral as Judge over 3 models + Judge evaluation
#   4) Quantitative context pruning + action decision test
# ============================================================

import os, asyncio, json, time, re
import pandas as pd
import streamlit as st
import httpx, nest_asyncio
from typing import List, Optional, Tuple, Dict, Any, Set
from pydantic import BaseModel, ValidationError, Field
from openai import AsyncOpenAI
from google import genai
from google.genai import types
from pathlib import Path

from dotenv import load_dotenv
from collections import Counter, defaultdict

# --- NEW IMPORTS for orchestrator infrastructure ---
from dataclasses import dataclass, field
from enum import Enum
import hashlib

# --- LEAF AGENT SCAFFOLD IMPORTS ---
# Only import what we directly use in streamlit_test_v5.py
# (Other classes like LeafAgent, SubTask, etc. are used internally by these)
from leaf_agent_scaffold import (
    SupervisorAgent,
    AgentType,
    WebResearchAgent,
    CodeExecutorAgent,
    ContentGeneratorAgent,
    ValidatorAgent,
    TaskPlanner,
    ResultSynthesizer,
    PolicyUpdater  # For self-correcting research pipeline
)

# --- STATEFUL COMPONENTS IMPORTS ---
from utils.dashboard_logger import DashboardLogger
from utils.stateful_components import (
    MemoryManager,
    SecurityAuditAgent,
    SelfCorrectionManager
)

# --- NEW IMPORTS for enhanced testing ---
from sklearn.metrics import classification_report, confusion_matrix

# --- PLOTLY IMPORTS for interactive visualizations ---
import plotly.graph_objects as go
from plotly.subplots import make_subplots

import warnings

# --- REFACTORED IMPORTS: Extracted modules ---
from config.scenarios import (
    PI_AGENT_GOAL_PROMPT,
    FOLDING_POLICY_BLOCK,
    CYBERSECURITY_GOAL_PROMPT,
    THREAT_POLICY_BLOCK,
    SMOKE_TEST_SCENARIOS,
    SUGGESTED_PROMPTS,
    DEFAULT_DATASET_PROMPTS,
    SKELETON_COLUMNS,
    ROW_LIMIT_OPTIONS,
    CANON_MAP as _CANON_MAP,
    TEST_FLOWS,
    JUDGE_SCHEMA,
    JUDGE_INSTRUCTIONS
)

from utils.visualizations import (
    render_test_flow_diagram,
    render_kpi_metrics,
    render_cost_dashboard,
    visualize_dataset_composition,
    render_model_comparison_chart
)

from utils.gantt_charts import (
    render_agent_gantt_chart,
    render_test5_gantt_chart
)

from utils.ui_components import (
    ModelSelector,
    ConfigDisplay,
    TestResultTabs
)

# --- NEW PHASE 2 IMPORTS: Additional extracted modules ---
from core.models import (
    Classification,
    ClassificationWithConf,
    SyntheticDataItem,
    ToolCallSequenceItem,
    PruningDataItem,
    TestSummaryAndRefinement,
    FactualConstraint,
    ValidationResultArtifact,
    convert_validation_to_artifact
)

from core.pricing import (
    fetch_openrouter_pricing,
    _to_openrouter_model_id,
    _to_native_model_id,
    _get_provider_from_model_id,
    custom_openrouter_price_lookup,
    custom_gemini_price_lookup,
    get_all_available_models
)

from utils.model_discovery import (
    fetch_openrouter_models_for_ui,
    fetch_openai_models,
    get_third_model_display_name,
    _normalize_ollama_root,
    OPENROUTER_MODEL,
    OPENAI_MODEL,
    THIRD_MODEL_KIND,
    THIRD_MODEL
)

from utils.data_helpers import (
    ensure_dataset_directory,
    save_dataset_to_file,
    save_results_df,
    load_classification_dataset,
    load_tool_sequence_dataset,
    load_context_pruning_dataset,
    _load_df_from_path,
    auto_generate_default_datasets,
    check_and_generate_datasets,
    _allowed_labels,
    _subset_for_run,
    _style_selected_rows,
    _normalize_label,
    DATASET_DIR,
    CLASSIFICATION_DATASET_PATH,
    TOOL_SEQUENCE_DATASET_PATH,
    CONTEXT_PRUNING_DATASET_PATH
)

from utils.helpers import (
    _retry,
    enhance_prompt_with_user_input,
    capture_run_config,
    display_run_config,
    _non_empty
)

from utils.execution_tracker import (
    ExecutionEvent,
    ExecutionTracker
)

from core.api_clients import (
    classify_with_openai,
    classify_with_gemini,
    classify_with_ollama,
    classify_with_openrouter,
    openai_structured_json,
    openrouter_json,
    ollama_json,
    generate_text_async,
    _classify_df_async,
    generate_synthetic_data,
    run_judge_ollama,
    run_pruner
)

# --- NEW PHASE 3 IMPORTS: Aggressive extraction modules ---
from core.unified_orchestrator import UnifiedOrchestrator

from core.test_runners import run_classification_flow

from utils.advanced_visualizations import (
    render_model_comparison_chart,
    render_organized_results,
    render_progress_replay,
    render_universal_gantt_chart,
    render_task_cards,
    render_single_task_card,
    render_live_agent_status,
    render_agent_task_cards
)

# --- PLOTLY CONFIG (to avoid deprecation warnings) ---
PLOTLY_CONFIG = {
    "displaylogo": False,
    "responsive": True,
    "scrollZoom": True
}

# ============================================================
# DEMONSTRATION SCENARIO CONFIGURATIONS
# ============================================================
# NOTE: Scenario configurations moved to config/scenarios.py
# Imported above as: PI_AGENT_GOAL_PROMPT, FOLDING_POLICY_BLOCK, etc.

async def run_live_smoke_test(scenario_key: str):
    """
    Executes a single-turn, critical-path smoke test for the selected scenario,
    with full logging to ExecutionTracker for immediate dashboard visibility.

    Args:
        scenario_key: Key from SMOKE_TEST_SCENARIOS dict

    Returns:
        Dict with success status, final answer, metadata, and code output
    """
    config = SMOKE_TEST_SCENARIOS[scenario_key]
    goal = config['goal']
    agents_to_use = config['agents']
    policy = config['policy']
    mode = config['mode']

    test_data = None
    if mode == 'inference':
        test_data = [{"query": "Test query for smoke test", "expected_sequence": []}]

    # Use a unique test name for the smoke test that the dashboard can filter on
    SMOKE_TEST_NAME = "Smoke Test Run"

    # Get tracker (don't reset to preserve other test data)
    tracker = st.session_state.get('execution_tracker')
    if not tracker:
        tracker = ExecutionTracker()
        st.session_state['execution_tracker'] = tracker

    # Temporarily configure specialized memory for the run
    if 'demo_memory_policy' in st.session_state:
        del st.session_state['demo_memory_policy']
    if policy:
        st.session_state['demo_memory_policy'] = policy
        st.session_state['demo_scenario'] = scenario_key.split('.')[1].strip()

    # Initialize Orchestrator and force load policies
    orchestrator = UnifiedOrchestrator(
        goal=goal,
        test_data=test_data,
        budget=Budget(mode="turns", max_turns=1),
        mode=mode,
        coordination_pattern='leaf_scaffold',
    )

    # Force initialize to load policies
    orchestrator._initialize_stateful_components()

    # Log Orchestrator start
    orch_id = f"smoke_{hash(scenario_key)}"
    tracker.emit(SMOKE_TEST_NAME, "start", orch_id, f"Smoke Test: {scenario_key}", "orchestrator", goal=goal[:80])

    # Execution - Single turn of the Leaf Agent Scaffold
    llm_client = GeminiLLMClient(GEMINI_API_KEY)
    leaf_agents = []
    agent_map = {
        "web_researcher": WebResearchAgent,
        "code_executor": CodeExecutorAgent,
        "validator": ValidatorAgent,
        "content_generator": ContentGeneratorAgent
    }

    # Map roles for clearer logging
    role_map = {
        "web_researcher": {"PI": "Vision/Perception Agent", "Cyber": "Threat Intel Agent"},
        "code_executor": {"PI": "Motor Control Agent", "Cyber": "Risk Score Agent"},
        "validator": {"PI": "Perception Feedback Agent", "Cyber": "Validator Agent"},
        "content_generator": {"PI": "Task Summary Agent", "Cyber": "Report Generator Agent"},
    }
    demo_type = "PI" if "PI Agent" in scenario_key else ("Cyber" if "Cybersecurity" in scenario_key else "General")

    for agent_type_str in agents_to_use:
        agent_class = agent_map.get(agent_type_str)
        if agent_class:
            agent = agent_class(llm_client)
            # Override name for demo clarity
            agent.name = role_map.get(agent_type_str, {}).get(demo_type, agent_class.__name__)
            leaf_agents.append(agent)

    supervisor = SupervisorAgent(leaf_agents, memory_manager=orchestrator.memory_manager)
    supervisor.task_planner = GeminiTaskPlanner(
        available_agents=[a.agent_type for a in leaf_agents],
        llm_client=llm_client
    )
    supervisor.result_synthesizer = GeminiResultSynthesizer(llm_client)

    # Turn Execution
    try:
        # Step 1: Planning
        planning_id = f"{orch_id}_plan"
        tracker.emit(SMOKE_TEST_NAME, "start", planning_id, "Task Planner", "sub_agent", parent_id=orch_id)
        sub_tasks = await supervisor.task_planner.decompose(goal, mode)
        tracker.emit(SMOKE_TEST_NAME, "complete", planning_id, "Task Planner", "sub_agent", parent_id=orch_id, tasks_count=len(sub_tasks))

        # Step 2: Sequential Execution of subtasks
        results = []
        for i, task in enumerate(sub_tasks):
            task_id = f"{orch_id}_task_{i}"
            agent_name = next((a.name for a in leaf_agents if a.agent_type == task.agent_type), task.agent_type.value)

            tracker.emit(SMOKE_TEST_NAME, "start", task_id, agent_name, "leaf_agent", parent_id=orch_id, description=task.description[:50])
            result = await supervisor.execute_single_task(task)
            results.append(result)

            status_key = "success" if result.success else "failure"
            tracker.emit(SMOKE_TEST_NAME, "complete", task_id, agent_name, "leaf_agent", parent_id=orch_id, success=result.success, output_size=len(result.output) if result.output else 0, status=status_key)

        # Step 3: Synthesis
        synthesis_id = f"{orch_id}_synth"
        tracker.emit(SMOKE_TEST_NAME, "start", synthesis_id, "Result Synthesizer", "sub_agent", parent_id=orch_id)
        final_result = await supervisor.result_synthesizer.synthesize(goal, results)
        tracker.emit(SMOKE_TEST_NAME, "complete", synthesis_id, "Result Synthesizer", "sub_agent", parent_id=orch_id, answer_size=len(final_result.answer))

        # Log Orchestrator completion
        tracker.emit(SMOKE_TEST_NAME, "complete", orch_id, f"Smoke Test: {scenario_key}", "orchestrator", final_score=1.0)

        return {
            "success": True,
            "final_answer": final_result.answer,
            "metadata": final_result.metadata,
            "code_output": next((r.output for r in results if r.agent_name in ["Motor Control Agent", "Risk Score Agent", "Code Executor Agent"] and r.output), "No specific code output.")
        }
    except Exception as e:
        # Log Orchestrator error
        tracker.emit(SMOKE_TEST_NAME, "error", orch_id, f"Smoke Test: {scenario_key}", "orchestrator", error=str(e))
        return {
            "success": False,
            "error": str(e),
            "metadata": orchestrator.memory_manager.get_snapshot(99) if orchestrator.memory_manager else {}
        }

# ============================================================
# HELPER FUNCTIONS FOR USER-INPUT-AWARE PROMPTS
# ============================================================

def enhance_prompt_with_user_input(base_prompt: str, user_input: str, context: Dict[str, Any]) -> str:
    """
    Enhances a generated prompt by incorporating user's existing input.

    Args:
        base_prompt: The generated template prompt
        user_input: What the user has already typed
        context: Additional context (dataset info, model selection, etc.)

    Returns:
        Enhanced prompt that preserves user intent while adding suggestions
    """
    if not user_input or len(user_input.strip()) < 10:
        return base_prompt

    # Extract key entities from user input
    user_lower = user_input.lower()

    # Check what user has already specified
    has_dataset_ref = any(word in user_lower for word in ['dataset', 'examples', 'test data', 'queries'])
    has_goal = any(word in user_lower for word in ['goal', 'objective', 'target', 'achieve'])
    has_approach = any(word in user_lower for word in ['approach', 'strategy', 'method', 'using'])

    # Build enhanced prompt
    sections = []

    # Preserve user's input as the primary goal
    sections.append(f"USER'S OBJECTIVE:\n{user_input.strip()}\n")

    # Add dataset context if not mentioned
    if not has_dataset_ref and context.get('dataset_size'):
        sections.append(f"DATASET CONTEXT:\n- {context['dataset_size']} examples available\n- Columns: {', '.join(context.get('columns', []))}\n")

    # Add approach suggestions if not mentioned
    if not has_approach and context.get('suggested_approach'):
        sections.append(f"SUGGESTED APPROACH:\n{context['suggested_approach']}\n")

    # Add success criteria if not mentioned
    if not has_goal and context.get('success_criteria'):
        sections.append(f"SUCCESS CRITERIA:\n{context['success_criteria']}\n")

    return '\n'.join(sections)


# ============================================================
# UNIVERSAL EXECUTION TRACKING SYSTEM
# ============================================================

# ExecutionEvent and ExecutionTracker moved to utils/execution_tracker.py

# --- COST TRACKER IMPORTS ---
from cost_tracker import CostTracker, combined_price_lookup, register_all_extractors


# --- TARGETED SUPPRESSION FOR PLOTLY/STREAMLIT KEYWORD ARGUMENTS ---
# Suppress the specific deprecation message and any Plotly deprecations that bubble up via Streamlit logs
warnings.filterwarnings(
    "ignore",
    message=r".*keyword arguments have been deprecated.*Use `config` instead.*",
)
# Also suppress general Plotly deprecation/future warnings that Streamlit may surface
warnings.filterwarnings("ignore", category=DeprecationWarning, module=r"^plotly(\.|$)")
warnings.filterwarnings("ignore", category=FutureWarning, module=r"^plotly(\.|$)")

load_dotenv()  # load env vars
nest_asyncio.apply()  # patch loop for Streamlit


# ---------- Config / env ----------
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", "")
# --- FIX: Use latest GPT-5 series as default (OpenRouter format) ---
OPENAI_MODEL = st.secrets.get("OPENAI_MODEL", "openai/gpt-5-mini")

# --- NEW: Dataset Directory Configuration ---
DATASET_DIR = st.secrets.get("DATASET_DIR", "test_dataset")
CLASSIFICATION_DATASET_PATH = os.path.join(DATASET_DIR, "classification_dataset.csv")
TOOL_SEQUENCE_DATASET_PATH = os.path.join(DATASET_DIR, "tool_sequence_dataset.csv")
CONTEXT_PRUNING_DATASET_PATH = os.path.join(DATASET_DIR, "context_pruning_dataset.csv")
# -------------------------------------------

OLLAMA_BASE_URL = st.secrets.get("OLLAMA_BASE_URL", "http://localhost:11434/api/generate")
OLLAMA_MODEL = st.secrets.get("OLLAMA_MODEL", "mistral-small:24b-instruct-2501-q4_K_M")

# Optional third model (env-driven)
THIRD_KIND = st.secrets.get("THIRD_KIND", "None")
THIRD_MODEL = st.secrets.get("THIRD_MODEL", "mistralai/mistral-small-3.2-24b-instruct" if THIRD_KIND == "OpenRouter" else "gpt-5-mini")

GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "")
# --- FIX: Use latest Gemini 2.5 series as default (OpenRouter format) ---
GEMINI_MODEL = st.secrets.get("GEMINI_MODEL", "google/gemini-2.5-flash")

# OpenRouter config
OPENROUTER_API_KEY = st.secrets.get("OPENROUTER_API_KEY", "")
OPENROUTER_MODEL = st.secrets.get("OPENROUTER_MODEL", "mistralai/mistral-small-3.2-24b-instruct")
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"
# Default provider toggles (sidebar deprecated; per-test controls set overrides)
use_ollama = False
use_openai = True
use_ollama_local = False


# --- NEW: API Routing Configuration ---
# Set to "openrouter" to route all calls through OpenRouter (unified pricing, simpler)
# Set to "native" to use native APIs (OpenAI SDK, Google Genai SDK) for advanced features
API_ROUTING_MODE = st.secrets.get("API_ROUTING_MODE", "openrouter").lower()  # "openrouter" or "native"

# --- PATCH 22/23: OpenRouter Model Fetching with Metadata ---
OPENROUTER_MODELS_URL = "https://openrouter.ai/api/v1/models"
# A default dictionary in case the API call fails or key is missing
DEFAULT_OPENROUTER_MODEL_METADATA = {
    "mistralai/mistral-small-3.2-24b-instruct": {'context': '131,072', 'input_cost': '$0.06', 'output_cost': '$0.18'},
    "deepseek/deepseek-v3.1-terminus": {'context': '128,000', 'input_cost': '$0.14', 'output_cost': '$0.28'},
}


# --- PATCH 25: Ollama Local Model Definition ---
OLLAMA_MODEL_METADATA = {
    "mistral:latest": {'context': '4,096', 'local_info': 'General Purpose'},
    "llama3:8b": {'context': '8,192', 'local_info': 'Next-Gen Llama'},
    "mistral-small:24b-instruct-2501-q4_K_M": {'context': '32,768', 'local_info': 'Mistral Small Quantized'},
    "Custom...": {'context': 'N/A', 'local_info': 'User Defined'}
}
# -----------------------------------------------

# --- Initialize cost tracker ---
if "cost_tracker" not in st.session_state:
    st.session_state.cost_tracker = CostTracker()
    register_all_extractors()

# --- PATCH 30: Dynamic OpenRouter Pricing Cache (TTL 30 days) ---
# Pricing functions moved to core/pricing.py

# Import pricing cache for backward compatibility
from core.pricing import fetch_openrouter_pricing

OPENROUTER_PRICING_CACHE = fetch_openrouter_pricing()
GEMINI_PRICING_CACHE = OPENROUTER_PRICING_CACHE  # Gemini pricing is in the same cache
OPENAI_PRICING_CACHE = OPENROUTER_PRICING_CACHE  # OpenAI pricing is in the same cache

# Skip to Pydantic models (pricing functions removed)
# All pricing functions (_load_pricing_from_disk, _save_pricing_to_disk, fetch_openrouter_pricing,
# _to_openrouter_model_id, _to_native_model_id, _get_provider_from_model_id,
# _fetch_models_from_openrouter, fetch_gemini_models_from_linkup, _get_default_gemini_models,
# _parse_gemini_models_from_linkup, custom_gemini_price_lookup, fetch_openai_models_from_linkup,
# _get_default_openai_models, _parse_openai_models_from_linkup, get_all_available_models,
# custom_openrouter_price_lookup, _normalize_ollama_root) moved to core/pricing.py and utils/model_discovery.py

# ---------- Pydantic structured outputs ----------
# Pydantic models moved to core/models.py
# Pricing functions removed (lines 478-1086) - see core/pricing.py and utils/model_discovery.py

# ---------- Pydantic structured outputs ----------
# Pydantic models moved to core/models.py
# (Removed ~600 lines of pricing functions - see core/pricing.py)

# All pricing and model discovery functions removed (~550 lines)
# See core/pricing.py and utils/model_discovery.py for:
# - _to_openrouter_model_id, _to_native_model_id, _get_provider_from_model_id
# - _fetch_models_from_openrouter, fetch_gemini_models_from_linkup
# - _get_default_gemini_models, _parse_gemini_models_from_linkup
# - custom_gemini_price_lookup, fetch_openai_models_from_linkup
# - _get_default_openai_models, _parse_openai_models_from_linkup
# - get_all_available_models, custom_openrouter_price_lookup
# - fetch_openrouter_models_for_ui, fetch_openai_models
# - get_third_model_display_name, _normalize_ollama_root

# ---------- Pydantic structured outputs ----------
# Pydantic models moved to core/models.py
# (Removed ~515 lines of pricing/model discovery functions)

# ---------- Pydantic structured outputs ----------
# Pydantic models moved to core/models.py
# All Pydantic model definitions removed - see core/models.py

    if gemini_models:
        return gemini_models

    # Hard fallback if OpenRouter fetch fails (using actual OpenRouter pricing)
    return {
        "google/gemini-2.5-flash": {
            'context': '1,048,576',
            'input_cost': '$0.300',
            'output_cost': '$2.500',
            'input_per_mtok_usd': 0.300,
            'output_per_mtok_usd': 2.500
        },
        "google/gemini-2.5-flash-lite": {
            'context': '1,048,576',
            'input_cost': '$0.100',
            'output_cost': '$0.400',
            'input_per_mtok_usd': 0.100,
            'output_per_mtok_usd': 0.400
        },
        "google/gemini-2.5-pro": {
            'context': '1,048,576',
            'input_cost': '$1.250',
            'output_cost': '$10.000',
            'input_per_mtok_usd': 1.250,
            'output_per_mtok_usd': 10.000
        },
    }

def _parse_gemini_models_from_linkup(data: dict) -> Dict[str, Dict[str, Any]]:
    """
    Parse Gemini model information from Linkup API response.
    Extracts model names, context windows, and pricing.
    """
    import re

    models = {}

    # Get text content from response
    text_content = []
    answer = data.get("answer", "")
    if answer:
        text_content.append(str(answer))

    sources = data.get("sources", [])
    for source in sources:
        content = source.get("content", "") or source.get("snippet", "")
        if content:
            text_content.append(str(content))

    full_text = "\n\n".join(text_content)

    # Regex patterns to find Gemini models
    # Looking for patterns like: gemini-2.5-flash, gemini-1.5-pro, etc.
    model_pattern = re.compile(r'gemini-[\d\.]+-(?:flash|pro|ultra)?', re.IGNORECASE)
    model_names = set(model_pattern.findall(full_text.lower()))

    # Context window patterns
    context_pattern = re.compile(r'(\d+(?:,\d+)*)\s*(?:tokens?|token\s+context|context\s+window)', re.IGNORECASE)

    # Pricing patterns
    price_pattern = re.compile(r'\$?([\d.]+)\s*(?:per|/)\s*(?:million|1M|M)\s*(?:tokens?|input|output)', re.IGNORECASE)

    for model_name in model_names:
        # Try to find context and pricing for this specific model
        # Look for text near the model name
        model_idx = full_text.lower().find(model_name)
        if model_idx != -1:
            # Extract window around model mention
            window_start = max(0, model_idx - 300)
            window_end = min(len(full_text), model_idx + 300)
            window_text = full_text[window_start:window_end]

            # Extract context
            context_match = context_pattern.search(window_text)
            context = context_match.group(1).replace(',', '') if context_match else 'N/A'

            # Extract pricing (simplified - may need refinement)
            prices = price_pattern.findall(window_text)
            input_cost = float(prices[0]) if len(prices) > 0 else 0.0
            output_cost = float(prices[1]) if len(prices) > 1 else 0.0

            # Determine if free (experimental models)
            is_free = 'exp' in model_name or 'free' in model_name or 'free' in window_text.lower()

            if is_free:
                input_cost = 0.0
                output_cost = 0.0

            models[model_name] = {
                'context': f"{int(context):,}" if context != 'N/A' else context,
                'input_cost': 'Free' if input_cost == 0 else f"${input_cost:.3f}",
                'output_cost': 'Free' if output_cost == 0 else f"${output_cost:.2f}",
                'input_per_mtok_usd': input_cost,
                'output_per_mtok_usd': output_cost
            }

    return models if models else _get_default_gemini_models()

# Fetch Gemini 2.5 models from OpenRouter (via default function)
GEMINI_MODELS_FULL = _get_default_gemini_models()

# Extract pricing cache
DEFAULT_GEMINI_PRICING = {
    model_id: {
        "input_per_mtok_usd": meta.get('input_per_mtok_usd', 0.0),
        "output_per_mtok_usd": meta.get('output_per_mtok_usd', 0.0)
    }
    for model_id, meta in GEMINI_MODELS_FULL.items()
}
# -------------------------------------------------------------

# Use the discovered models for pricing cache
GEMINI_PRICING_CACHE = DEFAULT_GEMINI_PRICING.copy()

# Use the discovered models for UI metadata
GEMINI_MODEL_METADATA = {
    model_id: {
        'context': meta.get('context', 'N/A'),
        'input_cost': meta.get('input_cost', 'N/A'),
        'output_cost': meta.get('output_cost', 'N/A')
    }
    for model_id, meta in GEMINI_MODELS_FULL.items()
}

def custom_gemini_price_lookup(provider: str, model: str) -> Optional[Dict[str, float]]:
    """Custom price lookup for Gemini models using 30-day cached pricing."""
    if provider == "Google":
        # Use the 30-day cached pricing data
        return GEMINI_PRICING_CACHE.get(model)
    return None
# ---------------------------------------------------------------

# --- PATCH 31: Dynamic OpenAI Model Discovery via Linkup API ---
@st.cache_data(ttl=60 * 60 * 24 * 30)  # Cache for 30 days
def fetch_openai_models_from_linkup() -> Dict[str, Dict[str, Any]]:
    """
    Fetch latest OpenAI models using Linkup API to search OpenAI's documentation.
    Returns dict mapping model_id to metadata (context, pricing, etc.)
    """
    try:
        import requests
        linkup_api_key = st.secrets.get("LINKUP_API_KEY", "")

        if not linkup_api_key:
            st.warning("⚠️ LINKUP_API_KEY not set. Using default OpenAI models.")
            return _get_default_openai_models()

        # Search for latest OpenAI models
        headers = {
            "Authorization": f"Bearer {linkup_api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "q": "OpenAI API models 2025 GPT-4o GPT-4 GPT-3.5 available models pricing context window tokens",
            "depth": "standard",
            "outputType": "sourcedAnswer",
            "includeImages": False,
            "includeDomains": ["openai.com", "platform.openai.com"],
            "includeInlineCitations": False,
            "includeSources": True
        }

        response = requests.post("https://api.linkup.so/v1/search", headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Extract model information from response
        models = _parse_openai_models_from_linkup(data)

        if models:
            st.success(f"✅ Discovered {len(models)} OpenAI models via Linkup API (cached for 30 days).")
            return models
        else:
            st.info("ℹ️ No models found via Linkup. Using defaults.")
            return _get_default_openai_models()

    except Exception as e:
        st.warning(f"⚠️ Linkup model discovery failed: {e}. Using defaults.")
        return _get_default_openai_models()

def _get_default_openai_models() -> Dict[str, Dict[str, Any]]:
    """Fallback default GPT-5 models - fetches live pricing from OpenRouter."""
    # Fetch live pricing from OpenRouter
    openai_models = _fetch_models_from_openrouter(provider_filter="openai")

    if openai_models:
        return openai_models

    # Hard fallback if OpenRouter fetch fails (using actual OpenRouter pricing)
    return {
        "openai/gpt-5": {
            'context': '400,000',
            'input_cost': '$1.250',
            'output_cost': '$10.000',
            'input_per_mtok_usd': 1.250,
            'output_per_mtok_usd': 10.000
        },
        "openai/gpt-5-mini": {
            'context': '400,000',
            'input_cost': '$0.250',
            'output_cost': '$2.000',
            'input_per_mtok_usd': 0.250,
            'output_per_mtok_usd': 2.000
        },
        "openai/gpt-5-nano": {
            'context': '400,000',
            'input_cost': '$0.050',
            'output_cost': '$0.400',
            'input_per_mtok_usd': 0.050,
            'output_per_mtok_usd': 0.400
        },
    }

def _parse_openai_models_from_linkup(data: dict) -> Dict[str, Dict[str, Any]]:
    """
    Parse OpenAI model information from Linkup API response.
    Extracts model names, context windows, and pricing.
    """
    import re

    models = {}

    # Get text content from response
    text_content = []
    answer = data.get("answer", "")
    if answer:
        text_content.append(str(answer))

    sources = data.get("sources", [])
    for source in sources:
        content = source.get("content", "") or source.get("snippet", "")
        if content:
            text_content.append(str(content))

    full_text = "\n\n".join(text_content)

    # Regex patterns to find OpenAI models
    # Looking for patterns like: gpt-4o, gpt-4-turbo, gpt-3.5-turbo, etc.
    model_pattern = re.compile(r'gpt-(?:4o|4|3\.5)(?:-turbo|-mini)?', re.IGNORECASE)
    model_names = set(model_pattern.findall(full_text.lower()))

    # Context window patterns
    context_pattern = re.compile(r'(\d+(?:,\d+)*)\s*(?:tokens?|token\s+context|context\s+window)', re.IGNORECASE)

    # Pricing patterns
    price_pattern = re.compile(r'\$?([\d.]+)\s*(?:per|/)\s*(?:million|1M|M)\s*(?:tokens?|input|output)', re.IGNORECASE)

    for model_name in model_names:
        # Try to find context and pricing for this specific model
        model_idx = full_text.lower().find(model_name)
        if model_idx != -1:
            # Extract window around model mention
            window_start = max(0, model_idx - 300)
            window_end = min(len(full_text), model_idx + 300)
            window_text = full_text[window_start:window_end]

            # Extract context
            context_match = context_pattern.search(window_text)
            context = context_match.group(1).replace(',', '') if context_match else 'N/A'

            # Extract pricing
            prices = price_pattern.findall(window_text)
            input_cost = float(prices[0]) if len(prices) > 0 else 0.0
            output_cost = float(prices[1]) if len(prices) > 1 else 0.0

            models[model_name] = {
                'context': f"{int(context):,}" if context != 'N/A' else context,
                'input_cost': f"${input_cost:.2f}" if input_cost > 0 else 'N/A',
                'output_cost': f"${output_cost:.2f}" if output_cost > 0 else 'N/A',
                'input_per_mtok_usd': input_cost,
                'output_per_mtok_usd': output_cost
            }

    return models if models else _get_default_openai_models()

# Fetch GPT-5 models from OpenRouter (via default function)
OPENAI_MODELS_FULL = _get_default_openai_models()

# Extract pricing cache
DEFAULT_OPENAI_PRICING = {
    model_id: {
        "input_per_mtok_usd": meta.get('input_per_mtok_usd', 0.0),
        "output_per_mtok_usd": meta.get('output_per_mtok_usd', 0.0)
    }
    for model_id, meta in OPENAI_MODELS_FULL.items()
}
# -------------------------------------------------------------

# Use the discovered models for pricing cache
OPENAI_PRICING_CACHE = DEFAULT_OPENAI_PRICING.copy()

# Use the discovered models for UI metadata
OPENAI_MODEL_METADATA = {
    model_id: {
        'context': meta.get('context', 'N/A'),
        'input_cost': meta.get('input_cost', 'N/A'),
        'output_cost': meta.get('output_cost', 'N/A')
    }
    for model_id, meta in OPENAI_MODELS_FULL.items()
}
# ---------------------------------------------------------------

# --- NEW: Get all available models for UI selection ---
def get_all_available_models() -> List[str]:
    """
    Get all available models from Gemini, OpenAI, and OpenRouter.
    Returns a list of model IDs in OpenRouter format.
    """
    all_models = []

    # Add Gemini models
    gemini_models = list(GEMINI_MODELS_FULL.keys())
    all_models.extend(gemini_models)

    # Add OpenAI models
    openai_models = list(OPENAI_MODELS_FULL.keys())
    all_models.extend(openai_models)

    # Add OpenRouter model (if different from above)
    if OPENROUTER_MODEL not in all_models:
        all_models.append(OPENROUTER_MODEL)

    # Add some common OpenRouter models
    common_openrouter = [
        "mistralai/mistral-small-3.2-24b-instruct",
        "anthropic/claude-3.5-sonnet",
        "meta-llama/llama-3.1-70b-instruct",
        "deepseek/deepseek-v3.1-terminus",
    ]

    for model in common_openrouter:
        if model not in all_models:
            all_models.append(model)

    return sorted(all_models)

# Get available models
AVAILABLE_MODELS = get_all_available_models()

# --- PATCH 30: Unified Custom Price Lookup Function ---
def custom_openrouter_price_lookup(provider: str, model: str) -> Optional[Dict[str, float]]:
    """Unified custom price lookup that uses 30-day cached pricing for all providers."""
    if provider == "OpenRouter":
        # Use the 30-day cached OpenRouter pricing
        return OPENROUTER_PRICING_CACHE.get(model)
    elif provider == "Google":
        # Use the 30-day cached Gemini pricing
        return custom_gemini_price_lookup(provider, model)
    elif provider == "OpenAI":
        # Use the 30-day cached OpenAI pricing
        return OPENAI_PRICING_CACHE.get(model)
    # Fallback to combined lookup for other providers
    return combined_price_lookup(provider, model)
# -------------------------------------------------------------------


# Helper to normalize Ollama base URL to server root
def _normalize_ollama_root(url: str) -> str:
    u = (url or "").rstrip("/")
    if u.endswith("/api/generate"):
        u = u[: -len("/api/generate")]
    return u or "http://localhost:11434"

# ---------- Pydantic structured outputs ----------
class Classification(BaseModel):
    classification_result: str
    rationale: str

class ClassificationWithConf(Classification):
    confidence: Optional[float] = Field(default=None, ge=0.0, le=1.0)

# --- PATCH 2: NEW SCHEMAS FOR DATA GENERATION AND SUMMARY ---
class SyntheticDataItem(BaseModel):
    query: str = Field(description="The user query or input text.")
    classification: str = Field(description="The expected classification label.") # Used for Classification data type

class ToolCallSequenceItem(BaseModel):
    query: str = Field(description="The user query requiring a specific tool sequence.")
    # We use classification for compatibility with the final DF structure, but its meaning changes
    expected_sequence: List[str] = Field(description="The sequential list of tool names or actions expected.")

# --- PATCH 15: Pruning Data Schema Definition ---
class PruningDataItem(BaseModel):
    instruction: str = Field(description="System instruction/persona given to the LLM.")
    summary: str = Field(description="Summary of the conversation history so far.")
    user_msgs: str = Field(description="Pipe-separated (||) list of previous user messages.")
    agent_resps: str = Field(description="Pipe-separated (||) list of previous agent responses.")
    tool_logs: str = Field(description="Pipe-separated (||) list of previous tool call logs/results.")
    new_question: str = Field(description="The latest user query.")
    expected_action: str = Field(description="The ground truth next action (general_answer, kb_lookup, tool_call).")
    expected_kept_keys: str = Field(description="Comma-separated list of context keys expected to be kept (e.g., instruction, summary).")
# -----------------------------------------------------------

class TestSummaryAndRefinement(BaseModel):
    findings_summary: str = Field(description="A concise summary of the test results and model performance.")
    key_suggestions: List[str] = Field(description="Actionable suggestions for prompt, architecture, or tool improvement.")
    suggested_improvement_code: Optional[str] = Field(default=None, description="The refined Python code, agent framework, or prompt string based on suggestions, ready for the next test iteration.")
    suggested_improvement_prompt_reasoning: Optional[str] = Field(default=None, description="Detailed reasoning for the suggested code or prompt improvement.")

# --- PATCH 31: Pydantic Schemas for Self-Correcting Research Pipeline ---
class FactualConstraint(BaseModel):
    """Schema for individual policy constraints extracted from validation failures."""
    constraint_id: str = Field(description="Unique identifier for the constraint (e.g., 'NO_PHD_CLAIM')")
    constraint_text: str = Field(description="The explicit rule/fact to enforce")
    priority: float = Field(default=0.5, ge=0.0, le=1.0, description="Importance score (0.0-1.0)")
    severity: str = Field(default="MEDIUM", description="Severity level: CRITICAL, HIGH, MEDIUM, LOW")

class ValidationResultArtifact(BaseModel):
    """Schema for ValidatorAgent output with policy update information."""
    confidence_score: float = Field(ge=0.0, le=1.0, description="Overall validation confidence (0.0-1.0)")
    final_verdict: str = Field(description="Verification status: 'Verified', 'Hallucination Detected', or 'Inconclusive'")
    policy_updates: List[FactualConstraint] = Field(default_factory=list, description="New constraints to add to memory")
    report_summary: str = Field(description="Cleaned summary for ContentGenerator")
    red_flags: Optional[List[Dict[str, Any]]] = Field(default_factory=list, description="Detected hallucination risks")

def convert_validation_to_artifact(validation_result: Dict[str, Any]) -> ValidationResultArtifact:
    """
    Convert ValidatorAgent's validation_result to ValidationResultArtifact schema.

    This enables type-safe policy updates while maintaining backward compatibility
    with the existing validation_result format.
    """
    # Map verdict to final_verdict
    verdict = validation_result.get("verdict", "UNKNOWN")
    if verdict == "VERIFIED":
        final_verdict = "Verified"
    elif verdict == "FAILED":
        final_verdict = "Hallucination Detected"
    else:
        final_verdict = "Inconclusive"

    # Convert policy_suggestions to FactualConstraint objects
    policy_updates = []
    for idx, suggestion in enumerate(validation_result.get("policy_suggestions", [])):
        # Extract constraint ID from suggestion text (e.g., "CONSTRAINT: No PhD claims" -> "NO_PHD_CLAIMS")
        constraint_id = f"CONSTRAINT_{idx + 1}"
        if ":" in suggestion:
            # Try to extract meaningful ID from constraint text
            constraint_text = suggestion.split(":", 1)[1].strip()
            # Generate ID from first few words
            words = constraint_text.upper().replace(",", "").replace(".", "").split()[:3]
            constraint_id = "_".join(words)
        else:
            constraint_text = suggestion

        # Determine severity based on hallucination risk
        red_flags = validation_result.get("red_flags", [])
        max_risk = max([rf.get("hallucination_risk", 0.0) for rf in red_flags], default=0.0)

        if max_risk >= 0.8:
            severity = "CRITICAL"
            priority = 0.9
        elif max_risk >= 0.6:
            severity = "HIGH"
            priority = 0.7
        elif max_risk >= 0.4:
            severity = "MEDIUM"
            priority = 0.5
        else:
            severity = "LOW"
            priority = 0.3

        policy_updates.append(FactualConstraint(
            constraint_id=constraint_id,
            constraint_text=constraint_text,
            priority=priority,
            severity=severity
        ))

    return ValidationResultArtifact(
        confidence_score=validation_result.get("confidence_score", 0.5),
        final_verdict=final_verdict,
        policy_updates=policy_updates,
        report_summary=validation_result.get("summary", "No summary available"),
        red_flags=validation_result.get("red_flags", [])
    )
# -----------------------------------------------------------

@st.cache_data(ttl=3600)  # Cache for 1 hour (UI list refreshes more frequently than pricing)
def fetch_openrouter_models_for_ui() -> Dict[str, Dict[str, Any]]:
    """Fetches models from OpenRouter that support structured outputs, with metadata for UI display."""
    if not OPENROUTER_API_KEY:
        st.warning("OPENROUTER_API_KEY not set. Using default model list.")
        return DEFAULT_OPENROUTER_MODEL_METADATA

    params = {
        "supported_parameters": "structured_outputs",
        "order": "newest",
    }
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }

    try:
        with httpx.Client(timeout=10) as client:
            response = client.get(OPENROUTER_MODELS_URL, headers=headers, params=params)
            response.raise_for_status()
            data = response.json()

            model_metadata = {}
            for m in data.get('data', []):
                model_id = m['id']

                # Extract context length
                context = m.get('context_length', 'N/A')

                # PATCH 30: Use the 30-day cached pricing data if available
                cached_pricing = OPENROUTER_PRICING_CACHE.get(model_id, {})
                if cached_pricing:
                    # Use cached pricing (already in per-million-tokens format)
                    input_cost = cached_pricing.get('input_per_mtok_usd', 0.0)
                    output_cost = cached_pricing.get('output_per_mtok_usd', 0.0)
                else:
                    # Fallback to API response pricing
                    pricing = m.get('pricing', {})
                    input_cost_raw = pricing.get('prompt', 0.0)
                    output_cost_raw = pricing.get('completion', 0.0)

                    try:
                        input_cost = float(input_cost_raw) * 1_000_000 if input_cost_raw else 0.0
                        output_cost = float(output_cost_raw) * 1_000_000 if output_cost_raw else 0.0
                    except (ValueError, TypeError):
                        input_cost = 0.0
                        output_cost = 0.0

                # Format context - handle both int and string types
                if isinstance(context, int):
                    context_str = f"{context:,}"
                elif isinstance(context, str):
                    context_str = context
                else:
                    context_str = str(context)

                # Format costs - ensure they are numeric before comparison
                try:
                    input_cost_num = float(input_cost) if not isinstance(input_cost, (int, float)) else input_cost
                    output_cost_num = float(output_cost) if not isinstance(output_cost, (int, float)) else output_cost
                except (ValueError, TypeError):
                    input_cost_num = 0.0
                    output_cost_num = 0.0

                model_metadata[model_id] = {
                    'context': context_str,
                    'input_cost': f"${input_cost_num:.2f}" if input_cost_num > 0 else "Free",
                    'output_cost': f"${output_cost_num:.2f}" if output_cost_num > 0 else "Free",
                }

            # Limit to 50 for a manageable dropdown
            limited_model_metadata = {k: v for k, v in list(model_metadata.items())[:50]}

            return limited_model_metadata

    except Exception as e:
        st.error(f"Failed to fetch models from OpenRouter: {e}. Using default list.")
        return DEFAULT_OPENROUTER_MODEL_METADATA

# Call the function once to populate the list used in the sidebar
OPENROUTER_MODEL_METADATA = fetch_openrouter_models_for_ui()
# -------------------------------------------------------------

# --- PATCH 31: Use Linkup-discovered OpenAI models ---
# Use the models discovered via Linkup API
@st.cache_data(ttl=3600)  # Cache for 1 hour (UI refresh)
def fetch_openai_models() -> Dict[str, Dict[str, Any]]:
    """Returns OpenAI models discovered via Linkup API (cached for 30 days)."""
    # Simply return the Linkup-discovered models
    return OPENAI_MODEL_METADATA.copy()

# Populate the OpenAI model metadata from Linkup-discovered models
_openai_model_ids = list(OPENAI_MODEL_METADATA.keys())
# -------------------------------------------------------------

SKELETON_COLUMNS = [
    "query", "classification",
    # openrouter mistral
    "classification_result_openrouter_mistral", "classification_result_openrouter_mistral_rationale", "classification_result_openrouter_mistral_confidence", "latency_openrouter_mistral",
    # openai (gpt-5-mini)
    "classification_result_openai", "classification_result_openai_rationale", "classification_result_openai_confidence", "latency_openai",
    # gemini
    "classification_result_gemini", "classification_result_gemini_rationale", "classification_result_gemini_confidence", "latency_gemini",
    # third model (optional)
    "classification_result_third", "classification_result_third_rationale", "classification_result_third_confidence", "latency_third",
    # ensemble / judge
    "weighted_pick_model", "weighted_pick_label",
    "judge_choice_model", "judge_choice_label", "judge_rationale",
]

# --- PATCH 19: Suggested Prompts for Data Generation Tab ---
SUGGESTED_PROMPTS = {
    "Classification": [
        "A classification dataset for identifying sentiment in social media posts, with labels: positive, negative, neutral.",
        "Generate queries asking for technical support for a banking application, categorized by: login_issue, transaction_error, account_settings, general_help.",
        "Create short news headlines classified into five topics: sports, finance, politics, tech, weather."
    ],
    "Tool/Agent Sequence": [
        "Queries that require a sequence of internal tools like: crm_lookup -> schedule_callback, or check_inventory -> order_part.",
        "Generate customer support questions requiring tool sequences for flight booking: check_availability -> select_seat -> confirm_payment.",
        "Queries needing a single tool call followed by a database query: user_auth -> internal_db_read."
    ],
    "Context Pruning": [
        "Conversational turns where the new question only requires the `tool_logs` and `new_question` to take the action `kb_lookup` (summary is irrelevant).",
        "Scenarios where the `instruction` and `summary` are critical to deciding the `tool_call` action.",
        "Generate examples where the `new_question` is a simple follow-up, and the action should be `general_answer`, keeping only `summary`."
    ]
}

# --- NEW: Default Dataset Generation Prompts ---
DEFAULT_DATASET_PROMPTS = {
    "Classification": """Generate a diverse classification dataset for an IT support chatbot with three categories:
1. 'general_chat' - casual greetings, small talk, non-technical questions
2. 'kb_lookup' - questions about IT concepts, tools, best practices, knowledge base queries
3. 'tool' - requests that require calling specific tools or APIs (e.g., database queries, system lookups, device information)

Create varied, realistic user queries that cover common IT support scenarios. Include questions about network devices, incidents, IP addresses, locations, and general IT knowledge.""",

    "Tool/Agent Sequence": """Generate queries that require multi-step tool sequences for an IT operations agent:
- Single tool calls: check_device, get_incident, lookup_ip
- Two-step sequences: authenticate_user -> query_database, check_inventory -> create_order
- Three-step sequences: validate_request -> fetch_data -> update_system

Include realistic IT operations scenarios like device management, incident tracking, and network operations.""",

    "Context Pruning": """Generate context pruning test cases for a conversational AI agent with three action types:
1. 'general_answer' - Simple questions that only need conversation summary
2. 'kb_lookup' - Knowledge base queries that need instruction and tool logs
3. 'tool_call' - Actions requiring full context (instruction, summary, tool logs)

Create realistic multi-turn conversations with varying context requirements. Include banking, IT support, and customer service scenarios."""
}
# -------------------------------------------------------------

# Rate limit (max concurrent calls)
CONCURRENCY_LIMIT = int(st.secrets.get("CONCURRENCY_LIMIT", "1000"))
_rate_limiter = asyncio.Semaphore(CONCURRENCY_LIMIT)


# ---------- Row-limit helpers ----------
from typing import Optional as _OptInt
ROW_LIMIT_OPTIONS = {"First 5": 5, "First 25": 25, "First 100": 100, "All": None}

# --- PATCH 13: Helper for Dynamic Third Model Name ---
def get_third_model_display_name() -> str:
    """Dynamically returns the name of the configured third model."""
    if THIRD_KIND == "None" or not THIRD_MODEL:
        return "Third Model (N/A)"

    model_name = THIRD_MODEL
    # Try to shorten OpenRouter model names for display
    if THIRD_KIND == "OpenRouter":
        model_name = model_name.split('/')[-1]

    # Cap length for readability
    if len(model_name) > 30:
        model_name = model_name[:27] + "..."

    return f"{THIRD_KIND} ({model_name})"
# ---------------------------------------------------

def _subset_for_run(df: pd.DataFrame, n: _OptInt[int]) -> pd.DataFrame:
    try:
        if n is None or not len(df) or (isinstance(n, int) and n >= len(df)):
            return df
        if isinstance(n, int) and n > 0:
            return df.head(n)
        return df
    except Exception:
        return df

def _style_selected_rows(df: pd.DataFrame, n: _OptInt[int]):
    try:
        # Truncate long rationale-like columns for display safety
        display_df = df.copy()
        rationale_cols = [c for c in display_df.columns if c.endswith("_rationale") or c == "judge_rationale"]
        for col in rationale_cols:
            try:
                display_df[col] = display_df[col].astype(str).str.slice(0, 150) + "..."
            except Exception:
                pass

        if n is None:
            selected = set(display_df.index)
        else:
            selected = set(display_df.index[: max(0, int(n))])
        def _hl(row):
            return ["background-color: #1E4594"] * len(row) if row.name in selected else [""] * len(row)
        return display_df.style.apply(lambda r: _hl(r), axis=1)
    except Exception:
        return df

# ---------- Helpers for saving and explaining ----------
def save_results_df(df: pd.DataFrame, test_name: str, row_limit: Optional[int], is_pruning_test: bool = False):
    """Saves the results DataFrame to the test_output directory."""
    try:
        output_dir = "test_output"
        os.makedirs(output_dir, exist_ok=True)
        timestamp = time.strftime("%Y%m%d-%H%M%S")

        if is_pruning_test:
            # For Test 4, the context is the length of the testset df
            limit_str = f"{len(df)}_rows"
            test_slug = "test_4_pruning"
        else:
            limit_str = f"{row_limit}_rows" if row_limit is not None else "all_rows"
            test_slug = test_name.lower().replace(" ", "_").replace(":", "").replace(",", "")

        filename = f"{output_dir}/{test_slug}_{limit_str}_{timestamp}.csv"
        df.to_csv(filename, index=False)
        st.success(f"Results saved to `{filename}`")
    except Exception as e:
        st.warning(f"Failed to save results: {e}")

# ---------- UI ----------
st.set_page_config(page_title="Classification + Eval (OpenRouter Mistral + OpenAI)", layout="wide")
st.title("🧩 Enhanced Classification + Evaluation Suite")
st.caption(f"Dataset directory: `{DATASET_DIR}` — Datasets auto-generated and persisted.")

# --- API Routing Configuration UI ---
with st.sidebar:
    st.header("⚙️ API Configuration")

    # Global API routing mode selector
    api_mode = st.radio(
        "API Routing Mode",
        options=["openrouter", "native"],
        index=0 if API_ROUTING_MODE == "openrouter" else 1,
        help="""
        **OpenRouter Mode**: Routes all API calls through OpenRouter
        - ✅ Unified pricing from one source
        - ✅ Consistent model IDs
        - ✅ Accurate cost tracking
        - ✅ Access to latest models

        **Native Mode**: Uses native provider APIs (OpenAI SDK, Google Genai SDK)
        - ✅ Access to advanced features (file analysis, vision, etc.)
        - ✅ Direct provider integration
        - ⚠️ Requires separate API keys for each provider
        """
    )

    # Update global routing mode
    globals()['API_ROUTING_MODE'] = api_mode

    st.divider()

    # Display current model configuration
    st.subheader("📋 Current Models")

    if api_mode == "openrouter":
        st.info("**Using OpenRouter for all calls**")
        st.code(f"OpenAI: {OPENAI_MODEL}\nGemini: {GEMINI_MODEL}\nOpenRouter: {OPENROUTER_MODEL}", language="text")
    else:
        st.info("**Using Native APIs**")
        st.code(f"OpenAI: {_to_native_model_id(OPENAI_MODEL)}\nGemini: {_to_native_model_id(GEMINI_MODEL)}", language="text")

    st.divider()

# ---------- NEW: Dataset Generation and Persistence Helpers ----------

def ensure_dataset_directory():
    """Ensure the test_dataset directory exists."""
    os.makedirs(DATASET_DIR, exist_ok=True)

def save_dataset_to_file(df: pd.DataFrame, dataset_type: str, model_used: Optional[str] = None, routing_mode: Optional[str] = None):
    """Save a dataset to the appropriate file based on type and write a .meta.json with model/routing."""
    ensure_dataset_directory()

    if dataset_type == "Classification":
        path = CLASSIFICATION_DATASET_PATH
    elif dataset_type == "Tool/Agent Sequence":
        path = TOOL_SEQUENCE_DATASET_PATH
    elif dataset_type == "Context Pruning":
        path = CONTEXT_PRUNING_DATASET_PATH
    else:
        st.warning(f"Unknown dataset type: {dataset_type}")
        return

    try:
        df.to_csv(path, index=False)
        # Write sidecar metadata for traceability
        meta_path = path.replace(".csv", ".meta.json")
        meta = {
            "model": model_used,
            "routing_mode": routing_mode or API_ROUTING_MODE,
            "when": pd.Timestamp.utcnow().isoformat(),
            "dataset_type": dataset_type,
        }
        # Attach cost tracker snapshot if available
        try:
            ct = st.session_state.cost_tracker
            meta["cost_tracker_totals"] = dict(ct.totals)
            meta["cost_tracker_calls"] = len(ct.by_call)
        except Exception:
            pass
        try:
            with open(meta_path, "w", encoding="utf-8") as f:
                json.dump(meta, f, indent=2)
        except Exception:
            pass

        if model_used:
            st.success(f"✅ Saved {dataset_type} dataset to `{path}` using {model_used} [{routing_mode or API_ROUTING_MODE}]")
        else:
            st.success(f"✅ Saved {dataset_type} dataset to `{path}`")
    except Exception as e:
        st.error(f"Failed to save {dataset_type} dataset: {e}")

def load_classification_dataset() -> pd.DataFrame:
    """Load classification dataset from file, or return empty DataFrame."""
    try:
        if os.path.exists(CLASSIFICATION_DATASET_PATH):
            df = pd.read_csv(CLASSIFICATION_DATASET_PATH)
            cols_map = {c.lower(): c for c in df.columns}
            q, c = cols_map.get("query"), cols_map.get("classification")
            if q and c:
                df2 = df[[q, c]].rename(columns={q: "query", c: "classification"})
            else:
                df2 = pd.DataFrame(columns=["query", "classification"])

            # Add skeleton columns
            for col in SKELETON_COLUMNS:
                if col not in df2.columns:
                    df2[col] = None

            # Backward compatibility
            backcompat_map = {
                "classification_result_ollama": "classification_result_openrouter_mistral",
                "classification_result_ollama_rationale": "classification_result_openrouter_mistral_rationale"
            }
            for old_col, new_col in backcompat_map.items():
                if old_col in df.columns and new_col in df2.columns:
                    try:
                        df2[new_col] = df[old_col]
                    except Exception:
                        pass

            return df2[SKELETON_COLUMNS]
        else:
            return pd.DataFrame(columns=SKELETON_COLUMNS)
    except Exception as e:
        st.warning(f"Could not load classification dataset: {e}")
        return pd.DataFrame(columns=SKELETON_COLUMNS)

def load_tool_sequence_dataset() -> pd.DataFrame:
    """Load tool/agent sequence dataset from file."""
    try:
        if os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
            return pd.read_csv(TOOL_SEQUENCE_DATASET_PATH)
        else:
            return pd.DataFrame(columns=["query", "expected_sequence"])
    except Exception as e:
        st.warning(f"Could not load tool sequence dataset: {e}")
        return pd.DataFrame(columns=["query", "expected_sequence"])

# ---------- Label Normalization Helpers ----------
_CANON_MAP = {
    # general
    "general": "general_answer", "general answer": "general_answer",
    "general_answer": "general_answer",

    # knowledge lookups
    "kb": "kb_lookup", "kb lookup": "kb_lookup", "kb_lookup": "kb_lookup",
    "knowledge": "kb_lookup", "knowledge_lookup": "kb_lookup",

    # tool calls
    "tool": "tool_call", "tool call": "tool_call", "tool_call": "tool_call", "toolcall": "tool_call",
}

def _normalize_label(s: Optional[str]) -> str:
    if not s:
        return ""
    s = str(s).strip().lower()
    s = re.sub(r"\s+", " ", s)
    s = s.replace("-", " ").replace("_", " ")
    canon = _CANON_MAP.get(s)
    if canon:
        return canon
    return s.replace(" ", "_")

def load_context_pruning_dataset() -> pd.DataFrame:
    """Load context pruning dataset from file."""
    try:
        if os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
            df = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH)
            # Optional hard fail on legacy labels
            VALID_ACTIONS = {"general_answer", "kb_lookup", "tool_call"}
            if "expected_action" in df.columns:
                invalid = set(df["expected_action"].dropna().map(_normalize_label)) - VALID_ACTIONS
                if invalid:
                    raise ValueError(f"Invalid action labels found: {sorted(invalid)}")
            return df
        else:
            required_cols = list(PruningDataItem.model_fields.keys())
            return pd.DataFrame(columns=required_cols)
    except Exception as e:
        st.warning(f"Could not load context pruning dataset: {e}")
        required_cols = list(PruningDataItem.model_fields.keys())
        return pd.DataFrame(columns=required_cols)

# ---------- Data loading ----------
def _load_df_from_path() -> pd.DataFrame:
    """Load classification dataset (backward compatibility wrapper)."""
    return load_classification_dataset()

# --- NEW: Auto-generate datasets on first run if they don't exist ---
async def auto_generate_default_datasets():
    """Generate default datasets if they don't exist in the test_dataset directory."""
    ensure_dataset_directory()

    datasets_to_generate = []

    # Check which datasets are missing
    if not os.path.exists(CLASSIFICATION_DATASET_PATH):
        datasets_to_generate.append(("Classification", CLASSIFICATION_DATASET_PATH, 100))
    if not os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
        datasets_to_generate.append(("Tool/Agent Sequence", TOOL_SEQUENCE_DATASET_PATH, 50))
    if not os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
        datasets_to_generate.append(("Context Pruning", CONTEXT_PRUNING_DATASET_PATH, 50))

    if not datasets_to_generate:
        return  # All datasets exist

    st.info(f"🔄 Generating {len(datasets_to_generate)} missing dataset(s)... This may take a moment.")

    # Use a simple, fast model for default generation
    default_gen_model = "openai/gpt-5-mini"

    for data_type, path, size in datasets_to_generate:
        try:
            with st.spinner(f"Generating {data_type} dataset ({size} items)..."):
                prompt = DEFAULT_DATASET_PROMPTS[data_type]
                df = await generate_synthetic_data(prompt, size, data_type, default_gen_model)

                if not df.empty:
                    df.to_csv(path, index=False)
                    st.success(f"✅ Generated and saved {data_type} dataset to `{path}`")
                else:
                    st.warning(f"⚠️ Failed to generate {data_type} dataset")
        except Exception as e:
            st.error(f"Error generating {data_type} dataset: {e}")

def check_and_generate_datasets():
    """Check if datasets exist, and generate them if needed (synchronous wrapper)."""
    ensure_dataset_directory()

    # Check if any datasets are missing
    missing = []
    if not os.path.exists(CLASSIFICATION_DATASET_PATH):
        missing.append("Classification")
    if not os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
        missing.append("Tool/Agent Sequence")
    if not os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
        missing.append("Context Pruning")

    if missing:
        st.warning(f"⚠️ Missing datasets: {', '.join(missing)}. Use the 'Preparation' tab to generate them.")

# Initialize session state
if "df" not in st.session_state:
    st.session_state.df = _load_df_from_path()

# --- Initialize Universal Execution Tracker ---
if 'execution_tracker' not in st.session_state:
    st.session_state['execution_tracker'] = ExecutionTracker()

# --- PATCH 16: Initialize Pruning DF in session state ---
if "agent_df" not in st.session_state:
    st.session_state.agent_df = load_tool_sequence_dataset()
if "pruning_df" not in st.session_state:
    st.session_state.pruning_df = load_context_pruning_dataset()
# --------------------------------------------------------

# Check for missing datasets on startup
if "datasets_checked" not in st.session_state:
    check_and_generate_datasets()
    st.session_state.datasets_checked = True

# ---------- Helpers ----------
def _allowed_labels(df: pd.DataFrame) -> List[str]:
    try:
        if "classification" in df.columns:
            return sorted({str(x) for x in df["classification"].dropna().unique().tolist()})
    except Exception:
        pass
    return []

async def _retry(op, attempts: int = 3, base_delay: float = 0.5):
    last = None
    for i in range(attempts):
        try:
            return await op()
        except Exception as e:
            last = e
            await asyncio.sleep(base_delay * (2 ** i))
    raise last

# ============================================================
# DASHBOARD VISUALIZATION HELPER FUNCTIONS
# ============================================================
# NOTE: Visualization functions moved to utils/visualizations.py and utils/gantt_charts.py
# Imported above as: render_test_flow_diagram, render_kpi_metrics, etc.

# render_kpi_metrics moved to utils/visualizations.py
# render_cost_dashboard moved to utils/visualizations.py

# visualize_dataset_composition moved to utils/visualizations.py

def generate_gantt_data(test_name: str, tracker: ExecutionTracker) -> pd.DataFrame:
    """Generate Gantt chart data from events."""

    events = tracker.get_test_events(test_name)

    if not events:
        return pd.DataFrame()

    # Group events by agent_id
    agent_timeline = {}

    for event in events:
        if event.agent_id not in agent_timeline:
            agent_timeline[event.agent_id] = {
                'name': event.agent_name,
                'type': event.agent_type,
                'parent': event.parent_id,
                'start': event.timestamp,
                'end': event.timestamp,
                'status': event.status,
                'progress': event.progress,
                'metadata': event.metadata.copy()
            }
        else:
            # Update end time and status
            agent_timeline[event.agent_id]['end'] = max(
                agent_timeline[event.agent_id]['end'],
                event.timestamp
            )
            agent_timeline[event.agent_id]['status'] = event.status
            agent_timeline[event.agent_id]['progress'] = max(
                agent_timeline[event.agent_id]['progress'],
                event.progress
            )
            # Merge metadata
            agent_timeline[event.agent_id]['metadata'].update(event.metadata)

    # Build DataFrame
    rows = []
    for agent_id, data in agent_timeline.items():
        duration = data['end'] - data['start']

        # Indent based on hierarchy
        indent = "  " if data['parent'] else ""

        rows.append({
            'Task': f"{indent}{data['name']}",
            'Start': data['start'],
            'Duration': duration,
            'Status': data['status'],
            'Progress': data['progress'],
            'Type': data['type'],
            'Metadata': data['metadata'],
            'Code': data['metadata'].get('code', '')  # For hover display
        })

    return pd.DataFrame(rows)


def capture_run_config(test_name: str, overrides: Optional[Dict[str, Any]] = None):
    """Snapshots the current run's configuration into session state.
    If overrides are provided (per-test settings), they take precedence.
    """
    if "last_run_configs" not in st.session_state:
        st.session_state.last_run_configs = {}

    config: Dict[str, Any] = {}
    allowed_labels = _allowed_labels(st.session_state.df)

    # Pull effective settings from overrides if provided
    use_openai_eff = overrides.get('use_openai') if overrides else use_openai
    use_ollama_eff = overrides.get('use_ollama') if overrides else use_ollama
    use_ollama_local_eff = overrides.get('use_ollama_local') if overrides else use_ollama_local
    openai_model_eff = overrides.get('openai_model') if overrides else OPENAI_MODEL
    openrouter_model_eff = overrides.get('openrouter_model') if overrides else OPENROUTER_MODEL
    ollama_model_eff = overrides.get('ollama_model') if overrides else OLLAMA_MODEL
    third_kind_eff = overrides.get('third_kind') if overrides else THIRD_KIND
    third_model_eff = overrides.get('third_model') if overrides else THIRD_MODEL

    # --- Classification Models (Tests 1-3) ---
    if test_name in ["Test 1", "Test 2", "Test 3"]:
        if use_openai_eff:
            config['OpenAI'] = {
                "model": openai_model_eff,
                "system_prompt": "Return a structured classification with optional confidence 0..1.",
                "user_prompt_template": f"Allowed labels: {allowed_labels if allowed_labels else '[unconstrained]'}.\nPick exactly ONE of these values in 'classification_result'.\nText: {{text}}\nRespond as JSON with keys: classification_result, rationale, confidence (0..1 optional)."
            }
        if use_ollama_eff:  # OpenRouter path
            config['OpenRouter'] = {
                "model": openrouter_model_eff,
                "provider": "OpenRouter",
                "system_prompt": f"Return ONLY compact JSON with keys: classification_result, rationale, confidence (0..1 optional). Allowed labels: {allowed_labels}.",
                "user_prompt_template": "{text}"
            }
        if use_ollama_local_eff:
            config['Ollama (Local)'] = {
                "model": ollama_model_eff,
                "provider": "Local Ollama",
                "system_prompt": "You only reply with JSON that matches the provided schema.",
                "user_prompt_template": f"Allowed labels: {allowed_labels if allowed_labels else '[unconstrained]'}\nText: {{text}}"
            }
        if test_name in ["Test 2", "Test 3"] and third_kind_eff != "None":
            config[f'Third Model ({third_kind_eff})'] = {
                "model": third_model_eff,
                "provider": third_kind_eff,
                "system_prompt": "Respond ONLY with JSON having keys classification_result, rationale, confidence (0..1 optional).",
                "user_prompt_template": "{text}"
            }

    # --- Judge Model (Test 3) ---
    if test_name == "Test 3":
        judge_model = st.session_state.get('judge_model', 'openai/gpt-5-mini')
        config['Judge'] = {
            "model": judge_model,
            "provider": _get_provider_from_model_id(judge_model),
            "instructions": JUDGE_INSTRUCTIONS
        }

    # --- Pruner Model (Test 4) ---
    if test_name == "Test 4":
        pruner_model = st.session_state.get('pruner_model', 'openai/gpt-5-mini')
        config['Pruner'] = {
            "model": pruner_model,
            "provider": _get_provider_from_model_id(pruner_model),
            "instructions": PRUNER_INSTRUCTIONS
        }

    st.session_state.last_run_configs[test_name] = config

def display_run_config(test_name: str):
    """Displays the captured configuration in a Streamlit expander."""
    if "last_run_configs" in st.session_state and test_name in st.session_state.last_run_configs:
        with st.expander("View Configuration for Last Run"):
            config_data = st.session_state.last_run_configs[test_name]
            if not config_data:
                st.write("No models were configured for this run.")
                return

            for provider, details in config_data.items():
                st.subheader(f"⚙️ {provider} Configuration")
                st.text(f"Model: {details['model']}")
                if "provider" in details:
                    st.text(f"Provider: {details['provider']}")

                if "instructions" in details:
                    st.text("Prompt / Instructions:")
                    st.code(details['instructions'], language='markdown')
                else:
                    if "system_prompt" in details:
                        st.text("System Prompt:")
                        st.code(details['system_prompt'], language='text')
                    if "user_prompt_template" in details:
                        st.text("User Prompt Template (where `{text}` is the input):")
                        st.code(details['user_prompt_template'], language='text')
                st.divider()

def _non_empty(s):
    return s.fillna("").astype(str).str.strip().replace("nan","").replace("None","").ne("").sum()

# ---------- Rigorous Reporting with Scikit-learn and LLM Explanation ----------
def generate_classification_report(y_true, y_pred, model_name, explain: bool = False):
    y_true_norm = [_normalize_label(s) for s in y_true]
    y_pred_norm = [_normalize_label(s) for s in y_pred]

    valid_indices = [i for i, (t, p) in enumerate(zip(y_true_norm, y_pred_norm)) if t and p]
    if not valid_indices:
        st.warning(f"No valid predictions for {model_name} to generate a report.")
        return None

    y_true_filtered = [y_true_norm[i] for i in valid_indices]
    y_pred_filtered = [y_pred_norm[i] for i in valid_indices]

    report_dict = classification_report(y_true_filtered, y_pred_filtered, output_dict=True, zero_division=0)

    st.subheader(f"Classification Report: {model_name}")
    st.dataframe(pd.DataFrame(report_dict).transpose().style.format("{:.2f}"))

    st.subheader(f"Confusion Matrix: {model_name}")
    labels = sorted(list(set(y_true_filtered) | set(y_pred_filtered)))
    cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=labels)

    # Interactive Plotly heatmap
    fig_cm = go.Figure(data=go.Heatmap(
        z=cm,
        x=labels,
        y=labels,
        colorscale='Blues',
        text=cm,
        texttemplate='%{text}',
        textfont={"size": 12},
        hovertemplate='True: %{y}<br>Predicted: %{x}<br>Count: %{z}<extra></extra>',
        colorbar=dict(title="Count")
    ))

    fig_cm.update_layout(
        title=f"Confusion Matrix: {model_name}",
        xaxis_title="Predicted",
        yaxis_title="True",
        height=max(400, len(labels) * 50),
        width=max(500, len(labels) * 50),
        xaxis={'side': 'bottom'},
        yaxis={'autorange': 'reversed'}
    )

    st.plotly_chart(fig_cm, use_container_width=True, config=PLOTLY_CONFIG)

    # --- NEW: Per-Class Performance Radar ---
    st.subheader(f"Per-Class Performance Radar: {model_name}")

    # Extract per-class metrics
    class_metrics = []
    for label in labels:
        if label in report_dict:
            class_metrics.append({
                'Class': label,
                'Precision': report_dict[label]['precision'],
                'Recall': report_dict[label]['recall'],
                'F1-Score': report_dict[label]['f1-score']
            })

    if class_metrics:
        fig_radar = go.Figure()

        for metric in ['Precision', 'Recall', 'F1-Score']:
            fig_radar.add_trace(go.Scatterpolar(
                r=[m[metric] for m in class_metrics],
                theta=[m['Class'] for m in class_metrics],
                fill='toself',
                name=metric
            ))

        fig_radar.update_layout(
            polar=dict(
                radialaxis=dict(visible=True, range=[0, 1])
            ),
            showlegend=True,
            height=400
        )

        st.plotly_chart(fig_radar, use_container_width=True, config=PLOTLY_CONFIG)

    # --- NEW: LLM Explanation ---
    if explain:
        with st.spinner(f"Asking an LLM to explain the '{model_name}' confusion matrix..."):
            try:
                cm_df = pd.DataFrame(cm, index=labels, columns=labels)
                cm_string = cm_df.to_string()
                prompt = f"""
As a data science expert, analyze this confusion matrix for the '{model_name}' model.

**Confusion Matrix:**
```
{cm_string}
```

**Instructions:**
1.  **Overall Performance:** Briefly state how well the model is performing in general.
2.  **Strengths:** Identify which classes the model predicts most accurately (high values on the diagonal).
3.  **Weaknesses/Confusions:** Point out the most significant misclassifications (large off-diagonal values). For each, clearly state 'The model confused class A (True) with class B (Predicted) X times.'
4.  **Conclusion:** Provide a concise summary and a potential next step for improving the model based on these confusions.

Keep the explanation clear, concise, and easy to understand. Use markdown for formatting.
"""
                explanation = asyncio.run(generate_text_async(prompt))
                st.markdown(explanation)
            except Exception as e:
                st.warning(f"Could not generate LLM explanation for the confusion matrix: {e}")

    return report_dict

# --- PATCH 3: Gemini Code Execution Helper ---

async def run_gemini_code_execution(
    system_prompt: str,
    user_contents: List[Any],
    model: str = "gemini-2.5-flash",
    max_turns: int = 5
) -> Tuple[List[types.Part], str, float]:
    """
    Runs a conversation with Gemini 2.5 Flash, enabling code execution tool for self-refinement.
    Returns (Final Parts, Best Code, Best Performance).
    """

    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY not set.")

    # CRITICAL FIX: Create client without async context manager
    # genai.Client doesn't support async context managers
    client = genai.Client(api_key=GEMINI_API_KEY)

    # Configure tool usage: Code Execution ONLY
    config = types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    )

    # Initial message structure for the *first* call
    initial_content_parts = [types.Part.from_text(text=system_prompt)] + [types.Part.from_text(text=c) if isinstance(c, str) else c for c in user_contents]

    history = [types.Content(parts=initial_content_parts)] # History starts with the prompt

    best_performance = -1.0
    best_attempt_code = ""

    # Synchronous worker function to call the Gemini API
    def sync_api_call(current_history):
        return client.models.generate_content(
            model=model,
            contents=current_history,
            config=config,
        )

    # Run loop
    for turn in range(max_turns):
        st.info(f"Agent Turn {turn+1}/{max_turns}: Sending context to Gemini...")

        try:
            # Execute the synchronous API call in a separate thread
            response = await asyncio.to_thread(sync_api_call, history)

            # Track the API call
            st.session_state.cost_tracker.update(
                provider="Google", model=model, api="generate_content",
                raw_response_obj=response, pricing_resolver=combined_price_lookup
            )

            # Add model response (parts) to history
            if response.candidates and response.candidates[0].content:
                model_content = response.candidates[0].content
                history.append(model_content)

                # Analyze parts from the current turn
                current_code = ""
                current_accuracy = 0.0

                st.markdown(f"**Gemini Reasoning (Turn {turn+1}):**")

                for part in model_content.parts:
                    if part.text:
                        st.text(part.text)
                    if part.executable_code:
                        current_code = part.executable_code.code
                        st.code(current_code, language='python')
                    if part.code_execution_result and part.code_execution_result.output:
                        code_output = part.code_execution_result.output
                        st.text(f"Code Execution Output:\n{code_output}")

                        # Extract accuracy from output
                        match = re.search(r"Accuracy: (\d+\.\d+)", code_output)
                        if match:
                            current_accuracy = float(match.group(1))
                            st.success(f"Performance Score: {current_accuracy:.4f}")

                # Update best performance
                if current_accuracy > best_performance:
                    best_performance = current_accuracy
                    best_attempt_code = current_code
                    st.balloons()
                    st.markdown(f"**NEW MAX PERFORMANCE ({best_performance:.4f}) CAPTURED**")

                # Prepare for next turn: Only send a small prompt to continue the conversation
                if turn < max_turns - 1:
                    history.append(types.Content(parts=[types.Part.from_text(text="Based on the code execution result (if any), please refine the AgentFramework code and re-test to maximize accuracy. Provide the full executable Python code.")]))

                # Check for termination condition
                if response.candidates[0].finish_reason == types.FinishReason.STOP and current_accuracy > 0:
                    st.info("Agent stopped self-refinement early.")
                    break

        except Exception as e:
            st.error(f"Gemini Code Execution failed at turn {turn+1}: {e}")
            break

    # Return the full history and the best attempt found
    final_parts = history[-1].parts if history else []
    return final_parts, best_attempt_code, best_performance

# --- ORCHESTRATOR INFRASTRUCTURE CLASSES ---

class Budget:
    """Budget manager with dual modes: turn-based or cost-based tracking"""
    def __init__(self,
                 mode: str = "turns",  # "turns" or "cost"
                 max_turns: int = 10,
                 max_cost_usd: float = 5.0,
                 max_tokens: int = 1_000_000):
        self.mode = mode
        self.max_turns = max_turns
        self.max_cost = max_cost_usd
        self.max_tokens = max_tokens

        self.current_turn = 0
        self.spent_cost = 0.0
        self.spent_tokens = 0

    def left(self) -> float:
        """Returns remaining budget as fraction (0.0 to 1.0)"""
        if self.mode == "turns":
            return (self.max_turns - self.current_turn) / self.max_turns if self.max_turns > 0 else 0.0
        else:  # "cost" mode
            cost_left = (self.max_cost - self.spent_cost) / self.max_cost if self.max_cost > 0 else 0.0
            token_left = (self.max_tokens - self.spent_tokens) / self.max_tokens if self.max_tokens > 0 else 0.0
            return min(cost_left, token_left)

    def advance_turn(self):
        """Increment turn counter"""
        self.current_turn += 1

    def consume(self, cost: float, tokens: int):
        """Track actual spending (useful for reporting in both modes)"""
        self.spent_cost += cost
        self.spent_tokens += tokens

    def exhausted(self) -> bool:
        """Check if budget is exhausted"""
        if self.mode == "turns":
            return self.current_turn >= self.max_turns
        else:
            return (self.spent_cost >= self.max_cost or
                    self.spent_tokens >= self.max_tokens)

@dataclass
class TurnMetrics:
    """Metrics tracked per turn for progress visualization"""
    turn: int
    tasks_attempted: int
    tasks_verified: int
    best_accuracy: float
    improvement: float  # delta from previous turn
    cost_spent: float
    tokens_used: int
    timestamp: float

@dataclass
class OrchestratorResult:
    """Unified result format for all orchestrator modes and patterns"""
    mode: str  # "inference", "analysis", "research"
    coordination_pattern: str  # "solo", "subagent", "multi_agent"
    final_score: float  # Accuracy, success rate, or quality score
    total_turns: int
    best_turn: int
    total_cost: float
    total_tokens: int
    solution: Any  # Code, consensus, or synthesis
    history: List[TurnMetrics]
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class Task:
    """Task representation for orchestrator"""
    id: str
    goal: str
    parent_ids: Set[str] = field(default_factory=set)
    inputs: Dict[str, Any] = field(default_factory=dict)
    priority: float = 0.0
    novelty_score: float = 0.0
    estimated_roi: float = 0.0
    code: Optional[str] = None

    def signature(self) -> str:
        """Hash for deduplication"""
        content = f"{self.goal}|{self.code}|{sorted(self.inputs.items())}"
        return hashlib.sha256(content.encode()).hexdigest()

@dataclass
class VerificationResult:
    """Verification result with claims and evidence"""
    task_id: str
    claims: List[str]
    evidence: Dict[str, Any]
    verdict: str  # "verified", "failed", "partial"
    confidence: float
    outputs: Any

class TaskCache:
    """Cache for task results with deduplication"""
    def __init__(self):
        self.cache: Dict[str, VerificationResult] = {}
        self.verified_tasks: Set[str] = set()

    def has(self, signature: str) -> bool:
        return signature in self.cache

    def get(self, signature: str) -> Optional[VerificationResult]:
        return self.cache.get(signature)

    def store(self, signature: str, result: VerificationResult):
        self.cache[signature] = result
        if result.verdict == "verified":
            self.verified_tasks.add(result.task_id)

class KnowledgeIndex:
    """Knowledge index for storing verified outputs"""
    def __init__(self):
        self.entries: List[Dict[str, Any]] = []
        self.embeddings = None  # Would use actual embeddings in production

    async def store(self, outputs: Any, verified: VerificationResult):
        """Flatten, embed, and tag verified outputs"""
        entry = {
            "task_id": verified.task_id,
            "outputs": outputs,
            "verdict": verified.verdict,
            "confidence": verified.confidence,
            "claims": verified.claims,
            "timestamp": time.time()
        }
        self.entries.append(entry)

    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """Search for relevant verified knowledge"""
        # Simplified: would use actual semantic search
        return self.entries[-top_k:]

class FineTuneDatasetCollector:
    """Collects examples for fine-tuning datasets"""
    def __init__(self):
        self.examples: List[Dict[str, Any]] = []

    def add_example(self, input_data: Any, output_data: Any, metadata: Optional[Dict] = None):
        """Add a training example"""
        self.examples.append({
            "input": input_data,
            "output": output_data,
            "metadata": metadata or {}
        })

    def export(self) -> List[Dict]:
        """Export collected examples"""
        return self.examples

# ============================================
# Agent Coordination Patterns
# ============================================

class AgentCoordinationPattern(Enum):
    """
    Coordination patterns define how agents work together.

    SOLO: Single agent executes the task independently
    SUBAGENT: Hierarchical delegation with specialized subagents (decomposer, generator, evaluator, etc.)
    MULTI_AGENT: Peer collaboration with independent proposals, cross-review, and consensus
    LEAF_SCAFFOLD: Hierarchical multi-agent scaffold with supervisor and specialized leaf agents
    """
    SOLO = "solo"
    SUBAGENT = "subagent"
    MULTI_AGENT = "multi_agent"
    LEAF_SCAFFOLD = "leaf_scaffold"


# ============================================
# Leaf Agent Scaffold Integration
# ============================================

class GeminiLLMClient:
    """Wrapper for Gemini API to use with leaf agents."""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = genai.Client(api_key=api_key)

    async def generate(self, prompt: str) -> str:
        """Generate text response."""
        response = await asyncio.to_thread(
            lambda: self.client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt
            )
        )

        # Track cost
        if st.session_state.cost_tracker:
            st.session_state.cost_tracker.update(
                provider="Google",
                model="gemini-2.5-flash",
                api="generate_content",
                raw_response_obj=response,
                pricing_resolver=custom_gemini_price_lookup
            )

        return response.text

    async def generate_with_code_exec(self, prompt: str) -> str:
        """Generate with code execution."""
        try:
            response = await asyncio.to_thread(
                lambda: self.client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        tools=[types.Tool(code_execution={})]
                    )
                )
            )

            # Track cost
            if st.session_state.cost_tracker:
                st.session_state.cost_tracker.update(
                    provider="Google",
                    model="gemini-2.5-flash",
                    api="generate_content",
                    raw_response_obj=response,
                    pricing_resolver=custom_gemini_price_lookup
                )

            # Extract code execution results
            output_parts = []
            if response and hasattr(response, 'candidates') and response.candidates:
                for part in response.candidates[0].content.parts:
                    if hasattr(part, 'text') and part.text:
                        output_parts.append(part.text)
                    elif hasattr(part, 'code_execution_result'):
                        if hasattr(part.code_execution_result, 'output') and part.code_execution_result.output:
                            output_parts.append(part.code_execution_result.output)

            result = "\n".join(output_parts) if output_parts else "Code execution completed but no output was generated."
            return result

        except Exception as e:
            # Return error message instead of raising to allow graceful handling
            return f"Code execution failed: {str(e)}"


class GeminiTaskPlanner(TaskPlanner):
    """Task planner using Gemini API."""

    def __init__(self, available_agents: List[AgentType], llm_client):
        super().__init__(available_agents)
        self.llm_client = llm_client

    async def _call_llm_for_planning(self, prompt: str) -> str:
        """Call Gemini for task planning."""
        response = await asyncio.to_thread(
            lambda: self.llm_client.client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
        )

        # Track cost
        if st.session_state.cost_tracker:
            st.session_state.cost_tracker.update(
                provider="Google",
                model="gemini-2.5-flash",
                api="generate_content",
                raw_response_obj=response,
                pricing_resolver=custom_gemini_price_lookup
            )

        return response.text


class GeminiResultSynthesizer(ResultSynthesizer):
    """Result synthesizer using Gemini API."""

    def __init__(self, llm_client):
        self.llm_client = llm_client

    async def _call_llm_for_synthesis(self, prompt: str) -> str:
        """Call Gemini for result synthesis."""
        return await self.llm_client.generate(prompt)


async def get_structured_summary_and_refinement(test_report: str, refinement_code: Optional[str] = None) -> TestSummaryAndRefinement:
    """Generates the structured summary and refinement suggestions using LLM."""

    SYSTEM_PROMPT = "You are an expert AI performance analyst and prompt engineer. Analyze the provided test report and suggest actionable improvements, providing the refined Python code or prompt structure if applicable."
    SUMMARY_SCHEMA = TestSummaryAndRefinement.model_json_schema()

    # PATCH 14: Create a strong, explicit instruction set naming the required Pydantic keys
    instruction_text = """
    Analyze the test report and suggest improvements.
    You MUST return JSON with the following required keys:
    1. 'findings_summary' (string): A concise summary of the test results and model performance.
    2. 'key_suggestions' (array of strings): Actionable suggestions for prompt, architecture, or tool improvement.
    3. 'suggested_improvement_code' (optional string): The refined Python code, agent framework, or prompt string.
    4. 'suggested_improvement_prompt_reasoning' (optional string): Detailed reasoning for the refinement.
    Ensure your JSON output strictly adheres to these exact key names.
    """

    payload = {
        "test_report": test_report,
        "refinement_artifact": refinement_code if refinement_code else "N/A",
        "instructions": instruction_text # Use the strict, key-naming instruction set
    }

    try:
        if use_openai and OPENAI_API_KEY:
            client = AsyncOpenAI(api_key=OPENAI_API_KEY)
            # The patched openai_structured_json ensures 'JSON' keyword is used,
            # and this strong instruction set ensures correct key names.
            raw_result = await openai_structured_json(client, OPENAI_MODEL, SYSTEM_PROMPT, payload)
        elif use_ollama and OPENROUTER_API_KEY:
            # OpenRouter (which is typically more reliable with schemas)
            raw_result = await openrouter_json(OPENROUTER_MODEL, SYSTEM_PROMPT, payload, "summary_refinement", SUMMARY_SCHEMA)
        else:
            return TestSummaryAndRefinement(findings_summary="No LLM provider configured for summarization.", key_suggestions=["Check API keys."], suggested_improvement_code=None)

        # Pydantic validation now runs on the (hopefully) correct keys
        return TestSummaryAndRefinement.model_validate(raw_result)

    except Exception as e:
        # Keep the existing robust error handling
        st.error(f"Error generating structured summary: {e}")
        # Note: If validation fails again, it means the LLM is ignoring the instructions entirely.
        return TestSummaryAndRefinement(findings_summary=f"Failed to generate structured summary due to error: {e}", key_suggestions=[], suggested_improvement_code=None)

async def display_final_summary_for_test(test_name: str, report_text: str, artifact: Optional[str] = None):
    st.divider()
    st.subheader(f"Final Analysis & Refinement for {test_name}")

    summary_result = await get_structured_summary_and_refinement(report_text, artifact)

    st.markdown(f"**Summary:** {summary_result.findings_summary}")
    st.markdown("**Key Suggestions:**")
    st.json(summary_result.key_suggestions)

    if summary_result.suggested_improvement_code:
        st.subheader("✨ Refined Prompt/Code Suggestion")
        # Determine if it's code or just a prompt
        lang = 'python' if 'class' in summary_result.suggested_improvement_code or 'def ' in summary_result.suggested_improvement_code else 'markdown'
        st.code(summary_result.suggested_improvement_code, language=lang)
    if summary_result.suggested_improvement_prompt_reasoning:
        st.subheader("💡 Reasoning for Refinement")
        st.markdown(summary_result.suggested_improvement_prompt_reasoning)

# ------------------------------------------------------------

async def classify_with_openai(text: str, allowed: List[str]) -> ClassificationWithConf:
    """
    Classify text using OpenAI with structured output.
    Routes through OpenRouter or native API based on API_ROUTING_MODE.
    Respects per-test override model via st.session_state['openai_model_override'] if set.
    """
    # Determine selected model (per-test override takes precedence)
    selected_oai_model = st.session_state.get('openai_model_override', OPENAI_MODEL)

    # Route through OpenRouter if configured
    if API_ROUTING_MODE == "openrouter":
        return await classify_with_openrouter(
            text, allowed,
            model=_to_openrouter_model_id(selected_oai_model, "openai")
        )

    # Use native OpenAI API
    if not OPENAI_API_KEY:
        raise ValueError("OPENAI_API_KEY not set")

    # Convert to native model ID (strip provider prefix if present)
    native_model = _to_native_model_id(selected_oai_model)

    async with _rate_limiter:
        client = AsyncOpenAI(api_key=OPENAI_API_KEY)
        allowed_hint = f"Allowed labels: {allowed if allowed else '[unconstrained]'}.\nPick exactly ONE of these values in 'classification_result'."
        try:
            resp = await client.chat.completions.parse(
                model=native_model,
                messages=[
                    {"role": "system", "content": "Return a structured classification with optional confidence 0..1."},
                    {"role": "user", "content": f"{allowed_hint}\nText: {text}\nRespond as JSON with keys: classification_result, rationale, confidence (0..1 optional)."}
                ],
                response_format=ClassificationWithConf
            )
            # Track the API call
            st.session_state.cost_tracker.update(
                provider="OpenAI", model=native_model, api="chat.completions.parse",
                raw_response_obj=resp, pricing_resolver=combined_price_lookup
            )
            parsed = resp.choices[0].message.parsed
            return parsed if isinstance(parsed, ClassificationWithConf) else ClassificationWithConf.model_validate(parsed)
        except Exception:
            comp = await client.chat.completions.create(
                model=native_model,
                messages=[
                    {"role": "system", "content": "Respond ONLY with JSON having keys classification_result, rationale, confidence (0..1 optional)."},
                    {"role": "user", "content": f"{allowed_hint}\nText: {text}"}
                ],
            )
            # Track the fallback API call
            st.session_state.cost_tracker.update(
                provider="OpenAI", model=native_model, api="chat.completions.create",
                raw_response_obj=comp, pricing_resolver=combined_price_lookup
            )
            content = comp.choices[0].message.content or "{}"
            if content.strip().startswith("```"):
                content = content.strip().split("\n", 1)[1].rsplit("```", 1)[0]
            data = json.loads(content)
            if "confidence" in data:
                try:
                    data["confidence"] = float(data["confidence"])
                except Exception:
                    data["confidence"] = None
            return ClassificationWithConf.model_validate(data)

async def openai_structured_json(client: AsyncOpenAI, model: str, system: str, user_jsonable: Any) -> Dict[str, Any]:
    # Ensure the system prompt includes an explicit request for JSON
    enhanced_system = system + " Respond ONLY with the requested JSON object."

    # Always use the native model ID with the OpenAI SDK
    native_model = _to_native_model_id(model)

    comp = await client.chat.completions.create(
        model=native_model,
        messages=[
            {"role":"system","content":enhanced_system},
            {"role":"user","content":json.dumps(user_jsonable, indent=2)}
        ],
        response_format={"type":"json_object"}
    )
    # Track the API call
    st.session_state.cost_tracker.update(
        provider="OpenAI", model=native_model, api="chat.completions.create",
        raw_response_obj=comp, pricing_resolver=combined_price_lookup
    )
    content = comp.choices[0].message.content or "{}"
    return json.loads(content)


# --- PATCH 28: Gemini Classification Function (with routing support) ---
async def classify_with_gemini(text: str, allowed: List[str], model: str) -> ClassificationWithConf:
    """
    Classify text using Google Gemini with structured output.
    Routes through OpenRouter or native API based on API_ROUTING_MODE.
    """
    # Route through OpenRouter if configured
    if API_ROUTING_MODE == "openrouter":
        return await classify_with_openrouter(text, allowed, model=_to_openrouter_model_id(model, "google"))

    # Use native Google Genai API
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY not set.")

    # Convert to native model ID (strip provider prefix if present)
    native_model = _to_native_model_id(model)

    # CRITICAL FIX: Create client without async context manager
    # genai.Client doesn't support async context managers
    async with _rate_limiter:
        try:
            client = genai.Client(api_key=GEMINI_API_KEY)

            schema_dict = ClassificationWithConf.model_json_schema()

            # System prompt enforces the schema and allowed labels
            system_prompt = f"You are a text classifier. Return ONLY a single JSON object that strictly adheres to the provided schema. The 'classification_result' MUST be one of the following: {allowed if allowed else 'any string'}."

            def sync_api_call():
                return client.models.generate_content(
                    model=native_model,
                    contents=[
                        types.Content(parts=[types.Part.from_text(text=system_prompt)]),
                        types.Content(parts=[types.Part.from_text(text=f"Text to classify: {text}")]),
                    ],
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json",
                        response_schema=schema_dict
                    ),
                )

            response = await asyncio.to_thread(sync_api_call)

            # Track the API call
            st.session_state.cost_tracker.update(
                provider="Google", model=native_model, api="generate_content",
                raw_response_obj=response, pricing_resolver=custom_gemini_price_lookup
            )

            data = json.loads(response.text)
            return ClassificationWithConf.model_validate(data)

        except Exception as e:
            return ClassificationWithConf(
                classification_result="",
                rationale=f"Gemini Error: {e}",
                confidence=None
            )
# ---------------------------------------------------------------


async def classify_with_ollama(base_url: str, model: str, text: str, allowed: List[str]) -> ClassificationWithConf:
    root, chat_url = _normalize_ollama_root(base_url), _normalize_ollama_root(base_url) + "/api/generate"
    schema = {"type": "object", "properties": {"classification_result": ({"type": "string", "enum": allowed} if allowed else {"type": "string"}), "rationale": {"type": "string"}, "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}}, "required": ["classification_result", "rationale"], "additionalProperties": False}
    messages = [{"role": "system", "content": "You only reply with JSON that matches the provided schema."}, {"role": "user", "content": f"Allowed labels: {allowed if allowed else '[unconstrained]'}\nText: {text}"}]
    async with _rate_limiter, httpx.AsyncClient(timeout=120) as hc:
        resp = await hc.post(chat_url, json={"model": model, "messages": messages, "stream": False, "format": schema})
        resp.raise_for_status()
        data = resp.json()
        # Track the API call
        st.session_state.cost_tracker.update(
            provider="Ollama", model=model, api="chat",
            raw_response_json=data, pricing_resolver=combined_price_lookup
        )
        content = (data.get("message", {}) or {}).get("content", "") if isinstance(data, dict) else ""
        try:
            obj = json.loads(content) if content else data
            if "confidence" in obj:
                try: obj["confidence"] = float(obj["confidence"])
                except Exception: obj["confidence"] = None
            return ClassificationWithConf.model_validate(obj)
        except ValidationError as ve:
            raise ValueError(f"Ollama validation error: {ve}")

async def ollama_json(base_url: str, model: str, system: str, user_jsonable: Any) -> Dict[str, Any]:
    body = {"model": model, "messages": [{"role":"system","content":system}, {"role":"user","content":json.dumps(user_jsonable, indent=2)}], "stream": False, "format": "json"}
    async with httpx.AsyncClient(timeout=120) as hc:
        r = await hc.post(_normalize_ollama_root(base_url) + "/api/generate", json=body)
        r.raise_for_status()
        data = r.json()
        # Track the API call
        st.session_state.cost_tracker.update(
            provider="Ollama", model=model, api="generate",
            raw_response_json=data, pricing_resolver=combined_price_lookup
        )
        txt = (data.get("message", {}) or {}).get("content") or data.get("response") or "{}"
        return json.loads(txt)

async def classify_with_openrouter(text: str, allowed: List[str], model: Optional[str] = None) -> ClassificationWithConf:
    if not OPENROUTER_API_KEY: raise ValueError("OPENROUTER_API_KEY not set")
    messages = [{"role": "system", "content": ("Return ONLY compact JSON with keys: classification_result, rationale, confidence (0..1 optional)." + (f" Allowed labels: {allowed}." if allowed else ""))}, {"role": "user", "content": text}]
    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
    async with _rate_limiter, httpx.AsyncClient(timeout=120) as hc:
        body1 = {"model": model or OPENROUTER_MODEL, "messages": messages}
        try:
            r1 = await hc.post(OPENROUTER_URL, headers=headers, json=body1)
            r1.raise_for_status()
            data1 = r1.json()
            # Track the API call
            st.session_state.cost_tracker.update(
                provider="OpenRouter", model=model or OPENROUTER_MODEL, api="chat.completions",
                raw_response_json=data1, pricing_resolver=custom_openrouter_price_lookup
            )
            content1 = (((data1 or {}).get("choices") or [{}])[0].get("message") or {}).get("content", "{}")
            if isinstance(content1, str):
                if content1.strip().startswith("```"): content1 = content1.strip().split("\n", 1)[1].rsplit("```", 1)[0]
                obj = json.loads(content1)
            else: obj = content1
            if "confidence" in obj:
                try: obj["confidence"] = float(obj["confidence"])
                except Exception: obj["confidence"] = None
            return ClassificationWithConf.model_validate(obj)
        except Exception as e1:
            schema = {"type": "object", "properties": {"classification_result": ({"type": "string", "enum": allowed} if allowed else {"type": "string"}), "rationale": {"type": "string"}, "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}}, "required": ["classification_result", "rationale"], "additionalProperties": False}
            body2 = {"model": model or OPENROUTER_MODEL, "messages": messages, "response_format": {"type": "json_schema", "json_schema": {"name": "classification", "strict": True, "schema": schema}}}
            try:
                r2 = await hc.post(OPENROUTER_URL, headers=headers, json=body2)
                r2.raise_for_status()
                data2 = r2.json()
                # Track the fallback API call
                st.session_state.cost_tracker.update(
                    provider="OpenRouter", model=model or OPENROUTER_MODEL, api="chat.completions",
                    raw_response_json=data2, pricing_resolver=custom_openrouter_price_lookup
                )
                content2 = (((data2 or {}).get("choices") or [{}])[0].get("message") or {}).get("content", "{}")
                obj2 = json.loads(content2) if isinstance(content2, str) else content2
                if "confidence" in obj2:
                    try: obj2["confidence"] = float(obj2["confidence"])
                    except Exception: obj2["confidence"] = None
                return ClassificationWithConf.model_validate(obj2)
            except Exception as e2:
                return ClassificationWithConf(classification_result="", rationale=f"OpenRouter error: {e1} | {e2}", confidence=None)

async def openrouter_json(model: str, system: str, user_jsonable: Any, schema_name: str, schema: Dict[str, Any]) -> Dict[str, Any]:
    if not OPENROUTER_API_KEY:
        raise ValueError("OPENROUTER_API_KEY not set")

    # Base headers + optional referral metadata (per OpenRouter docs)
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }
    _ref = st.secrets.get("OPENROUTER_HTTP_REFERER") or st.secrets.get("SITE_URL")
    _title = st.secrets.get("OPENROUTER_X_TITLE") or st.secrets.get("SITE_NAME")
    if _ref:
        headers["HTTP-Referer"] = _ref
    if _title:
        headers["X-Title"] = _title

    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": json.dumps(user_jsonable, indent=2)},
    ]

    # First try: JSON schema format (omit 'strict' to maximize compatibility)
    body_schema = {
        "model": model,
        "messages": messages,
        "response_format": {
            "type": "json_schema",
            "json_schema": {
                "name": schema_name,
                "schema": schema,
            },
        },
    }

    async with httpx.AsyncClient(timeout=120) as hc:
        try:
            r = await hc.post(OPENROUTER_URL, headers=headers, json=body_schema)
            r.raise_for_status()
            data = r.json()
            st.session_state.cost_tracker.update(
                provider="OpenRouter", model=model, api="chat.completions",
                raw_response_json=data, pricing_resolver=custom_openrouter_price_lookup,
            )
            content = (((data or {}).get("choices") or [{}])[0].get("message") or {}).get("content", "{}")
            return json.loads(content) if isinstance(content, str) else content
        except httpx.HTTPStatusError as e:
            # Fallback for 400/422 or other request validation errors: use json_object
            status = getattr(e.response, "status_code", None)
            if status not in (400, 422):
                raise

            body_fallback = {
                "model": model,
                "messages": messages,
                "response_format": {"type": "json_object"},
            }
            r2 = await hc.post(OPENROUTER_URL, headers=headers, json=body_fallback)
            r2.raise_for_status()
            data2 = r2.json()
            st.session_state.cost_tracker.update(
                provider="OpenRouter", model=model, api="chat.completions",
                raw_response_json=data2, pricing_resolver=custom_openrouter_price_lookup,
            )
            content2 = (((data2 or {}).get("choices") or [{}])[0].get("message") or {}).get("content", "{}")
            return json.loads(content2) if isinstance(content2, str) else content2

async def generate_text_async(prompt: str) -> str:
    """Generates text using the first available provider (prioritizes OpenRouter)."""
    # Prioritize OpenRouter if enabled
    if use_ollama and OPENROUTER_API_KEY:
        try:
            headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}"}
            messages = [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": prompt}]
            body = {"model": OPENROUTER_MODEL, "messages": messages, "max_tokens": 512}
            async with httpx.AsyncClient(timeout=60) as hc:
                r = await hc.post(OPENROUTER_URL, headers=headers, json=body)
                r.raise_for_status()
                data = r.json()
                # Track the API call
                st.session_state.cost_tracker.update(
                    provider="OpenRouter", model=OPENROUTER_MODEL, api="chat.completions",
                    raw_response_json=data, pricing_resolver=custom_openrouter_price_lookup
                )
                return data.get("choices", [{}])[0].get("message", {}).get("content", "Error: No content.")
        except Exception as e:
            # If OpenRouter fails, fall through to OpenAI if it's enabled
            if not (use_openai and OPENAI_API_KEY):
                return f"Error during OpenRouter call: {e}"

    # Fallback to OpenAI if enabled
    if use_openai and OPENAI_API_KEY:
        try:
            client = AsyncOpenAI(api_key=OPENAI_API_KEY)
            comp = await client.chat.completions.create(model=OPENAI_MODEL, messages=[{"role": "user", "content": prompt}], max_tokens=512)
            # Track the API call
            st.session_state.cost_tracker.update(
                provider="OpenAI", model=OPENAI_MODEL, api="chat.completions.create",
                raw_response_obj=comp, pricing_resolver=combined_price_lookup
            )
            return comp.choices[0].message.content or "Error: No content."
        except Exception as e:
            return f"Error during OpenAI call: {e}"

    return "Error: No text generation provider is configured or enabled."


# ---------- Core Runner (UPDATED FOR LATENCY) ----------
async def _classify_df_async(df: pd.DataFrame, use_openai: bool, use_ollama: bool, openrouter_model: str, use_ollama_local: bool = False, ollama_base_url: str = "", ollama_model: str = "", third_kind: str = "None", third_model: str = "") -> pd.DataFrame:
    labels = _allowed_labels(df)
    for col in SKELETON_COLUMNS:
        if col not in df.columns:
            df[col] = None

    # This inner worker function remains the same
    async def worker(idx: int, text: str):
        oai_res = {"result": None, "latency": 0.0}
        oll_res = {"result": None, "latency": 0.0}
        thd_res = {"result": None, "latency": 0.0}
        gmn_res = {"result": None, "latency": 0.0}  # PATCH 28: Gemini Result

        async def run_task(task_fn, result_dict):
            start_time = time.monotonic()
            try: result_dict["result"] = await _retry(task_fn)
            except Exception as e: result_dict["result"] = ClassificationWithConf(classification_result="", rationale=f"Error: {e}", confidence=None)
            result_dict["latency"] = time.monotonic() - start_time

        tasks_to_run = []
        if use_openai: tasks_to_run.append(run_task(lambda: classify_with_openai(text, labels), oai_res))
        if use_ollama:
            async def openrouter_task():
                try: return await _retry(lambda: classify_with_openrouter(text, labels, model=openrouter_model))
                except Exception as e1:
                    if use_ollama_local:
                        try: return await _retry(lambda: classify_with_ollama(ollama_base_url, ollama_model, text, labels))
                        except Exception as e2: raise Exception(f"OpenRouter failed: {e1}; Local Ollama failed: {e2}")
                    raise e1
            tasks_to_run.append(run_task(openrouter_task, oll_res))
        elif use_ollama_local:
            tasks_to_run.append(run_task(lambda: classify_with_ollama(ollama_base_url, ollama_model, text, labels), oll_res))

        # PATCH 28: Add Gemini task if enabled
        if st.session_state.get('use_gemini'):
            gemini_model = st.session_state.get('gemini_model', 'gemini-2.5-flash')
            tasks_to_run.append(run_task(lambda: classify_with_gemini(text, labels, gemini_model), gmn_res))

        if third_kind != "None":
            async def third_task_fn():
                if third_kind == "OpenAI":
                    client = AsyncOpenAI(api_key=OPENAI_API_KEY)
                    comp = await client.chat.completions.create(model=third_model, messages=[{"role": "system", "content": "Respond ONLY with JSON having keys classification_result, rationale, confidence (0..1 optional)."}, {"role": "user", "content": text}])
                    content = comp.choices[0].message.content or "{}"
                    if content.strip().startswith("```"): content = content.strip().split("\n", 1)[1].rsplit("```", 1)[0]
                    data = json.loads(content)
                    if "confidence" in data:
                        try: data["confidence"] = float(data["confidence"])
                        except Exception: data["confidence"] = None
                    return ClassificationWithConf.model_validate(data)
                elif third_kind == "OpenRouter":
                    return await classify_with_openrouter(text, labels, model=third_model)
            tasks_to_run.append(run_task(third_task_fn, thd_res))

        await asyncio.gather(*tasks_to_run)  # Gather all results
        return idx, oai_res, oll_res, thd_res, gmn_res  # PATCH 28: Return Gemini results

    # --- BATCHING LOGIC WITH PROGRESS METADATA COLLECTION ---

    # Get execution tracker
    tracker = st.session_state.get('execution_tracker')
    test_name = "Classification Test"  # Will be set dynamically based on which test is running

    # Emit start event
    if tracker:
        tracker.emit(test_name, "start", "classification_run", "Classification Run",
                    "orchestrator", total_items=len(df))

    # Define a reasonable batch size. 100 is safe for most systems.
    BATCH_SIZE = 100

    # Create a list of all coroutines to be run, but don't schedule them yet.
    all_coroutines = [worker(idx, q) for idx, row in df.iterrows() if (q := str(row.get("query", "")).strip())]

    if not all_coroutines:
        return df

    progress = st.progress(0.0, text="Running classification...")
    all_results = []
    total_coroutines = len(all_coroutines)

    # Collect progress metadata for replay visualization
    progress_metadata = {
        'batch_timestamps': [],
        'batch_sizes': [],
        'cumulative_counts': [],
        'batch_latencies': [],
        'success_rates': []
    }

    start_time = time.time()

    # Process the coroutines in batches
    for i in range(0, total_coroutines, BATCH_SIZE):
        batch_num = i // BATCH_SIZE + 1
        batch_id = f"batch_{batch_num}"
        batch_start = time.time()

        # Emit batch start
        if tracker:
            tracker.emit(test_name, "start", batch_id, f"Batch {batch_num}",
                        "batch", parent_id="classification_run",
                        batch_size=min(BATCH_SIZE, total_coroutines - i))

        batch_coroutines = all_coroutines[i:i + BATCH_SIZE]

        # Run only the current batch
        batch_results = await asyncio.gather(*batch_coroutines)
        all_results.extend(batch_results)

        batch_end = time.time()

        # Collect metadata
        completed_count = len(all_results)
        batch_latency = batch_end - batch_start

        # Calculate success rate for this batch
        successful = sum(1 for _, oai, oll, thd, gmn in batch_results
                        if (oai["result"] is not None or oll["result"] is not None or
                            thd["result"] is not None or gmn["result"] is not None))
        success_rate = successful / len(batch_results) if batch_results else 0.0

        progress_metadata['batch_timestamps'].append(batch_end - start_time)
        progress_metadata['batch_sizes'].append(len(batch_results))
        progress_metadata['cumulative_counts'].append(completed_count)
        progress_metadata['batch_latencies'].append(batch_latency)
        progress_metadata['success_rates'].append(success_rate)

        # Emit batch complete
        if tracker:
            tracker.emit(test_name, "complete", batch_id, f"Batch {batch_num}",
                        "batch", parent_id="classification_run",
                        items_processed=len(batch_results),
                        success_rate=success_rate,
                        latency=batch_latency)

        # Update progress after each batch completes
        progress.progress(completed_count / total_coroutines, text=f"Running classification... ({completed_count}/{total_coroutines})")

    progress.empty()

    # Store progress metadata in session state for replay
    if 'last_progress_metadata' not in st.session_state:
        st.session_state.last_progress_metadata = {}
    st.session_state.last_progress_metadata['classification'] = progress_metadata

    # Emit completion
    if tracker:
        tracker.emit(test_name, "complete", "classification_run", "Classification Run",
                    "orchestrator", total_items=len(df),
                    total_batches=len(progress_metadata['batch_timestamps']))

    # --- RESULT PROCESSING ---
    for idx, oai_res, oll_res, thd_res, gmn_res in all_results:  # PATCH 28: Process Gemini results
        if (oll := oll_res["result"]) is not None:
            df.loc[idx, "classification_result_openrouter_mistral"] = _normalize_label(str(oll.classification_result or ""))
            df.loc[idx, "classification_result_openrouter_mistral_rationale"] = oll.rationale or "(no rationale)"
            df.loc[idx, "classification_result_openrouter_mistral_confidence"] = oll.confidence
            df.loc[idx, "latency_openrouter_mistral"] = oll_res["latency"]
        if (oai := oai_res["result"]) is not None:
            df.loc[idx, "classification_result_openai"] = _normalize_label(str(oai.classification_result or ""))
            df.loc[idx, "classification_result_openai_rationale"] = oai.rationale or "(no rationale)"
            df.loc[idx, "classification_result_openai_confidence"] = oai.confidence
            df.loc[idx, "latency_openai"] = oai_res["latency"]
        if (gmn := gmn_res["result"]) is not None:  # PATCH 28: Write Gemini results
            df.loc[idx, "classification_result_gemini"] = _normalize_label(str(gmn.classification_result or ""))
            df.loc[idx, "classification_result_gemini_rationale"] = gmn.rationale or "(no rationale)"
            df.loc[idx, "classification_result_gemini_confidence"] = gmn.confidence
            df.loc[idx, "latency_gemini"] = gmn_res["latency"]
        if (thd := thd_res["result"]) is not None:
            df.loc[idx, "classification_result_third"] = _normalize_label(str(thd.classification_result or ""))
            df.loc[idx, "classification_result_third_rationale"] = thd.rationale or "(no rationale)"
            df.loc[idx, "classification_result_third_confidence"] = thd.confidence
            df.loc[idx, "latency_third"] = thd_res["latency"]

    return df

# ---------- NEW: Smarter Weighted Pick (Test 2) ----------
def _smarter_weighted_pick_row(row: pd.Series, f1_maps: Dict[str, Dict[str, float]]) -> Tuple[Optional[str], Optional[str]]:
    scores = {}
    model_preds = {
        "mistral": {"pred": row.get("classification_result_openrouter_mistral"), "conf": row.get("classification_result_openrouter_mistral_confidence")},
        "gpt5": {"pred": row.get("classification_result_openai"), "conf": row.get("classification_result_openai_confidence")},
        "third": {"pred": row.get("classification_result_third"), "conf": row.get("classification_result_third_confidence")}
    }

    for model, data in model_preds.items():
        pred = _normalize_label(data["pred"])
        if pred and model in f1_maps:
            conf = float(data["conf"] or 0.0)
            class_f1 = f1_maps[model].get(pred, 0.0) # Use F1 for the predicted class
            scores[model] = class_f1 * conf

    if not scores:
        return None, None

    model_pick = max(scores, key=scores.get)
    label_pick = model_preds[model_pick]["pred"]
    return model_pick, label_pick

# --- UPDATED: Flexible Judge and Pruner Functions ---
JUDGE_SCHEMA = {"type": "object","properties": {"final_choice_model": {"type": "string", "description": "One of: mistral, gpt5, third"}, "final_label": {"type": "string"}, "judge_rationale": {"type": "string"}},"required": ["final_choice_model", "final_label", "judge_rationale"], "additionalProperties": False}
JUDGE_INSTRUCTIONS = "You are a neutral judge... Return ONLY JSON with: final_choice_model, final_label, judge_rationale."

async def run_judge_flexible(payload: Dict[str, Any], model: str = None) -> Dict[str, Any]:
    """Run judge with flexible model selection."""
    if model is None:
        model = st.session_state.get('judge_model', 'openai/gpt-5-mini')

    # Route based on API_ROUTING_MODE or provider
    provider = _get_provider_from_model_id(model)

    if API_ROUTING_MODE == "openrouter" or provider in ["mistralai", "anthropic", "meta-llama", "deepseek"]:
        # Use OpenRouter
        return await openrouter_json(_to_openrouter_model_id(model), JUDGE_INSTRUCTIONS, payload, "judge", JUDGE_SCHEMA)
    elif provider == "openai":
        # Use native OpenAI
        native_model = _to_native_model_id(model)
        return await openai_structured_json(AsyncOpenAI(api_key=OPENAI_API_KEY), native_model, JUDGE_INSTRUCTIONS, payload)
    else:
        # Default to OpenRouter
        return await openrouter_json(_to_openrouter_model_id(model), JUDGE_INSTRUCTIONS, payload, "judge", JUDGE_SCHEMA)

# Legacy functions for backward compatibility
async def run_judge_ollama(payload: Dict[str, Any]) -> Dict[str, Any]:
    return await run_judge_flexible(payload, st.session_state.get('judge_model', OPENROUTER_MODEL))

async def run_judge_openai(payload: Dict[str, Any]) -> Dict[str, Any]:
    return await run_judge_flexible(payload, st.session_state.get('judge_model', OPENAI_MODEL))
PRUNER_SCHEMA = {
    "type": "object",
    "properties": {
        "kept_context_keys": {
            "type": "array",
            "description": "An array of essential context keys to keep.",
            "items": {
                "type": "string",
                "enum": ["instruction", "summary", "user_messages", "agent_responses", "tool_logs"]
            }
        },
        "action": {
            "type": "string",
            "enum": ["general_answer", "kb_lookup", "tool_call"],
            "description": "The best next action to take."
        },
        "prune_rationale": {
            "type": "string",
            "description": "Briefly explain why you chose these keys and this action."
        }
    },
    "required": ["kept_context_keys", "action", "prune_rationale"],
    "additionalProperties": False
}

PRUNER_INSTRUCTIONS = """
You are an expert AI assistant that analyzes conversational context to plan the next step.
Your goal is to identify the minimum essential context needed to answer the user's `new_question` and decide on the correct `action`.

AVAILABLE CONTEXT KEYS: ["instruction", "summary", "user_messages", "agent_responses", "tool_logs"]


Think step-by-step:
1.  **Analyze the `new_question`**: What is the user's core intent?
2.  **Review the `context` items**: Determine which of the available keys are directly relevant to the question. For example, if the question is about a past tool result, keep `tool_logs`.
3.  **Choose the `action` based on this logic**:
    - `tool_call`: Use this if the `new_question` requires **new, external information** that is not present in the provided context.
    - `kb_lookup`: Use this if the answer is likely **already present** within the provided `context` (e.g., in `agent_responses` or `tool_logs`).
    - `general_answer`: Use this for conversational follow-ups, summarizations, or clarifications that don't require tools or deep context.

Return ONLY a valid JSON object matching the required schema.
"""

async def run_pruner(payload: Dict[str, Any], model: str = None) -> Dict[str, Any]:
    """Run pruner with flexible model selection."""
    if model is None:
        model = st.session_state.get('pruner_model', 'openai/gpt-5-mini')

    # Route based on API_ROUTING_MODE or provider
    provider = _get_provider_from_model_id(model)

    try:
        if API_ROUTING_MODE == "openrouter" or provider in ["mistralai", "anthropic", "meta-llama", "deepseek"]:
            # Use OpenRouter
            return await openrouter_json(_to_openrouter_model_id(model), PRUNER_INSTRUCTIONS, payload, "pruner", PRUNER_SCHEMA)
        elif provider == "openai":
            # Use native OpenAI
            native_model = _to_native_model_id(model)
            return await openai_structured_json(AsyncOpenAI(api_key=OPENAI_API_KEY), native_model, PRUNER_INSTRUCTIONS, payload)
        else:
            # Default to OpenRouter
            return await openrouter_json(_to_openrouter_model_id(model), PRUNER_INSTRUCTIONS, payload, "pruner", PRUNER_SCHEMA)
    except Exception as e:
        # Fallback to OpenAI if available
        if OPENAI_API_KEY and provider != "openai":
            try:
                return await openai_structured_json(AsyncOpenAI(api_key=OPENAI_API_KEY), _to_native_model_id(OPENAI_MODEL), PRUNER_INSTRUCTIONS, payload)
            except:
                pass
        raise e

# ======================= Sidebar & Main Layout =======================
with st.sidebar:
    st.header("Providers & Models (Deprecated)")
    st.info("Model selection moved into each test section. Configure per test below. This sidebar block will be removed.")

    # --- PATCH 26/29: Unified Format Function & UI Updates ---
    def format_model_option(model_id: str) -> str:
        """Format model ID with metadata for display in dropdown (unified for all providers)."""
        if model_id == "Custom...":
            return model_id

        # Check Gemini first due to potential model naming conflicts
        if model_id in GEMINI_MODEL_METADATA:
            metadata = GEMINI_MODEL_METADATA.get(model_id, {})
            context = metadata.get('context', 'N/A')
            input_cost = metadata.get('input_cost', 'N/A')
            output_cost = metadata.get('output_cost', 'N/A')
            return f"{model_id} (Ctx: {context}, In: {input_cost}/M, Out: {output_cost}/M)"

        # Determine provider based on ID structure
        if model_id in OPENROUTER_MODEL_METADATA:
            metadata = OPENROUTER_MODEL_METADATA.get(model_id, {})
            context = metadata.get('context', 'N/A')
            input_cost = metadata.get('input_cost', 'N/A')
            output_cost = metadata.get('output_cost', 'N/A')
            display_name = model_id.split('/')[-1]
            return f"{display_name} (Ctx: {context}, In: {input_cost}/M, Out: {output_cost}/M)"

        if model_id in OPENAI_MODEL_METADATA:
            metadata = OPENAI_MODEL_METADATA.get(model_id, {})
            context = metadata.get('context', 'N/A')
            input_cost = metadata.get('input_cost', 'N/A')
            output_cost = metadata.get('output_cost', 'N/A')
            return f"{model_id} (Ctx: {context}, In: {input_cost}/M, Out: {output_cost}/M)"

        if model_id in OLLAMA_MODEL_METADATA:
            metadata = OLLAMA_MODEL_METADATA.get(model_id, {})
            context = metadata.get('context', 'N/A')
            info = metadata.get('local_info', 'N/A')
            return f"{model_id} (Ctx: {context}, Info: {info})"

        return model_id
    # -------------------------------------------------------------

    # 1. OpenRouter Selection (Main Model)
    _openrouter_model_ids = list(OPENROUTER_MODEL_METADATA.keys()) + ["Custom..."]
    _default_idx_or = _openrouter_model_ids.index(OPENROUTER_MODEL) if OPENROUTER_MODEL in _openrouter_model_ids else (len(_openrouter_model_ids) - 1)
    _selected_or = st.selectbox("OpenRouter model", options=_openrouter_model_ids, index=_default_idx_or, format_func=format_model_option, key="openrouter_main_select")
    OPENROUTER_MODEL = st.text_input("Custom OpenRouter model ID", value=OPENROUTER_MODEL) if _selected_or == "Custom..." else _selected_or
    if use_ollama and not OPENROUTER_API_KEY: st.warning("OPENROUTER_API_KEY not set.")

    # 2. Gemini Selection (PATCH 29)
    use_gemini = st.checkbox("Use Google Gemini (structured output)", value=False, key='use_gemini')
    _gemini_model_ids = list(GEMINI_MODEL_METADATA.keys())
    GEMINI_MODEL = st.secrets.get("GEMINI_MODEL", "gemini-2.5-flash")
    _default_idx_gmn = _gemini_model_ids.index(GEMINI_MODEL) if GEMINI_MODEL in _gemini_model_ids else 0
    selected_gemini = st.selectbox("Gemini model", options=_gemini_model_ids, index=_default_idx_gmn, format_func=format_model_option, key="gemini_main_select")
    st.session_state['gemini_model'] = selected_gemini
    if use_gemini and not GEMINI_API_KEY: st.warning("GEMINI_API_KEY not set.")

    # 3. OpenAI Selection
    use_openai = st.checkbox("Use OpenAI (structured output)", value=True)
    _openai_model_ids = list(OPENAI_MODEL_METADATA.keys())
    _default_idx_oai = _openai_model_ids.index(OPENAI_MODEL) if OPENAI_MODEL in _openai_model_ids else 0
    OPENAI_MODEL = st.selectbox("OpenAI model", options=_openai_model_ids, index=_default_idx_oai, format_func=format_model_option, key="openai_main_select")
    if use_openai and not OPENAI_API_KEY: st.warning("OPENAI_API_KEY not set.")

    # 4. Ollama Selection (Local)
    use_ollama_local = st.checkbox("Use Ollama (local/private)", value=False)
    OLLAMA_BASE_URL = st.text_input("Ollama base URL", value=OLLAMA_BASE_URL)
    _ollama_model_ids = list(OLLAMA_MODEL_METADATA.keys())
    _default_idx_ollama = _ollama_model_ids.index(OLLAMA_MODEL) if OLLAMA_MODEL in _ollama_model_ids else (len(_ollama_model_ids) - 1)
    _selected_ollama = st.selectbox("Ollama model", options=_ollama_model_ids, index=_default_idx_ollama, format_func=format_model_option, key="ollama_main_select")
    OLLAMA_MODEL = st.text_input("Custom Ollama model ID", value=OLLAMA_MODEL) if _selected_ollama == "Custom..." else _selected_ollama

    st.divider()
    st.subheader("Third Model (Test 2/3)")
    THIRD_KIND = st.selectbox("Third model kind", ["None", "OpenRouter", "OpenAI"], index=["None","OpenRouter","OpenAI"].index(THIRD_KIND if THIRD_KIND in ["None","OpenRouter","OpenAI"] else "None"))
    if THIRD_KIND == "OpenRouter":
        THIRD_MODEL = st.selectbox("Third model (OpenRouter)", options=_openrouter_model_ids, format_func=format_model_option, index=0, key="openrouter_third_select")
    elif THIRD_KIND == "OpenAI":
        THIRD_MODEL = st.selectbox("Third model (OpenAI)", options=_openai_model_ids, format_func=format_model_option, index=0, key="openai_third_select")
    else: THIRD_MODEL = ""
    st.divider()
    st.subheader("DataFrame Controls")
    if st.button("Clear results"):
        for c in st.session_state.df.columns:
            if c not in ["query", "classification"]: st.session_state.df[c] = None
        st.info("Cleared model outputs")
    st.subheader("Row limit for tests")
    _limit_choice = st.selectbox("Rows to test", list(ROW_LIMIT_OPTIONS.keys()), index=3)
    ROW_LIMIT_N = ROW_LIMIT_OPTIONS[_limit_choice]
    st.divider()
    st.subheader("Analysis Options")
    explain_cm = st.checkbox("Explain Confusion Matrices with LLM", value=True, help="Uses the selected OpenRouter/OpenAI model to provide a natural language explanation of confusion matrix results.")

    # --- COST TRACKING UI ---
    st.divider()
    st.subheader("💰 Cost Tracking")

    ct = st.session_state.cost_tracker

    # Display totals
    st.metric("Total Cost", f"${ct.totals['total_cost_usd']:.4f}")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Input Cost", f"${ct.totals['input_cost_usd']:.4f}")
        st.metric("Prompt Tokens", f"{ct.totals['prompt_tokens']:,}")
    with col2:
        st.metric("Output Cost", f"${ct.totals['output_cost_usd']:.4f}")
        st.metric("Completion Tokens", f"{ct.totals['completion_tokens']:,}")

    st.metric("Total Tokens", f"{ct.totals['total_tokens']:,}")

    # Interactive cost visualizations
    render_cost_dashboard()

    # Display breakdown by provider/model
    with st.expander("Cost Breakdown by Model"):
        summary = ct.get_summary()
        if summary:
            for (provider, model), stats in summary.items():
                st.markdown(f"**{provider} / {model}**")
                st.text(f"  Calls: {stats['calls']}")
                st.text(f"  Tokens: {stats['total_tokens']:,} ({stats['prompt_tokens']:,} in / {stats['completion_tokens']:,} out)")
                st.text(f"  Cost: ${stats['total_cost_usd']:.4f} (${stats['input_cost_usd']:.4f} in / ${stats['output_cost_usd']:.4f} out)")
                st.divider()
        else:
            st.info("No API calls tracked yet.")

    # Display recent calls
    with st.expander("Recent API Calls"):
        if ct.by_call:
            recent_calls = ct.by_call[-10:]  # Last 10 calls
            for i, call in enumerate(reversed(recent_calls), 1):
                st.markdown(f"**Call {len(ct.by_call) - i + 1}**: {call['provider']} / {call['model']}")
                st.text(f"  API: {call['api']}")
                st.text(f"  Tokens: {call['total_tokens']:,} ({call['prompt_tokens']:,} in / {call['completion_tokens']:,} out)")
                st.text(f"  Cost: ${call['total_cost_usd']:.4f}")
                st.divider()
        else:
            st.info("No API calls tracked yet.")

    # Reset button
    if st.button("Reset Cost Tracking"):
        st.session_state.cost_tracker.reset()
        st.success("Cost tracking reset!")
        st.rerun()


# --- PATCH 5: Update Tab Definitions ---
tabs = st.tabs([
    "Preparation: Data Generation", # New Tab 0
    "Test 1: Classify, F1, Latency & Analysis",
    "Test 2: Advanced Ensembling",
    "Test 3: LLM as Judge",
    "Test 4: Quantitative Pruning",
    "Test 5: Agent Self-Refinement (Code Ex.)", # Tab 5
    "Agent Dashboard" # NEW Tab 6
])
# -------------------------------------

# NEW: Data Generation Function
async def generate_synthetic_data(task_prompt: str, data_size: int, data_type: str, generation_model: str) -> pd.DataFrame:

    # --- PATCH 16: Define the schema for a single item based on data type ---
    if data_type == "Classification":
        item_schema = SyntheticDataItem.model_json_schema()
    elif data_type == "Tool/Agent Sequence":
        item_schema = ToolCallSequenceItem.model_json_schema()
    elif data_type == "Context Pruning":
        item_schema = PruningDataItem.model_json_schema()
    else:
        raise ValueError(f"Invalid data type: {data_type}")
    # -----------------------------------------------------------------------

    # Define a schema wrapper to ensure the output is a single dict with a list inside,
    # which is safer for structured JSON APIs.
    list_wrapper_schema = {
        "type": "object",
        "properties": {"items": {"type": "array", "items": item_schema}},
        "required": ["items"]
    }

    # User payload includes instructions for the LLM
    user_input = {
        "required_output_count": data_size,
        "task_description": task_prompt,
        "instructions": f"Generate exactly {data_size} distinct data items, ensuring all output is wrapped in a JSON object under the key 'items'.",
        "target_schema": list_wrapper_schema
    }

    st.info(f"Generating {data_size} items using {generation_model}...")

    try:
        raw_result = {}

        # --- UPDATED: Flexible model routing for data generation ---
        provider = _get_provider_from_model_id(generation_model)
        system = "You are a synthetic data generator. Respond ONLY with a single JSON object matching the requested schema, containing a list of objects under the key 'items'."

        if API_ROUTING_MODE == "openrouter" or provider in ["mistralai", "anthropic", "meta-llama", "deepseek"]:
            # Use OpenRouter
            if not OPENROUTER_API_KEY:
                raise ValueError("OPENROUTER_API_KEY not set for OpenRouter mode.")
            raw_result = await openrouter_json(_to_openrouter_model_id(generation_model), system, user_input, "synthetic_dataset_list", list_wrapper_schema)

        elif provider == "openai":
            # Use native OpenAI
            if not OPENAI_API_KEY:
                raise ValueError("OPENAI_API_KEY not set for OpenAI model.")
            client = AsyncOpenAI(api_key=OPENAI_API_KEY)
            native_model = _to_native_model_id(generation_model)
            raw_result = await openai_structured_json(client, native_model, system, user_input)

        elif provider == "google":
            # For Gemini, route through OpenRouter (native Gemini doesn't support structured JSON well)
            if not OPENROUTER_API_KEY:
                raise ValueError("OPENROUTER_API_KEY required for Gemini models in data generation.")
            raw_result = await openrouter_json(_to_openrouter_model_id(generation_model), system, user_input, "synthetic_dataset_list", list_wrapper_schema)

        else:
            # Default to OpenRouter
            if not OPENROUTER_API_KEY:
                raise ValueError("OPENROUTER_API_KEY not set.")
            raw_result = await openrouter_json(_to_openrouter_model_id(generation_model), system, user_input, "synthetic_dataset_list", list_wrapper_schema)

        # --- Robust Extraction ---
        data_list = []
        if isinstance(raw_result, dict):
            # Explicitly extract from the 'items' key enforced by the schema
            data_list = raw_result.get('items', [])

        if not isinstance(data_list, list) or not data_list:
             # Fallback: If 'items' didn't work, try to find a list anywhere else in the top level
             if isinstance(raw_result, dict):
                 for key, value in raw_result.items():
                     if isinstance(value, list) and all(isinstance(i, dict) for i in value):
                         data_list = value
                         break

        if not data_list:
             raise ValueError(f"Could not extract a list of structured items from the LLM response: {raw_result}")

        # Final check and DataFrame creation
        df = pd.DataFrame(data_list)
        return df

    except Exception as e:
        st.error(f"Data generation failed: {e}")
        return pd.DataFrame()
# --------------------------------------------------------------------------


with tabs[0]:
    st.header("Preparation: Synthetic Data Generation")

    # --- NEW: Display dataset status ---
    st.subheader("📊 Dataset Status")
    col1, col2, col3 = st.columns(3)

    with col1:
        if os.path.exists(CLASSIFICATION_DATASET_PATH):
            df_class = pd.read_csv(CLASSIFICATION_DATASET_PATH)
            st.success(f"✅ Classification\n({len(df_class)} rows)")
        else:
            st.warning("⚠️ Classification\n(Missing)")

    with col2:
        if os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
            df_tool = pd.read_csv(TOOL_SEQUENCE_DATASET_PATH)
            st.success(f"✅ Tool/Agent Seq.\n({len(df_tool)} rows)")
        else:
            st.warning("⚠️ Tool/Agent Seq.\n(Missing)")

    with col3:
        if os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
            df_prune = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH)
            st.success(f"✅ Context Pruning\n({len(df_prune)} rows)")
        else:
            st.warning("⚠️ Context Pruning\n(Missing)")

    st.divider()

    # --- NEW: Dataset Composition Visualization ---
    st.subheader("📊 Dataset Composition Analysis")

    # Create tabs for each dataset type
    dataset_viz_tabs = st.tabs(["Classification", "Tool/Agent Sequence", "Context Pruning"])

    with dataset_viz_tabs[0]:
        if os.path.exists(CLASSIFICATION_DATASET_PATH):
            df_class = pd.read_csv(CLASSIFICATION_DATASET_PATH)
            visualize_dataset_composition(df_class, dataset_type="Classification")
        else:
            st.info("Generate the Classification dataset to see composition analysis.")

    with dataset_viz_tabs[1]:
        if os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
            df_tool = pd.read_csv(TOOL_SEQUENCE_DATASET_PATH)
            visualize_dataset_composition(df_tool, dataset_type="Tool/Agent Sequence")
        else:
            st.info("Generate the Tool/Agent Sequence dataset to see composition analysis.")

    with dataset_viz_tabs[2]:
        if os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
            df_prune = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH)
            visualize_dataset_composition(df_prune, dataset_type="Context Pruning")
        else:
            st.info("Generate the Context Pruning dataset to see composition analysis.")

    st.divider()

    # --- NEW: Cross-Dataset Analytics Section ---
    # Only display if at least one dataset exists
    if any([os.path.exists(p) for p in [CLASSIFICATION_DATASET_PATH, TOOL_SEQUENCE_DATASET_PATH, CONTEXT_PRUNING_DATASET_PATH]]):
        st.subheader("📊 Cross-Dataset Analytics")

        analytics_tabs = st.tabs(["Dataset Comparison", "Generation History", "Quality Metrics", "Token Economics"])

        # Tab 1: Dataset Comparison Matrix
        with analytics_tabs[0]:
            st.markdown("### Dataset Comparison Matrix")

            # Collect statistics for all datasets
            dataset_stats = []

            # Classification dataset
            if os.path.exists(CLASSIFICATION_DATASET_PATH):
                try:
                    df_class = pd.read_csv(CLASSIFICATION_DATASET_PATH)
                    unique_labels = df_class['classification'].nunique() if 'classification' in df_class.columns else 0
                    avg_query_len = df_class['query'].astype(str).str.len().mean() if 'query' in df_class.columns else 0
                    missing_pct = (df_class.isnull().sum().sum() / df_class.size * 100) if df_class.size > 0 else 0
                    file_size_kb = os.path.getsize(CLASSIFICATION_DATASET_PATH) / 1024

                    dataset_stats.append({
                        'Dataset': 'Classification',
                        'Total Items': len(df_class),
                        'Unique Labels/Actions': unique_labels,
                        'Avg Query Length (chars)': round(avg_query_len, 1),
                        'Missing Values (%)': round(missing_pct, 2),
                        'File Size (KB)': round(file_size_kb, 2)
                    })
                except Exception as e:
                    st.warning(f"Error reading Classification dataset: {e}")
            else:
                dataset_stats.append({
                    'Dataset': 'Classification',
                    'Total Items': 'N/A',
                    'Unique Labels/Actions': 'N/A',
                    'Avg Query Length (chars)': 'N/A',
                    'Missing Values (%)': 'N/A',
                    'File Size (KB)': 'N/A'
                })

            # Tool/Agent Sequence dataset
            if os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
                try:
                    df_tool = pd.read_csv(TOOL_SEQUENCE_DATASET_PATH)
                    unique_seqs = df_tool['expected_sequence'].nunique() if 'expected_sequence' in df_tool.columns else 0
                    avg_query_len = df_tool['query'].astype(str).str.len().mean() if 'query' in df_tool.columns else 0
                    missing_pct = (df_tool.isnull().sum().sum() / df_tool.size * 100) if df_tool.size > 0 else 0
                    file_size_kb = os.path.getsize(TOOL_SEQUENCE_DATASET_PATH) / 1024

                    dataset_stats.append({
                        'Dataset': 'Tool/Agent Sequence',
                        'Total Items': len(df_tool),
                        'Unique Labels/Actions': unique_seqs,
                        'Avg Query Length (chars)': round(avg_query_len, 1),
                        'Missing Values (%)': round(missing_pct, 2),
                        'File Size (KB)': round(file_size_kb, 2)
                    })
                except Exception as e:
                    st.warning(f"Error reading Tool/Agent Sequence dataset: {e}")
            else:
                dataset_stats.append({
                    'Dataset': 'Tool/Agent Sequence',
                    'Total Items': 'N/A',
                    'Unique Labels/Actions': 'N/A',
                    'Avg Query Length (chars)': 'N/A',
                    'Missing Values (%)': 'N/A',
                    'File Size (KB)': 'N/A'
                })

            # Context Pruning dataset
            if os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
                try:
                    df_prune = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH)
                    unique_actions = df_prune['expected_action'].nunique() if 'expected_action' in df_prune.columns else 0
                    avg_query_len = df_prune['new_question'].astype(str).str.len().mean() if 'new_question' in df_prune.columns else 0
                    missing_pct = (df_prune.isnull().sum().sum() / df_prune.size * 100) if df_prune.size > 0 else 0
                    file_size_kb = os.path.getsize(CONTEXT_PRUNING_DATASET_PATH) / 1024

                    dataset_stats.append({
                        'Dataset': 'Context Pruning',
                        'Total Items': len(df_prune),
                        'Unique Labels/Actions': unique_actions,
                        'Avg Query Length (chars)': round(avg_query_len, 1),
                        'Missing Values (%)': round(missing_pct, 2),
                        'File Size (KB)': round(file_size_kb, 2)
                    })
                except Exception as e:
                    st.warning(f"Error reading Context Pruning dataset: {e}")
            else:
                dataset_stats.append({
                    'Dataset': 'Context Pruning',
                    'Total Items': 'N/A',
                    'Unique Labels/Actions': 'N/A',
                    'Avg Query Length (chars)': 'N/A',
                    'Missing Values (%)': 'N/A',
                    'File Size (KB)': 'N/A'
                })

            # Create DataFrame and display with styling
            stats_df = pd.DataFrame(dataset_stats)

            # Apply gradient styling only to numeric columns
            def style_stats(df):
                styled = df.style
                numeric_cols = ['Total Items', 'Avg Query Length (chars)', 'File Size (KB)']
                for col in numeric_cols:
                    if col in df.columns:
                        # Only apply gradient to numeric values (not 'N/A')
                        try:
                            styled = styled.background_gradient(subset=[col], cmap='Blues', vmin=0)
                        except:
                            pass
                return styled

            st.dataframe(style_stats(stats_df), use_container_width=True)

            # Comparison Charts
            st.markdown("### Visual Comparison")
            comp_col1, comp_col2 = st.columns(2)

            with comp_col1:
                # Bar chart: Total items
                valid_stats = [s for s in dataset_stats if s['Total Items'] != 'N/A']
                if valid_stats:
                    fig_items = go.Figure(data=[go.Bar(
                        x=[s['Dataset'] for s in valid_stats],
                        y=[s['Total Items'] for s in valid_stats],
                        text=[s['Total Items'] for s in valid_stats],
                        textposition='auto',
                        marker_color='#1f77b4',
                        hovertemplate='<b>%{x}</b><br>Items: %{y}<extra></extra>'
                    )])
                    fig_items.update_layout(
                        title="Total Items Comparison",
                        xaxis_title="Dataset",
                        yaxis_title="Total Items",
                        height=300,
                        margin=dict(l=20, r=20, t=40, b=40)
                    )
                    st.plotly_chart(fig_items, use_container_width=True, config=PLOTLY_CONFIG)
                else:
                    st.info("No datasets available for comparison")

            with comp_col2:
                # Bar chart: Average query complexity
                valid_stats = [s for s in dataset_stats if s['Avg Query Length (chars)'] != 'N/A']
                if valid_stats:
                    fig_complexity = go.Figure(data=[go.Bar(
                        x=[s['Dataset'] for s in valid_stats],
                        y=[s['Avg Query Length (chars)'] for s in valid_stats],
                        text=[f"{s['Avg Query Length (chars)']:.1f}" for s in valid_stats],
                        textposition='auto',
                        marker_color='#ff7f0e',
                        hovertemplate='<b>%{x}</b><br>Avg Length: %{y:.1f} chars<extra></extra>'
                    )])
                    fig_complexity.update_layout(
                        title="Average Query Complexity",
                        xaxis_title="Dataset",
                        yaxis_title="Avg Character Length",
                        height=300,
                        margin=dict(l=20, r=20, t=40, b=40)
                    )
                    st.plotly_chart(fig_complexity, use_container_width=True, config=PLOTLY_CONFIG)
                else:
                    st.info("No datasets available for comparison")

        # Tab 2: Generation History & Cost Trends
        with analytics_tabs[1]:
            st.markdown("### Generation History & Cost Trends")

            # Check if generation history exists
            gen_history = st.session_state.get("generation_history", [])

            if not gen_history:
                st.info("No generation history available. Generate datasets to see cost trends.")
            else:
                # Convert to DataFrame for easier manipulation
                hist_df = pd.DataFrame(gen_history)

                # Add run number
                hist_df['run_number'] = range(1, len(hist_df) + 1)

                # Calculate cumulative cost
                hist_df['cumulative_cost'] = hist_df['cost_usd'].cumsum()

                # 1. Dual-Axis Cost Trend Chart
                st.markdown("#### Cost Trends Over Time")

                fig_cost_trend = make_subplots(specs=[[{"secondary_y": True}]])

                # Primary axis: Cost per generation run
                fig_cost_trend.add_trace(
                    go.Scatter(
                        x=hist_df['run_number'],
                        y=hist_df['cost_usd'],
                        mode='lines+markers',
                        name='Cost per Run',
                        line=dict(color='#2ca02c', width=2),
                        marker=dict(size=8),
                        hovertemplate='Run #%{x}<br>Cost: $%{y:.4f}<extra></extra>'
                    ),
                    secondary_y=False
                )

                # Secondary axis: Cumulative cost
                fig_cost_trend.add_trace(
                    go.Scatter(
                        x=hist_df['run_number'],
                        y=hist_df['cumulative_cost'],
                        mode='lines',
                        name='Cumulative Cost',
                        line=dict(color='#d62728', width=2, dash='dash'),
                        hovertemplate='Run #%{x}<br>Total: $%{y:.4f}<extra></extra>'
                    ),
                    secondary_y=True
                )

                # Update axes
                fig_cost_trend.update_xaxes(title_text="Generation Run Number")
                fig_cost_trend.update_yaxes(title_text="Cost per Run (USD)", secondary_y=False)
                fig_cost_trend.update_yaxes(title_text="Cumulative Cost (USD)", secondary_y=True)

                fig_cost_trend.update_layout(
                    height=400,
                    hovermode='x unified',
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                )

                st.plotly_chart(fig_cost_trend, use_container_width=True, config=PLOTLY_CONFIG)

                # 2. Token Usage Trends
                st.markdown("#### Token Usage Trends")

                fig_tokens = go.Figure()

                fig_tokens.add_trace(go.Scatter(
                    x=hist_df['run_number'],
                    y=hist_df['tokens'],
                    mode='lines+markers',
                    name='Tokens Used',
                    fill='tozeroy',
                    fillcolor='rgba(31, 119, 180, 0.3)',
                    line=dict(color='#1f77b4', width=2),
                    marker=dict(size=6),
                    hovertemplate='Run #%{x}<br>Tokens: %{y:,}<extra></extra>'
                ))

                fig_tokens.update_layout(
                    title="Token Usage Over Time",
                    xaxis_title="Generation Run Number",
                    yaxis_title="Total Tokens Used",
                    height=300,
                    margin=dict(l=20, r=20, t=40, b=40)
                )

                st.plotly_chart(fig_tokens, use_container_width=True, config=PLOTLY_CONFIG)

                # 3. Model Efficiency Comparison (if multiple models used)
                unique_models = hist_df['model'].nunique()

                if unique_models >= 2:
                    st.markdown("#### Model Efficiency Comparison")

                    # Calculate efficiency metrics per model
                    model_efficiency = hist_df.groupby('model').agg({
                        'cost_usd': 'sum',
                        'tokens': 'sum',
                        'total_items': 'sum'
                    }).reset_index()

                    model_efficiency['cost_per_item'] = model_efficiency['cost_usd'] / model_efficiency['total_items']
                    model_efficiency['tokens_per_item'] = model_efficiency['tokens'] / model_efficiency['total_items']

                    eff_col1, eff_col2 = st.columns(2)

                    with eff_col1:
                        # Cost per item by model
                        fig_cost_eff = go.Figure(data=[go.Bar(
                            x=model_efficiency['model'],
                            y=model_efficiency['cost_per_item'],
                            text=[f"${v:.4f}" for v in model_efficiency['cost_per_item']],
                            textposition='auto',
                            marker_color='#2ca02c',
                            hovertemplate='<b>%{x}</b><br>Cost/Item: $%{y:.4f}<extra></extra>'
                        )])
                        fig_cost_eff.update_layout(
                            title="Cost per Item by Model",
                            xaxis_title="Model",
                            yaxis_title="Cost per Item (USD)",
                            height=300,
                            margin=dict(l=20, r=20, t=40, b=40)
                        )
                        st.plotly_chart(fig_cost_eff, use_container_width=True, config=PLOTLY_CONFIG)

                    with eff_col2:
                        # Tokens per item by model
                        fig_tokens_eff = go.Figure(data=[go.Bar(
                            x=model_efficiency['model'],
                            y=model_efficiency['tokens_per_item'],
                            text=[f"{v:.0f}" for v in model_efficiency['tokens_per_item']],
                            textposition='auto',
                            marker_color='#1f77b4',
                            hovertemplate='<b>%{x}</b><br>Tokens/Item: %{y:.0f}<extra></extra>'
                        )])
                        fig_tokens_eff.update_layout(
                            title="Tokens per Item by Model",
                            xaxis_title="Model",
                            yaxis_title="Tokens per Item",
                            height=300,
                            margin=dict(l=20, r=20, t=40, b=40)
                        )
                        st.plotly_chart(fig_tokens_eff, use_container_width=True, config=PLOTLY_CONFIG)

        # Tab 3: Quality Metrics Dashboard
        with analytics_tabs[2]:
            st.markdown("### Quality Metrics Dashboard")

            # 3-column layout for each dataset
            qm_col1, qm_col2, qm_col3 = st.columns(3)

            # --- Column 1: Classification Dataset Quality ---
            with qm_col1:
                st.markdown("#### 📋 Classification Dataset")

                if os.path.exists(CLASSIFICATION_DATASET_PATH):
                    try:
                        df_class = pd.read_csv(CLASSIFICATION_DATASET_PATH)

                        if not df_class.empty and 'classification' in df_class.columns:
                            # Calculate balance score (min/max class ratio)
                            class_counts = df_class['classification'].value_counts()
                            if len(class_counts) > 1:
                                balance_score = (class_counts.min() / class_counts.max() * 100) if class_counts.max() > 0 else 0
                            else:
                                balance_score = 100  # Perfect balance if only one class

                            # Gauge chart for balance
                            fig_gauge_class = go.Figure(go.Indicator(
                                mode="gauge+number",
                                value=balance_score,
                                title={'text': "Balance Score"},
                                gauge={
                                    'axis': {'range': [0, 100]},
                                    'bar': {'color': "darkblue"},
                                    'steps': [
                                        {'range': [0, 50], 'color': "lightgray"},
                                        {'range': [50, 80], 'color': "lightyellow"},
                                        {'range': [80, 100], 'color': "lightgreen"}
                                    ],
                                    'threshold': {
                                        'line': {'color': "red", 'width': 4},
                                        'thickness': 0.75,
                                        'value': 90
                                    }
                                }
                            ))
                            fig_gauge_class.update_layout(height=250, margin=dict(l=20, r=20, t=40, b=20))
                            st.plotly_chart(fig_gauge_class, use_container_width=True, config=PLOTLY_CONFIG)

                            # Metrics below gauge
                            metric_col1, metric_col2 = st.columns(2)
                            with metric_col1:
                                diversity = len(class_counts)
                                st.metric("Diversity", f"{diversity} classes")
                            with metric_col2:
                                completeness = (1 - df_class.isnull().sum().sum() / df_class.size) * 100 if df_class.size > 0 else 0
                                st.metric("Completeness", f"{completeness:.1f}%")

                            # Keyword analysis for classification
                            st.markdown("**Top Keywords by Label**")

                            # Simple stop words list
                            stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                                        'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',
                                        'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
                                        'would', 'should', 'could', 'may', 'might', 'must', 'can', 'this',
                                        'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'}

                            if 'query' in df_class.columns:
                                # Get top label (most common)
                                top_label = class_counts.index[0]
                                label_queries = df_class[df_class['classification'] == top_label]['query'].astype(str)

                                # Extract words
                                all_words = []
                                for query in label_queries:
                                    words = re.findall(r'\b[a-z]{3,}\b', query.lower())
                                    all_words.extend([w for w in words if w not in stop_words])

                                if all_words:
                                    word_counts = Counter(all_words).most_common(5)

                                    # Horizontal bar chart
                                    fig_keywords = go.Figure(data=[go.Bar(
                                        y=[w[0] for w in word_counts],
                                        x=[w[1] for w in word_counts],
                                        orientation='h',
                                        marker_color='darkblue',
                                        text=[w[1] for w in word_counts],
                                        textposition='auto'
                                    )])
                                    fig_keywords.update_layout(
                                        title=f"Top 5 Keywords: {top_label}",
                                        xaxis_title="Frequency",
                                        yaxis_title="",
                                        height=200,
                                        margin=dict(l=20, r=20, t=40, b=20)
                                    )
                                    st.plotly_chart(fig_keywords, use_container_width=True, config=PLOTLY_CONFIG)
                                else:
                                    st.info("No keywords extracted")

                        else:
                            st.info("Dataset is empty or missing required columns")
                    except Exception as e:
                        st.warning(f"Error analyzing Classification dataset: {e}")
                else:
                    st.info("Classification dataset not found")

            # --- Column 2: Tool/Agent Sequence Dataset Quality ---
            with qm_col2:
                st.markdown("#### 🔧 Tool/Agent Sequence")

                if os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
                    try:
                        df_tool = pd.read_csv(TOOL_SEQUENCE_DATASET_PATH)

                        if not df_tool.empty and 'expected_sequence' in df_tool.columns:
                            # Calculate average sequence length (0-10 scale)
                            avg_seq_length = df_tool['expected_sequence'].astype(str).str.split(',').str.len().mean()
                            # Normalize to 0-100 scale (assume max 10 steps)
                            balance_score = min((avg_seq_length / 10) * 100, 100) if avg_seq_length > 0 else 0

                            # Gauge chart
                            fig_gauge_tool = go.Figure(go.Indicator(
                                mode="gauge+number",
                                value=balance_score,
                                title={'text': "Complexity Score"},
                                gauge={
                                    'axis': {'range': [0, 100]},
                                    'bar': {'color': "darkgreen"},
                                    'steps': [
                                        {'range': [0, 50], 'color': "lightgray"},
                                        {'range': [50, 80], 'color': "lightyellow"},
                                        {'range': [80, 100], 'color': "lightgreen"}
                                    ],
                                    'threshold': {
                                        'line': {'color': "red", 'width': 4},
                                        'thickness': 0.75,
                                        'value': 90
                                    }
                                }
                            ))
                            fig_gauge_tool.update_layout(height=250, margin=dict(l=20, r=20, t=40, b=20))
                            st.plotly_chart(fig_gauge_tool, use_container_width=True, config=PLOTLY_CONFIG)

                            # Metrics below gauge
                            metric_col1, metric_col2 = st.columns(2)
                            with metric_col1:
                                diversity = df_tool['expected_sequence'].nunique()
                                st.metric("Unique Sequences", f"{diversity}")
                            with metric_col2:
                                completeness = (1 - df_tool.isnull().sum().sum() / df_tool.size) * 100 if df_tool.size > 0 else 0
                                st.metric("Completeness", f"{completeness:.1f}%")

                            # Sequence length distribution
                            st.markdown("**Sequence Length Distribution**")
                            seq_lengths = df_tool['expected_sequence'].astype(str).str.split(',').str.len()

                            fig_seq_dist = go.Figure(data=[go.Histogram(
                                x=seq_lengths,
                                marker_color='darkgreen',
                                nbinsx=10
                            )])
                            fig_seq_dist.update_layout(
                                title="Steps per Sequence",
                                xaxis_title="Number of Steps",
                                yaxis_title="Frequency",
                                height=200,
                                margin=dict(l=20, r=20, t=40, b=20)
                            )
                            st.plotly_chart(fig_seq_dist, use_container_width=True, config=PLOTLY_CONFIG)

                        else:
                            st.info("Dataset is empty or missing required columns")
                    except Exception as e:
                        st.warning(f"Error analyzing Tool/Agent dataset: {e}")
                else:
                    st.info("Tool/Agent dataset not found")

            # --- Column 3: Context Pruning Dataset Quality ---
            with qm_col3:
                st.markdown("#### ✂️ Context Pruning")

                if os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
                    try:
                        df_prune = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH)

                        if not df_prune.empty and 'expected_action' in df_prune.columns:
                            # Calculate action balance ratio
                            action_counts = df_prune['expected_action'].value_counts()
                            if len(action_counts) > 1:
                                balance_score = (action_counts.min() / action_counts.max() * 100) if action_counts.max() > 0 else 0
                            else:
                                balance_score = 100

                            # Gauge chart
                            fig_gauge_prune = go.Figure(go.Indicator(
                                mode="gauge+number",
                                value=balance_score,
                                title={'text': "Action Balance"},
                                gauge={
                                    'axis': {'range': [0, 100]},
                                    'bar': {'color': "darkorange"},
                                    'steps': [
                                        {'range': [0, 50], 'color': "lightgray"},
                                        {'range': [50, 80], 'color': "lightyellow"},
                                        {'range': [80, 100], 'color': "lightgreen"}
                                    ],
                                    'threshold': {
                                        'line': {'color': "red", 'width': 4},
                                        'thickness': 0.75,
                                        'value': 90
                                    }
                                }
                            ))
                            fig_gauge_prune.update_layout(height=250, margin=dict(l=20, r=20, t=40, b=20))
                            st.plotly_chart(fig_gauge_prune, use_container_width=True, config=PLOTLY_CONFIG)

                            # Metrics below gauge
                            metric_col1, metric_col2 = st.columns(2)
                            with metric_col1:
                                diversity = len(action_counts)
                                st.metric("Action Types", f"{diversity}")
                            with metric_col2:
                                completeness = (1 - df_prune.isnull().sum().sum() / df_prune.size) * 100 if df_prune.size > 0 else 0
                                st.metric("Completeness", f"{completeness:.1f}%")

                            # Action distribution
                            st.markdown("**Action Distribution**")

                            fig_actions = go.Figure(data=[go.Bar(
                                y=action_counts.index.tolist(),
                                x=action_counts.values.tolist(),
                                orientation='h',
                                marker_color='darkorange',
                                text=action_counts.values.tolist(),
                                textposition='auto'
                            )])
                            fig_actions.update_layout(
                                title="Actions by Type",
                                xaxis_title="Count",
                                yaxis_title="",
                                height=200,
                                margin=dict(l=20, r=20, t=40, b=20)
                            )
                            st.plotly_chart(fig_actions, use_container_width=True, config=PLOTLY_CONFIG)

                        else:
                            st.info("Dataset is empty or missing required columns")
                    except Exception as e:
                        st.warning(f"Error analyzing Context Pruning dataset: {e}")
                else:
                    st.info("Context Pruning dataset not found")

        # Tab 4: Token Economics
        with analytics_tabs[3]:
            st.markdown("### Token Economics Dashboard")

            # Model pricing reference (per 1M tokens)
            model_prices = {
                'gpt-5-mini': {'input': 0.25, 'output': 2.0, 'display': 'GPT-5 Mini'},
                'gpt-5': {'input': 1.25, 'output': 10.0, 'display': 'GPT-5'},
                'gemini-2.5-flash': {'input': 0.30, 'output': 2.50, 'display': 'Gemini 2.5 Flash'}
            }

            # Collect token data from all datasets
            dataset_token_data = []

            # Helper function to estimate tokens (rough approximation: chars / 4)
            def estimate_tokens(text_series):
                return (text_series.astype(str).str.len() / 4).sum()

            # Classification dataset
            if os.path.exists(CLASSIFICATION_DATASET_PATH):
                try:
                    df_class = pd.read_csv(CLASSIFICATION_DATASET_PATH)
                    if not df_class.empty and 'query' in df_class.columns:
                        input_tokens = estimate_tokens(df_class['query'])
                        # Assume output is classification label (small)
                        output_tokens = len(df_class) * 5  # ~5 tokens per classification
                        dataset_token_data.append({
                            'dataset': 'Classification',
                            'input_tokens': input_tokens,
                            'output_tokens': output_tokens,
                            'total_tokens': input_tokens + output_tokens,
                            'items': len(df_class)
                        })
                except Exception as e:
                    st.warning(f"Error processing Classification dataset: {e}")

            # Tool/Agent Sequence dataset
            if os.path.exists(TOOL_SEQUENCE_DATASET_PATH):
                try:
                    df_tool = pd.read_csv(TOOL_SEQUENCE_DATASET_PATH)
                    if not df_tool.empty and 'query' in df_tool.columns:
                        input_tokens = estimate_tokens(df_tool['query'])
                        # Assume output is sequence (medium)
                        if 'expected_sequence' in df_tool.columns:
                            output_tokens = estimate_tokens(df_tool['expected_sequence'])
                        else:
                            output_tokens = len(df_tool) * 20  # ~20 tokens per sequence
                        dataset_token_data.append({
                            'dataset': 'Tool/Agent Sequence',
                            'input_tokens': input_tokens,
                            'output_tokens': output_tokens,
                            'total_tokens': input_tokens + output_tokens,
                            'items': len(df_tool)
                        })
                except Exception as e:
                    st.warning(f"Error processing Tool/Agent dataset: {e}")

            # Context Pruning dataset
            if os.path.exists(CONTEXT_PRUNING_DATASET_PATH):
                try:
                    df_prune = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH)
                    if not df_prune.empty:
                        # Input includes conversation history + new question
                        input_tokens = 0
                        if 'conversation_history' in df_prune.columns:
                            input_tokens += estimate_tokens(df_prune['conversation_history'])
                        if 'new_question' in df_prune.columns:
                            input_tokens += estimate_tokens(df_prune['new_question'])
                        # Output is action decision (small)
                        output_tokens = len(df_prune) * 10  # ~10 tokens per action
                        dataset_token_data.append({
                            'dataset': 'Context Pruning',
                            'input_tokens': input_tokens,
                            'output_tokens': output_tokens,
                            'total_tokens': input_tokens + output_tokens,
                            'items': len(df_prune)
                        })
                except Exception as e:
                    st.warning(f"Error processing Context Pruning dataset: {e}")

            if dataset_token_data:
                # 1. Violin plot: Token distribution comparison
                st.markdown("#### Token Distribution Comparison")

                # Prepare data for violin plot
                violin_data = []
                for data in dataset_token_data:
                    # Create distribution (approximate per-item tokens)
                    tokens_per_item = data['total_tokens'] / data['items'] if data['items'] > 0 else 0
                    # Simulate distribution around mean (for visualization)
                    for _ in range(min(int(data['items']), 100)):  # Cap at 100 for performance
                        violin_data.append({
                            'Dataset': data['dataset'],
                            'Tokens': tokens_per_item
                        })

                if violin_data:
                    df_violin = pd.DataFrame(violin_data)

                    fig_violin = go.Figure()

                    for dataset_name in df_violin['Dataset'].unique():
                        dataset_tokens = df_violin[df_violin['Dataset'] == dataset_name]['Tokens']
                        fig_violin.add_trace(go.Violin(
                            y=dataset_tokens,
                            name=dataset_name,
                            box_visible=True,
                            meanline_visible=True
                        ))

                    fig_violin.update_layout(
                        title="Token Distribution by Dataset",
                        yaxis_title="Tokens per Item",
                        height=350,
                        margin=dict(l=20, r=20, t=40, b=40)
                    )
                    st.plotly_chart(fig_violin, use_container_width=True, config=PLOTLY_CONFIG)

                # 2. Cost Calculator
                st.markdown("#### Cost Calculator")

                selected_model = st.selectbox(
                    "Select Model for Cost Estimation",
                    options=list(model_prices.keys()),
                    format_func=lambda x: model_prices[x]['display']
                )

                if selected_model:
                    pricing = model_prices[selected_model]

                    # Calculate costs for each dataset
                    cost_data = []
                    for data in dataset_token_data:
                        input_cost = (data['input_tokens'] / 1_000_000) * pricing['input']
                        output_cost = (data['output_tokens'] / 1_000_000) * pricing['output']
                        total_cost = input_cost + output_cost

                        cost_data.append({
                            'Dataset': data['dataset'],
                            'Input Tokens': f"{data['input_tokens']:,.0f}",
                            'Output Tokens': f"{data['output_tokens']:,.0f}",
                            'Input Cost': f"${input_cost:.4f}",
                            'Output Cost': f"${output_cost:.4f}",
                            'Total Cost': f"${total_cost:.4f}",
                            'total_cost_numeric': total_cost
                        })

                    # Display cost table
                    cost_df = pd.DataFrame(cost_data)
                    display_df = cost_df.drop(columns=['total_cost_numeric'])

                    st.dataframe(display_df, use_container_width=True)

                    # Total summary
                    total_cost_all = sum([c['total_cost_numeric'] for c in cost_data])
                    st.metric("**Total Estimated Cost**", f"${total_cost_all:.4f}",
                             help=f"Total cost for all datasets using {model_prices[selected_model]['display']}")

                    # 3. Bar chart: Total estimated costs by dataset
                    st.markdown("#### Cost Breakdown by Dataset")

                    fig_costs = go.Figure(data=[go.Bar(
                        x=[c['Dataset'] for c in cost_data],
                        y=[c['total_cost_numeric'] for c in cost_data],
                        text=[f"${c['total_cost_numeric']:.4f}" for c in cost_data],
                        textposition='auto',
                        marker_color=['#1f77b4', '#ff7f0e', '#2ca02c'][:len(cost_data)],
                        hovertemplate='<b>%{x}</b><br>Total Cost: $%{y:.4f}<extra></extra>'
                    )])

                    fig_costs.update_layout(
                        title=f"Estimated Costs by Dataset ({model_prices[selected_model]['display']})",
                        xaxis_title="Dataset",
                        yaxis_title="Total Cost (USD)",
                        height=350,
                        margin=dict(l=20, r=20, t=40, b=40)
                    )
                    st.plotly_chart(fig_costs, use_container_width=True, config=PLOTLY_CONFIG)

                    # 4. Input vs Output cost comparison
                    st.markdown("#### Input vs Output Cost Analysis")

                    fig_io_costs = go.Figure()

                    datasets = [c['Dataset'] for c in cost_data]
                    input_costs = [(float(c['Input Cost'].replace('$', ''))) for c in cost_data]
                    output_costs = [(float(c['Output Cost'].replace('$', ''))) for c in cost_data]

                    fig_io_costs.add_trace(go.Bar(
                        name='Input Cost',
                        x=datasets,
                        y=input_costs,
                        marker_color='lightblue',
                        text=[f"${v:.4f}" for v in input_costs],
                        textposition='auto'
                    ))

                    fig_io_costs.add_trace(go.Bar(
                        name='Output Cost',
                        x=datasets,
                        y=output_costs,
                        marker_color='lightcoral',
                        text=[f"${v:.4f}" for v in output_costs],
                        textposition='auto'
                    ))

                    fig_io_costs.update_layout(
                        title="Input vs Output Costs",
                        xaxis_title="Dataset",
                        yaxis_title="Cost (USD)",
                        barmode='group',
                        height=300,
                        margin=dict(l=20, r=20, t=40, b=40)
                    )
                    st.plotly_chart(fig_io_costs, use_container_width=True, config=PLOTLY_CONFIG)

            else:
                st.info("No datasets available for token economics analysis. Generate datasets to see cost estimates.")

    # Define use case configurations
    USE_CASE_CONFIGS = {
        "🚀 Development (Fast)": {
            "description": "Quick iteration, prompt engineering, initial testing",
            "total_items": 200,
            "time_estimate": "2-5 minutes",
            "cost_estimate": "$0.10-0.30",
            "statistical_power": "Low-Medium",
            "config": [
                ("Classification", 100, CLASSIFICATION_DATASET_PATH),
                ("Tool/Agent Sequence", 50, TOOL_SEQUENCE_DATASET_PATH),
                ("Context Pruning", 50, CONTEXT_PRUNING_DATASET_PATH)
            ]
        },
        "🧪 Testing (Recommended)": {
            "description": "Pre-production validation, reliable metrics, stakeholder demos",
            "total_items": 550,
            "time_estimate": "10-15 minutes",
            "cost_estimate": "$0.25-0.75",
            "statistical_power": "Medium-High",
            "config": [
                ("Classification", 300, CLASSIFICATION_DATASET_PATH),
                ("Tool/Agent Sequence", 150, TOOL_SEQUENCE_DATASET_PATH),
                ("Context Pruning", 100, CONTEXT_PRUNING_DATASET_PATH)
            ]
        },
        "🏭 Production (Robust)": {
            "description": "Final validation, regulatory compliance, production deployment",
            "total_items": 950,
            "time_estimate": "20-40 minutes",
            "cost_estimate": "$0.50-1.50",
            "statistical_power": "High",
            "config": [
                ("Classification", 500, CLASSIFICATION_DATASET_PATH),
                ("Tool/Agent Sequence", 250, TOOL_SEQUENCE_DATASET_PATH),
                ("Context Pruning", 200, CONTEXT_PRUNING_DATASET_PATH)
            ]
        }
    }

    # Use case selector
    selected_use_case = st.selectbox(
        "Select Use Case",
        options=list(USE_CASE_CONFIGS.keys()),
        index=0,  # Default to Development
        help="Choose based on your testing needs and time constraints"
    )

    # Display use case details
    use_case_info = USE_CASE_CONFIGS[selected_use_case]

    col_info1, col_info2, col_info3, col_info4 = st.columns(4)
    with col_info1:
        st.metric("Total Items", use_case_info["total_items"])
    with col_info2:
        st.metric("Time Estimate", use_case_info["time_estimate"])
    with col_info3:
        st.metric("Est. Cost", use_case_info["cost_estimate"])
    with col_info4:
        st.metric("Statistical Power", use_case_info["statistical_power"])

    st.info(f"**Use for:** {use_case_info['description']}")

    # Show breakdown
    with st.expander("📊 Dataset Breakdown"):
        for data_type, size, _ in use_case_info["config"]:
            st.write(f"- **{data_type}**: {size} items")

    # --- Quick Generate: Choose model used for all datasets ---
    default_quick_model = "openai/gpt-5-mini"
    if default_quick_model not in AVAILABLE_MODELS:
        default_quick_model = AVAILABLE_MODELS[0] if AVAILABLE_MODELS else OPENAI_MODEL
    default_quick_index = AVAILABLE_MODELS.index(default_quick_model) if default_quick_model in AVAILABLE_MODELS else 0

    quick_gen_model = st.selectbox(
        "Generation Model (Quick Generate)",
        options=AVAILABLE_MODELS,
        index=default_quick_index,
        help="Model used for Quick Generate across all selected datasets."
    )
    st.session_state["quick_gen_model"] = quick_gen_model
    st.caption(f"Using model: {quick_gen_model} via {API_ROUTING_MODE}")

    # Generate button
    if st.button(f"🔄 Generate All Datasets ({selected_use_case})", use_container_width=True, type="primary"):
        try:
            datasets_config = use_case_info["config"]
            quick_gen_model = st.session_state.get("quick_gen_model", quick_gen_model)
            # Snapshot totals for whole batch and prepare meta collection
            ct0 = st.session_state.cost_tracker if "cost_tracker" in st.session_state else None
            batch_pre_totals = dict(ct0.totals) if ct0 else {}
            batch_pre_calls = len(ct0.by_call) if ct0 else 0
            meta_files: List[str] = []

            all_success = True

            # Show progress
            progress_bar = st.progress(0)
            status_text = st.empty()

            total_datasets = len(datasets_config)

            for idx, (data_type, size, path) in enumerate(datasets_config):
                # Snapshot cost totals before this dataset
                pre_totals = dict(st.session_state.cost_tracker.totals) if "cost_tracker" in st.session_state else {}
                pre_calls = len(st.session_state.cost_tracker.by_call) if "cost_tracker" in st.session_state else 0

                status_text.text(f"Generating {data_type} dataset ({size} items)... [{idx+1}/{total_datasets}]")
                progress_bar.progress((idx) / total_datasets)

                try:
                    prompt = DEFAULT_DATASET_PROMPTS[data_type]
                    df = asyncio.run(generate_synthetic_data(prompt, size, data_type, quick_gen_model))

                    if not df.empty:
                        df.to_csv(path, index=False)
                        # Write sidecar meta for traceability
                        # Compute cost deltas for this dataset
                        try:
                            ct = st.session_state.cost_tracker
                            delta_usage = {
                                "prompt_tokens": max(ct.totals.get("prompt_tokens", 0) - pre_totals.get("prompt_tokens", 0), 0),
                                "completion_tokens": max(ct.totals.get("completion_tokens", 0) - pre_totals.get("completion_tokens", 0), 0),
                                "total_tokens": max(ct.totals.get("total_tokens", 0) - pre_totals.get("total_tokens", 0), 0),
                            }
                            delta_cost = {
                                "input_cost_usd": round(max(ct.totals.get("input_cost_usd", 0.0) - pre_totals.get("input_cost_usd", 0.0), 0.0), 4),
                                "output_cost_usd": round(max(ct.totals.get("output_cost_usd", 0.0) - pre_totals.get("output_cost_usd", 0.0), 0.0), 4),
                                "total_cost_usd": round(max(ct.totals.get("total_cost_usd", 0.0) - pre_totals.get("total_cost_usd", 0.0), 0.0), 4),
                            }
                            delta_calls = max(len(ct.by_call) - pre_calls, 0)
                        except Exception:
                            delta_usage, delta_cost, delta_calls = {}, {}, 0

                        try:
                            meta_path = path.replace(".csv", ".meta.json")
                            with open(meta_path, "w", encoding="utf-8") as f:
                                json.dump({
                                    "model": quick_gen_model,
                                    "routing_mode": API_ROUTING_MODE,
                                    "when": pd.Timestamp.utcnow().isoformat(),
                                    "dataset_type": data_type,
                                    "size": size,
                                    "use_case": selected_use_case,
                                    "calls": delta_calls,
                                    "usage": delta_usage,
                                    "cost": delta_cost,
                                }, f, indent=2)
                            meta_files.append(meta_path)
                        except Exception:
                            pass
                        st.success(f"✅ Generated {data_type} dataset ({len(df)} rows) using {quick_gen_model} [{API_ROUTING_MODE}]")
                    else:
                        st.warning(f"⚠️ Failed to generate {data_type} dataset")
                        all_success = False
                except Exception as e:
                    st.error(f"Error generating {data_type} dataset: {e}")
                    all_success = False

            progress_bar.progress(1.0)
            status_text.text("Generation complete!")

            if all_success:
                # Reload datasets into session state
                st.session_state.df = load_classification_dataset()
                st.session_state.agent_df = load_tool_sequence_dataset()
                st.session_state.pruning_df = load_context_pruning_dataset()

                # Compute batch deltas for tokens, cost, and calls
                try:
                    ct = st.session_state.cost_tracker
                    d_tokens = max(ct.totals.get("total_tokens", 0) - batch_pre_totals.get("total_tokens", 0), 0)
                    d_cost = round(max(ct.totals.get("total_cost_usd", 0.0) - batch_pre_totals.get("total_cost_usd", 0.0), 0.0), 4)
                    d_calls = max(len(ct.by_call) - batch_pre_calls, 0)
                except Exception:
                    d_tokens, d_cost, d_calls = 0, 0.0, 0

                # Persist meta file list and summary for the expander/history
                st.session_state["last_generated_meta_files"] = meta_files
                summary = {
                    "when": pd.Timestamp.utcnow().isoformat(),
                    "use_case": selected_use_case,
                    "model": quick_gen_model,
                    "routing": API_ROUTING_MODE,
                    "total_items": use_case_info['total_items'],
                    "tokens": d_tokens,
                    "cost_usd": d_cost,
                    "calls": d_calls,
                    "meta_files": meta_files,
                }
                st.session_state["last_generation_summary"] = summary
                if "generation_history" not in st.session_state:
                    st.session_state["generation_history"] = []
                st.session_state["generation_history"].append(summary)

                st.success(f"✅ All datasets generated successfully! Total: {use_case_info['total_items']} items using {quick_gen_model} [{API_ROUTING_MODE}] • Tokens: +{d_tokens:,} • Cost: +${d_cost:.4f} • Calls: +{d_calls}")
                st.balloons()
                st.rerun()
        except Exception as e:
            st.error(f"Failed to generate datasets: {e}")

    # Compact expander to view last generation metadata
    if st.session_state.get("last_generated_meta_files"):
        with st.expander("🧾 View last generation metadata"):
            files = st.session_state.get("last_generated_meta_files", [])
            # Compact summary table
            rows = []
            for p in files:
                try:
                    with open(p, "r", encoding="utf-8") as f:
                        meta = json.load(f)
                        rows.append({
                            "dataset_type": meta.get("dataset_type"),
                            "size": meta.get("size"),
                            "total_tokens": ((meta.get("usage") or {}).get("total_tokens")),
                            "total_cost_usd": ((meta.get("cost") or {}).get("total_cost_usd")),
                        })
                except Exception:
                    pass
            if rows:
                try:
                    st.dataframe(pd.DataFrame(rows), use_container_width=True, height=min(300, 60+28*len(rows)))
                except Exception:
                    st.write(rows)
            # Raw JSON entries
            for p in files:
                try:
                    st.markdown(f"**{os.path.basename(p)}**")
                    with open(p, "r", encoding="utf-8") as f:
                        st.json(json.load(f))
                except Exception as e:
                    st.text(f"{p}: {e}")

    # History expander
    if st.session_state.get("generation_history"):
        with st.expander("🗂️ Generation history"):
            hist = st.session_state.get("generation_history", [])
            try:
                dfh = pd.DataFrame(hist)
                st.dataframe(dfh[["when","use_case","model","routing","total_items","tokens","cost_usd","calls"]].sort_values("when", ascending=False), use_container_width=True)
            except Exception:
                st.write(hist)

    st.divider()

    # --- PATCH 20: Restructure Tab 0 UI for dynamic prompt generation ---
    # Initialize prompt key
    if 'generation_prompt_text' not in st.session_state:
        st.session_state['generation_prompt_text'] = SUGGESTED_PROMPTS["Classification"][0]

    st.subheader("🎯 Generate Individual Dataset")
    c_model, c_size, c_type = st.columns(3)

    with c_model:
        # --- UPDATED: Flexible model selection from all available models ---
        default_gen_model = "openai/gpt-5-mini"
        if default_gen_model not in AVAILABLE_MODELS:
            default_gen_model = AVAILABLE_MODELS[0] if AVAILABLE_MODELS else OPENAI_MODEL

        default_index = AVAILABLE_MODELS.index(default_gen_model) if default_gen_model in AVAILABLE_MODELS else 0

        gen_model = st.selectbox(
            "Generation Model",
            options=AVAILABLE_MODELS,
            index=default_index,
            help="Select any model to generate synthetic data. Defaults to gpt-5-mini."
        )

    with c_size:
        size_options = {5: "5 Pairs", 25: "25 Pairs", 100: "100 Pairs", 1000: "1000 Pairs"}
        data_size_label = st.selectbox("Dataset Size", options=list(size_options.values()), index=2)
        data_size = list(size_options.keys())[list(size_options.values()).index(data_size_label)]

    with c_type:
        # --- PATCH 17: Added "Context Pruning" option ---
        data_type = st.selectbox("Data Type", ["Classification", "Tool/Agent Sequence", "Context Pruning"])

    # Define the helper function for random prompt selection (user-input-aware)
    def set_random_prompt(current_data_type):
        """Generate random prompt that's aware of user's existing input."""
        import random

        prompts = SUGGESTED_PROMPTS.get(current_data_type, [])
        if not prompts:
            st.session_state['generation_prompt_text'] = f"Generate synthetic data for the '{current_data_type}' task."
            return

        # Get current user input
        current_input = st.session_state.get('generation_prompt_text', '')

        # Select base prompt
        base_prompt = random.choice(prompts)

        # Build context
        context = {
            'dataset_size': st.session_state.get('data_size', 100),
            'columns': ['query', 'classification'] if current_data_type == 'Classification' else ['query', 'expected_sequence'],
            'suggested_approach': base_prompt.split('\n')[0] if '\n' in base_prompt else base_prompt[:100],
            'success_criteria': '85%+ accuracy on test set'
        }

        # Enhance with user input
        enhanced = enhance_prompt_with_user_input(base_prompt, current_input, context)

        st.session_state['generation_prompt_text'] = enhanced

    # 1. Random Prompt Generator Button
    if st.button(f"🎲 Randomly Generate Prompt for {data_type}", use_container_width=True):
        set_random_prompt(data_type)
        st.rerun() # Rerun to update the text area immediately

    # 2. Text Area (linked to session state)
    st.text_area(
        "Describe the dataset requirements:",
        height=150,
        key='generation_prompt_text',
        help="Provide a detailed description of the type of queries, the required complexity, and the target labels/sequences."
    )

    # 3. Suggestion Pills
    st.markdown("##### Suggested Starting Points (Click to use):")
    selected_prompts = SUGGESTED_PROMPTS.get(data_type, [])

    if selected_prompts:
        cols = st.columns(len(selected_prompts))
        for i, prompt in enumerate(selected_prompts):
            with cols[i]:
                if st.button(prompt[:30] + "...", key=f"pill_{data_type}_{i}", help=prompt, use_container_width=True):
                    st.session_state['generation_prompt_text'] = prompt
                    st.rerun() # Rerun to update the text area
    else:
        st.info("No suggestions available for this data type.")

    # 4. Execution Button (reads from session state)
    if st.button("Generate & Save Dataset", use_container_width=True):
        generation_prompt_used = st.session_state.get('generation_prompt_text', "").strip()
        if not generation_prompt_used:
            st.warning("Please provide a dataset description prompt.")
        else:
            # Need to run generation synchronously in Streamlit context
            with st.spinner(f"Generating {data_size} {data_type} items..."):
                new_df = asyncio.run(generate_synthetic_data(generation_prompt_used, data_size, data_type, gen_model))

            if not new_df.empty:

                # Check if we are generating for the classification tests (Test 1-3)
                if data_type == "Classification" and "classification" in new_df.columns:
                    # Re-initialize the dataframe with all necessary classification skeleton columns
                    st.session_state.df = new_df[["query", "classification"]].copy()
                    for col in SKELETON_COLUMNS:
                        if col not in st.session_state.df.columns:
                            st.session_state.df[col] = None

                    # Save to file
                    save_dataset_to_file(new_df[["query", "classification"]], data_type, model_used=gen_model, routing_mode=API_ROUTING_MODE)
                    st.success(f"Generated {len(new_df)} classification items using {gen_model} [{API_ROUTING_MODE}] and updated the main dataset (Tests 1-3).")

                    # Visualize generated dataset
                    st.subheader("📊 Generated Dataset Analysis")
                    visualize_dataset_composition(new_df, dataset_type=data_type)
                    st.dataframe(new_df.head(10), use_container_width=True)

                # If it's an Agent test set, save it separately/use a different structure
                elif data_type == "Tool/Agent Sequence" and "expected_sequence" in new_df.columns:
                    # Save agent data to a specific session state key
                    st.session_state.agent_df = new_df[["query", "expected_sequence"]].copy()

                    # Save to file
                    save_dataset_to_file(new_df, data_type, model_used=gen_model, routing_mode=API_ROUTING_MODE)
                    st.success(f"Generated {len(new_df)} agent sequence items using {gen_model} [{API_ROUTING_MODE}]. Ready for Test 5.")

                    # Visualize generated dataset
                    st.subheader("📊 Generated Dataset Analysis")
                    visualize_dataset_composition(new_df, dataset_type=data_type)
                    st.dataframe(new_df.head(10), use_container_width=True)

                # --- PATCH 17: Save Pruning Data to session state ---
                elif data_type == "Context Pruning" and "new_question" in new_df.columns:
                    # Test 4 requires specific columns defined in PruningDataItem
                    required_cols = list(PruningDataItem.model_fields.keys())
                    st.session_state.pruning_df = new_df[[c for c in required_cols if c in new_df.columns]].copy()

                    # Save to file
                    save_dataset_to_file(new_df, data_type, model_used=gen_model, routing_mode=API_ROUTING_MODE)
                    st.success(f"Generated {len(new_df)} context pruning cases using {gen_model} [{API_ROUTING_MODE}]. Ready for Test 4.")

                    # Visualize generated dataset
                    st.subheader("📊 Generated Dataset Analysis")
                    visualize_dataset_composition(new_df, dataset_type=data_type)
                    st.dataframe(new_df.head(10), use_container_width=True)
                # ----------------------------------------------------

                st.dataframe(new_df, use_container_width=True)

                # --- Force rerun to update the Dataset Status display ---
                st.rerun()
            else:
                st.error("Data generation failed to produce valid output.")
    # -------------------------------------------------------------


# ---------- Test 1 ----------
with tabs[1]:
    # Dashboard header
    render_test_flow_diagram(1, "Test 1: Two-Model Classification + F1/Latency Analysis")

    # Dataset preview for classification tests (Tests 1-3)
    st.subheader("Dataset Preview (Classification)")
    _df_preview = st.session_state.df if "df" in st.session_state else pd.DataFrame(columns=SKELETON_COLUMNS)

    # Show dataset source
    if os.path.exists(CLASSIFICATION_DATASET_PATH):
        dataset_source = f"📁 Source: `{CLASSIFICATION_DATASET_PATH}` ({len(_df_preview)} rows)"
    else:
        dataset_source = "⚠️ No dataset loaded. Generate datasets in the Preparation tab."

    st.caption(dataset_source)
    st.dataframe(_style_selected_rows(_df_preview, ROW_LIMIT_N), use_container_width=True, height=250)
    st.caption(f"Highlighted {len(_subset_for_run(_df_preview, ROW_LIMIT_N))} of {len(_df_preview)} rows will be used in Tests 1-3.")

    st.divider()

    # --- Per-Test Model Selection (Test 1) ---
    st.subheader("Model Selection (Test 1)")
    t1_col1, t1_col2 = st.columns(2)
    with t1_col1:
        t1_use_ollama = st.checkbox("Use OpenRouter (Test 1)", value=True, key="t1_use_ollama")
        _t1_or_ids = list(OPENROUTER_MODEL_METADATA.keys())
        _t1_or_default = _t1_or_ids.index(OPENROUTER_MODEL) if OPENROUTER_MODEL in _t1_or_ids else 0
        t1_openrouter_model = st.selectbox("OpenRouter model (Test 1)", options=_t1_or_ids, index=_t1_or_default, key="t1_or_model")
    with t1_col2:
        t1_use_openai = st.checkbox("Use OpenAI (Test 1)", value=True, key="t1_use_openai")
        _t1_oai_ids = list(OPENAI_MODEL_METADATA.keys())
        _t1_oai_default = _t1_oai_ids.index(OPENAI_MODEL) if OPENAI_MODEL in _t1_oai_ids else 0
        t1_openai_model = st.selectbox("OpenAI model (Test 1)", options=_t1_oai_ids, index=_t1_oai_default, key="t1_oai_model")

    # Collapsible configuration section
    with st.expander("⚙️ Test Configuration", expanded=False):
        row_limit_display = [k for k, v in ROW_LIMIT_OPTIONS.items() if v == ROW_LIMIT_N]
        row_limit_str = row_limit_display[0] if row_limit_display else f"{ROW_LIMIT_N} rows"
        st.markdown(f"""
        Models: OpenRouter={t1_openrouter_model if t1_use_ollama else '—'}; OpenAI={t1_openai_model if t1_use_openai else '—'}
        Row Limit: {row_limit_str}
        Providers Enabled: OpenRouter={t1_use_ollama}, OpenAI={t1_use_openai}
        Explain Confusion Matrix: {explain_cm}
        """)

    st.divider()

    up = st.file_uploader("Upload CSV (query, classification)", type=["csv"], key="t1_up")
    if st.button("Load uploaded (replace DF)", key="t1_load") and up:
        # ... (loading logic unchanged) ...
        st.rerun()

    if st.button("▶️ Run Test 1", type="primary", use_container_width=True):
        # Persist per-test OpenAI model override for this run
        st.session_state['openai_model_override'] = t1_openai_model

        overrides = {
            'use_openai': t1_use_openai,
            'openai_model': t1_openai_model,
            'use_ollama': t1_use_ollama,
            'openrouter_model': t1_openrouter_model,
        }
        capture_run_config("Test 1", overrides)  # CAPTURE

        run_classification_flow(
            include_third_model=False,
            use_openai_override=t1_use_openai,
            use_ollama_override=t1_use_ollama,
            openrouter_model_override=t1_openrouter_model,
        )

        # The analysis code below remains the same, but now runs on fresh data
        df = _subset_for_run(st.session_state.df, ROW_LIMIT_N)

        if len(df) and "classification" in df.columns:
            # 1. FIRST: Progress replay (what just happened)
            st.subheader("📈 Processing Timeline")
            render_progress_replay("classification")

            st.divider()

            # 2. THEN: Organized results with subtabs
            st.subheader("📊 Test Results")
            render_organized_results(
                df,
                test_type="classification",
                model_cols=["openrouter_mistral", "openai"],
                model_names=["Mistral (OpenRouter)", "OpenAI"]
            )

            st.divider()

        # --- Save results at the end of the test run ---
        save_results_df(st.session_state.df, "Test 1", ROW_LIMIT_N)
        # --- PATCH 7: Structured Summary Call for Test 1 ---
        if len(df) and "classification" in df.columns:
            report_text = f"Test 1 Results (N={len(df)}): Mistral Avg Latency: {df['latency_openrouter_mistral'].mean():.2f}s, OpenAI Avg Latency: {df['latency_openai'].mean():.2f}s. Performance reports calculated."
            loop = asyncio.get_event_loop()
            loop.run_until_complete(display_final_summary_for_test("Test 1 Classification", report_text))
        # ----------------------------------------------------

# ---------- Test 2 ----------
with tabs[2]:
    # Dashboard header
    render_test_flow_diagram(2, "Test 2: Advanced Ensembling with Per-Class F1 Weighting")


    # --- Per-Test Model Selection (Test 2) ---
    st.subheader("Model Selection (Test 2)")
    t2_c1, t2_c2 = st.columns(2)
    with t2_c1:
        t2_use_ollama = st.checkbox("Use OpenRouter (Test 2)", value=True, key="t2_use_ollama")
        _t2_or_ids = list(OPENROUTER_MODEL_METADATA.keys())
        _t2_or_default = _t2_or_ids.index(OPENROUTER_MODEL) if OPENROUTER_MODEL in _t2_or_ids else 0
        t2_openrouter_model = st.selectbox("OpenRouter model (Test 2)", options=_t2_or_ids, index=_t2_or_default, key="t2_or_model")
    with t2_c2:
        t2_use_openai = st.checkbox("Use OpenAI (Test 2)", value=True, key="t2_use_openai")
        _t2_oai_ids = list(OPENAI_MODEL_METADATA.keys())
        _t2_oai_default = _t2_oai_ids.index(OPENAI_MODEL) if OPENAI_MODEL in _t2_oai_ids else 0
        t2_openai_model = st.selectbox("OpenAI model (Test 2)", options=_t2_oai_ids, index=_t2_oai_default, key="t2_oai_model")

    t2_third_kind = st.selectbox("Third model provider (Test 2)", ["None", "OpenRouter", "OpenAI"], index=["None","OpenRouter","OpenAI"].index(THIRD_KIND if THIRD_KIND in ["None","OpenRouter","OpenAI"] else "None"), key="t2_third_kind")
    t2_third_model = ""
    if t2_third_kind == "OpenRouter":
        t2_third_model = st.selectbox("Third model (OpenRouter)", options=_t2_or_ids, key="t2_third_model_or")
    elif t2_third_kind == "OpenAI":
        t2_third_model = st.selectbox("Third model (OpenAI)", options=_t2_oai_ids, key="t2_third_model_oai")
    # Collapsible configuration section
    with st.expander("⚙️ Test Configuration", expanded=False):
        row_limit_display = [k for k, v in ROW_LIMIT_OPTIONS.items() if v == ROW_LIMIT_N]
        row_limit_str = row_limit_display[0] if row_limit_display else f"{ROW_LIMIT_N} rows"
        st.markdown(f"""
        Models: OpenRouter={t2_openrouter_model if t2_use_ollama else '—'}; OpenAI={t2_openai_model if t2_use_openai else '—'}; Third={t2_third_model if t2_third_kind != 'None' else '—'}
        Row Limit: {row_limit_str}
        Weighting Strategy: Confidence × Per-Class F1 Score
        Explain Confusion Matrix: {explain_cm}
        """)

    st.divider()
    st.info("This test uses a smarter weighting: score = confidence * F1-score-of-the-predicted-class.")


    if st.button("▶️ Run Test 2", type="primary", use_container_width=True):
        # Persist per-test OpenAI model override for this run
        st.session_state['openai_model_override'] = t2_openai_model

        overrides = {
            'use_openai': t2_use_openai,
            'openai_model': t2_openai_model,
            'use_ollama': t2_use_ollama,
            'openrouter_model': t2_openrouter_model,
            'third_kind': t2_third_kind,
            'third_model': t2_third_model,
        }
        capture_run_config("Test 2", overrides)  # CAPTURE
        # First, run classification for configured models
        run_classification_flow(
            include_third_model=(t2_third_kind != "None"),
            use_openai_override=t2_use_openai,
            use_ollama_override=t2_use_ollama,
            openrouter_model_override=t2_openrouter_model,
            third_kind_override=t2_third_kind,
            third_model_override=t2_third_model,
        )



        # First, ensure classifications exist by running Test 1 logic
        # ... (classification run logic as in Test 1) ...
        df = _subset_for_run(st.session_state.df, ROW_LIMIT_N)
        if len(df):
            # --- PATCH 13: Dynamic name for reporting ---
            third_model_name = get_third_model_display_name()

            y_true = df["classification"].tolist()
            # Generate reports including the third model (for ensemble calculation)
            report_m = generate_classification_report(y_true, df["classification_result_openrouter_mistral"].tolist(), "Mistral", explain=False)
            report_g = generate_classification_report(y_true, df["classification_result_openai"].tolist(), "OpenAI", explain=False)
            report_t = generate_classification_report(y_true, df["classification_result_third"].tolist(), third_model_name, explain=False) if "classification_result_third" in df else None
            # ---------------------------------------------

            # Create a map from class to its F1 score for each model
            f1_maps = {
                "mistral": {label: data['f1-score'] for label, data in report_m.items() if isinstance(data, dict)} if report_m else {},
                "gpt5": {label: data['f1-score'] for label, data in report_g.items() if isinstance(data, dict)} if report_g else {},
                "third": {label: data['f1-score'] for label, data in report_t.items() if isinstance(data, dict)} if report_t else {},
            }

            picks = df.apply(lambda row: _smarter_weighted_pick_row(row, f1_maps), axis=1)
            df['weighted_pick_model'], df['weighted_pick_label'] = zip(*picks)
            st.session_state.df.loc[df.index, ['weighted_pick_model', 'weighted_pick_label']] = df[['weighted_pick_model', 'weighted_pick_label']]

            # 1. FIRST: Progress replay
            st.subheader("📈 Processing Timeline")
            render_progress_replay("classification")

            st.divider()

            # 2. THEN: Organized results with subtabs
            st.subheader("📊 Test Results")
            render_organized_results(
                df,
                test_type="classification",
                model_cols=["openrouter_mistral", "openai", "third"],
                model_names=["Mistral (OpenRouter)", "OpenAI", third_model_name]
            )

            st.divider()

            # 2.5. Confidence Distribution Analysis
            st.subheader("📊 Confidence Distribution Analysis")

            conf_cols = [
                ('classification_result_openrouter_mistral_confidence', 'Mistral'),
                ('classification_result_openai_confidence', 'OpenAI'),
                ('classification_result_third_confidence', third_model_name)
            ]

            fig_conf = go.Figure()

            for col, name in conf_cols:
                if col in df.columns:
                    confidences = df[col].dropna()
                    fig_conf.add_trace(go.Violin(
                        y=confidences,
                        name=name,
                        box_visible=True,
                        meanline_visible=True,
                        hovertemplate=f'<b>{name}</b><br>Confidence: %{{y:.3f}}<extra></extra>'
                    ))

            fig_conf.update_layout(
                title="Confidence Score Distribution by Model",
                yaxis_title="Confidence Score",
                height=400,
                showlegend=True
            )

            st.plotly_chart(fig_conf, use_container_width=True, config=PLOTLY_CONFIG)

            st.divider()

            # 3. Ensemble results
            with st.expander("🎯 Ensemble Performance (Weighted Pick)", expanded=True):
                st.info("This ensemble uses: score = confidence × F1-score-of-the-predicted-class")
                generate_classification_report(y_true, df["weighted_pick_label"].tolist(), "Smarter Weighted Pick", explain=explain_cm)

            # --- Save results at the end of the test run ---
            save_results_df(st.session_state.df, "Test 2", ROW_LIMIT_N)
            # --- PATCH 7: Structured Summary Call for Test 2 ---
            report_text = f"Test 2 Ensemble Results (N={len(df)}): Weighted Pick Macro F1: {report_m.get('weighted avg', {}).get('f1-score', 0.0):.4f}. Focus analysis on model disagreement."
            loop = asyncio.get_event_loop()
            loop.run_until_complete(display_final_summary_for_test("Test 2 Advanced Ensembling", report_text))
            # ----------------------------------------------------

# ---------- Test 3 ----------
with tabs[3]:
    # Dashboard header
    render_test_flow_diagram(3, "Test 3: LLM as Judge")

    # --- Classification Models (Test 3) ---
    st.subheader("Classification Models (Test 3)")
    t3_c1, t3_c2 = st.columns(2)
    with t3_c1:
        t3_use_ollama = st.checkbox("Use OpenRouter (Test 3)", value=True, key="t3_use_ollama")
        _t3_or_ids = list(OPENROUTER_MODEL_METADATA.keys())
        _t3_or_default = _t3_or_ids.index(OPENROUTER_MODEL) if OPENROUTER_MODEL in _t3_or_ids else 0
        t3_openrouter_model = st.selectbox("OpenRouter model (Test 3)", options=_t3_or_ids, index=_t3_or_default, key="t3_or_model")
    with t3_c2:
        t3_use_openai = st.checkbox("Use OpenAI (Test 3)", value=True, key="t3_use_openai")
        _t3_oai_ids = list(OPENAI_MODEL_METADATA.keys())
        _t3_oai_default = _t3_oai_ids.index(OPENAI_MODEL) if OPENAI_MODEL in _t3_oai_ids else 0
        t3_openai_model = st.selectbox("OpenAI model (Test 3)", options=_t3_oai_ids, index=_t3_oai_default, key="t3_oai_model")

    t3_third_kind = st.selectbox("Third model provider (Test 3)", ["None", "OpenRouter", "OpenAI"], index=["None","OpenRouter","OpenAI"].index(THIRD_KIND if THIRD_KIND in ["None","OpenRouter","OpenAI"] else "None"), key="t3_third_kind")
    t3_third_model = ""
    if t3_third_kind == "OpenRouter":
        t3_third_model = st.selectbox("Third model (OpenRouter)", options=_t3_or_ids, key="t3_third_model_or")
    elif t3_third_kind == "OpenAI":
        t3_third_model = st.selectbox("Third model (OpenAI)", options=_t3_oai_ids, key="t3_third_model_oai")

    # Collapsible configuration section
    with st.expander("⚙️ Test Configuration", expanded=False):
        row_limit_display = [k for k, v in ROW_LIMIT_OPTIONS.items() if v == ROW_LIMIT_N]
        row_limit_str = row_limit_display[0] if row_limit_display else f"{ROW_LIMIT_N} rows"
        st.markdown(f"""
        Models: OpenRouter={t3_openrouter_model if t3_use_ollama else '—'}; OpenAI={t3_openai_model if t3_use_openai else '—'}; Third={t3_third_model if t3_third_kind != 'None' else '—'}
        Judge Model: {st.session_state.get('judge_model', 'openai/gpt-5-mini')}
        Row Limit: {row_limit_str}
        Judging Strategy: Weighted F1 scores + confidence
        Explain Confusion Matrix: {explain_cm}
        """)

    st.divider()

    # --- NEW: Judge Model Selector ---
    col1, col2 = st.columns([3, 1])
    with col1:
        default_judge_model = "openai/gpt-5-mini"
        if default_judge_model not in AVAILABLE_MODELS:
            default_judge_model = AVAILABLE_MODELS[0] if AVAILABLE_MODELS else OPENAI_MODEL

        default_judge_index = AVAILABLE_MODELS.index(default_judge_model) if default_judge_model in AVAILABLE_MODELS else 0

        judge_model = st.selectbox(
            "Judge Model",
            options=AVAILABLE_MODELS,
            index=default_judge_index,
            key='judge_model',
            help="Select the model to act as the judge. Defaults to gpt-5-mini for cost-effectiveness."
        )

    with col2:
        st.metric("Judge Model", _to_native_model_id(judge_model))

    if st.button("▶️ Run Test 3 (Judge)", type="primary", use_container_width=True):
        # Persist per-test OpenAI model override for this run
        st.session_state['openai_model_override'] = t3_openai_model

        overrides = {
            'use_openai': t3_use_openai,
            'openai_model': t3_openai_model,
            'use_ollama': t3_use_ollama,
            'openrouter_model': t3_openrouter_model,
            'third_kind': t3_third_kind,
            'third_model': t3_third_model,
        }
        capture_run_config("Test 3", overrides)  # CAPTURE
        # Always ensure fresh classification data is available for all models
        run_classification_flow(
            include_third_model=(t3_third_kind != "None"),
            use_openai_override=t3_use_openai,
            use_ollama_override=t3_use_ollama,
            openrouter_model_override=t3_openrouter_model,
            third_kind_override=t3_third_kind,
            third_model_override=t3_third_model,
        )

        df = _subset_for_run(st.session_state.df, ROW_LIMIT_N)

        if not len(df):
            st.info("No rows to judge.")
        else:
            with st.spinner("Running Judge on classified rows..."):
                # --- 1. Compute global F1s for weights (null-safe + filtered) ---
                y_true_all = df["classification"].fillna("").map(_normalize_label).tolist()

                # Helper to get F1 scores
                def get_f1_report_dict(pred_series):
                    pred_list = pred_series.fillna("").map(_normalize_label).tolist()
                    valid_indices = [i for i, (t, p) in enumerate(zip(y_true_all, pred_list)) if t and p]
                    if not valid_indices: return None
                    y_true_f = [y_true_all[i] for i in valid_indices]
                    y_pred_f = [pred_list[i] for i in valid_indices]
                    return classification_report(y_true_f, y_pred_f, output_dict=True, zero_division=0)

                report_m = get_f1_report_dict(df.get("classification_result_openrouter_mistral", pd.Series(dtype='str')))
                report_g = get_f1_report_dict(df.get("classification_result_openai", pd.Series(dtype='str')))
                report_t = get_f1_report_dict(df.get("classification_result_third", pd.Series(dtype='str')))

                global_f1s = {
                    "mistral": report_m['macro avg']['f1-score'] if report_m else 0.0,
                    "gpt5": report_g['macro avg']['f1-score'] if report_g else 0.0,
                    "third": report_t['macro avg']['f1-score'] if report_t else 0.0,
                }

                # --- 2. Define and run the async judge worker ---
                async def _judge_all_rows_async(df_to_judge, f1_scores):

                    async def judge_one_row(idx, row):
                        payload = {
                            "query": row.get("query"),
                            "candidates": {
                                "mistral": {"label": row.get("classification_result_openrouter_mistral"), "confidence": row.get("classification_result_openrouter_mistral_confidence"), "rationale": row.get("classification_result_openrouter_mistral_rationale")},
                                "gpt5": {"label": row.get("classification_result_openai"), "confidence": row.get("classification_result_openai_confidence"), "rationale": row.get("classification_result_openai_rationale")},
                                "third": {"label": row.get("classification_result_third"), "confidence": row.get("classification_result_third_confidence"), "rationale": row.get("classification_result_third_rationale")} if pd.notna(row.get("classification_result_third")) else None
                            },
                            "weighted_scores": {k: v for k, v in [
                                ("mistral", (f1_scores.get("mistral", 0) * float(row.get("classification_result_openrouter_mistral_confidence") or 0))),
                                ("gpt5", (f1_scores.get("gpt5", 0) * float(row.get("classification_result_openai_confidence") or 0))),
                                ("third", (f1_scores.get("third", 0) * float(row.get("classification_result_third_confidence") or 0)))
                            ] if v > 0}
                        }
                        try:
                            # Using run_judge_ollama as the primary (which calls OpenRouter)
                            result = await run_judge_ollama(payload)
                        except Exception as e:
                            result = {"final_choice_model": None, "final_label": None, "judge_rationale": f"Judge Error: {e}"}

                        # Return index with the result for safe mapping
                        return idx, result

                    tasks = [judge_one_row(idx, row) for idx, row in df_to_judge.iterrows() if str(row.get("query", "")).strip()]
                    return await asyncio.gather(*tasks)

                # --- 3. Execute the async runner and write back results safely ---
                loop = asyncio.get_event_loop()
                judge_results = loop.run_until_complete(_judge_all_rows_async(df, global_f1s))

                for idx, res in judge_results:
                    st.session_state.df.at[idx, "judge_choice_model"] = res.get("final_choice_model")
                    st.session_state.df.at[idx, "judge_choice_label"] = _normalize_label(res.get("final_label"))
                    st.session_state.df.at[idx, "judge_rationale"] = res.get("judge_rationale")

                st.success(f"Judging complete for {len(judge_results)} rows.")

                # Get fresh data with judge results
                final_df = _subset_for_run(st.session_state.df, ROW_LIMIT_N)
                third_model_name = get_third_model_display_name()

                # 1. FIRST: Progress replay
                st.subheader("📈 Processing Timeline")
                render_progress_replay("classification")

                st.divider()

                # 2. THEN: Organized results with subtabs
                st.subheader("📊 Test Results (Individual Models)")
                render_organized_results(
                    final_df,
                    test_type="classification",
                    model_cols=["openrouter_mistral", "openai", "third"],
                    model_names=["Mistral (OpenRouter)", "OpenAI", third_model_name]
                )

                st.divider()

                # 2.5. Judge Decision Flow Visualization
                st.subheader("👨‍⚖️ Judge Decision Flow")

                # Create Sankey diagram showing which models were chosen
                judge_choices = final_df['judge_choice_model'].value_counts()

                # Build flow data
                labels = ['Mistral', 'OpenAI', third_model_name, 'Judge Decision']
                source = [0, 1, 2]  # Models
                target = [3, 3, 3]  # All flow to judge
                values = [
                    judge_choices.get('mistral', 0),
                    judge_choices.get('gpt5', 0),
                    judge_choices.get('third', 0)
                ]

                fig_sankey = go.Figure(data=[go.Sankey(
                    node=dict(
                        pad=15,
                        thickness=20,
                        label=labels,
                        color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
                    ),
                    link=dict(
                        source=source,
                        target=target,
                        value=values,
                        color=['rgba(31,119,180,0.4)', 'rgba(255,127,14,0.4)', 'rgba(44,160,44,0.4)']
                    )
                )])

                fig_sankey.update_layout(
                    title="Model Selection by Judge",
                    height=300
                )

                st.plotly_chart(fig_sankey, use_container_width=True, config=PLOTLY_CONFIG)

                st.divider()

                # 3. Judge-specific performance
                with st.expander("👨‍⚖️ Judge Performance (Detailed)", expanded=True):
                    st.info("The judge selects the best answer from the 3 models based on weighted F1 scores and confidence.")
                    generate_classification_report(
                        final_df["classification"].tolist(),
                        final_df["judge_choice_label"].tolist(),
                        "LLM Judge",
                        explain=explain_cm
                    )

                    # Show judge decisions
                    st.subheader("Judge Decisions")
                    display_cols = [
                        "query", "classification",
                        "classification_result_openrouter_mistral", "classification_result_openai", "classification_result_third",
                        "judge_choice_model", "judge_choice_label", "judge_rationale"
                    ]
                    existing_cols = [col for col in display_cols if col in final_df.columns]

                    # Rename third column for clarity
                    display_df = final_df.copy()
                    if "classification_result_third" in existing_cols:
                        display_df = display_df.rename(columns={"classification_result_third": f"classification_result_{third_model_name}"})
                        existing_cols = [col if col != "classification_result_third" else f"classification_result_{third_model_name}" for col in existing_cols]

                    # Truncate judge rationale for display to prevent MemoryError
                    judge_display_df = display_df[existing_cols].copy()
                    if "judge_rationale" in judge_display_df.columns:
                        judge_display_df["judge_rationale"] = judge_display_df["judge_rationale"].astype(str).str.slice(0, 150) + "..."
                    st.dataframe(judge_display_df, use_container_width=True)

                    # Download option
                    csv = display_df[existing_cols].to_csv(index=False).encode('utf-8')
                    st.download_button(
                        "⬇️ Download Judge Results",
                        data=csv,
                        file_name="test3_judge_results.csv",
                        mime="text/csv",
                        use_container_width=True
                    )

                # --- Save results at the end of the test run ---
                save_results_df(st.session_state.df, "Test 3", ROW_LIMIT_N)
                # --- PATCH 7: Structured Summary Call for Test 3 ---
                report_text = f"Test 3 LLM Judge Results (N={len(final_df)}): Judge selection finalized. Focus analysis on optimizing judge prompt or candidate presentation."
                loop = asyncio.get_event_loop()
                loop.run_until_complete(display_final_summary_for_test("Test 3 LLM as Judge", report_text, JUDGE_INSTRUCTIONS))
                # ----------------------------------------------------


# ---------- Test 4 ----------
with tabs[4]:
    show_main_df_previews = False

    # Dashboard header
    render_test_flow_diagram(4, "Test 4: Quantitative Context Pruning & Action")

    # Collapsible configuration section
    with st.expander("⚙️ Test Configuration", expanded=False):
        st.markdown(f"""
        **Pruner Model:** Configurable (default: gpt-5-mini)
        **Test Data Source:** Generated data or {CONTEXT_PRUNING_DATASET_PATH}
        **Metrics:** Action Accuracy + Jaccard Similarity for kept keys
        **Expected Columns:** instruction, summary, user_msgs, agent_resps, tool_logs, new_question, expected_action, expected_kept_keys
        """)

    st.divider()

    # --- NEW: Pruner Model Selector ---
    col1, col2 = st.columns([3, 1])
    with col1:
        default_pruner_model = "openai/gpt-5-mini"
        if default_pruner_model not in AVAILABLE_MODELS:
            default_pruner_model = AVAILABLE_MODELS[0] if AVAILABLE_MODELS else OPENAI_MODEL

        default_pruner_index = AVAILABLE_MODELS.index(default_pruner_model) if default_pruner_model in AVAILABLE_MODELS else 0

        pruner_model = st.selectbox(
            "Pruner Model",
            options=AVAILABLE_MODELS,
            index=default_pruner_index,
            key='pruner_model',
            help="Select the model to perform context pruning. Defaults to gpt-5-mini for cost-effectiveness."
        )

    with col2:
        st.metric("Pruner Model", _to_native_model_id(pruner_model))

    st.info(f"This test runs the pruner against either generated data or `{CONTEXT_PRUNING_DATASET_PATH}`.")
    st.code("Expected columns: instruction, summary, user_msgs, agent_resps, tool_logs, new_question, expected_action, expected_kept_keys (comma-separated)")

    # --- PATCH 18: Check for session state data first ---
    if not st.session_state.pruning_df.empty:
        pruning_df = st.session_state.pruning_df
        st.subheader(f"Pruning Testset Preview (Generated Data, N={len(pruning_df)})")
        st.dataframe(pruning_df, use_container_width=True)
        loaded_from_file = False
    else:
        # Fallback to file if session state is empty
        try:
            pruning_df = pd.read_csv(CONTEXT_PRUNING_DATASET_PATH).fillna("")
            st.subheader(f"Pruning Testset Preview (Loaded from {CONTEXT_PRUNING_DATASET_PATH})")
            st.dataframe(pruning_df, use_container_width=True)
            loaded_from_file = True
        except FileNotFoundError:
            st.error(f"Create `{CONTEXT_PRUNING_DATASET_PATH}` or generate data in the Preparation tab to run this test.")
            pruning_df = None # Ensure df is None if not found
            loaded_from_file = True # Treat as file error for messaging
    # -------------------------------------------------------

    if st.button("▶️ Run Test 4 (Pruning)", type="primary", use_container_width=True) and pruning_df is not None:
        capture_run_config("Test 4") # CAPTURE
        async def run_all_pruners():
            results = []
            # Note: The logic below assumes the column names match the PruningDataItem schema,
            # which is guaranteed if loaded from session state (PATCH 17) or if the CSV is compliant.
            for _, row in pruning_df.iterrows():
                # The columns in the dataframe are already assumed to be strings/pipe-separated.
                context_items = {
                    "instruction": row.get("instruction", ""),
                    "summary": row.get("summary", ""),
                    "user_messages": str(row.get("user_msgs", "")).split("||"),
                    "agent_responses": str(row.get("agent_resps", "")).split("||"),
                    "tool_logs": str(row.get("tool_logs", "")).split("||"),
                }
                payload = { "context": context_items, "new_question": row.get("new_question", "") }
                results.append(run_pruner(payload))
            return await asyncio.gather(*results)

        with st.spinner(f"Running pruner on {len(pruning_df)} test cases..."):
            pruner_outputs = asyncio.run(run_all_pruners())

        correct_actions, key_scores = 0, []
        results_data = [] # For the detailed results table

        for i, row in pruning_df.iterrows():
            output = pruner_outputs[i]

            # --- Action Comparison ---
            model_action = output.get('action')
            expected_action = row['expected_action']
            action_correct = (model_action == expected_action)
            if action_correct:
                correct_actions += 1

            # --- Kept Keys Comparison (Jaccard) ---
            # Ensure expected_kept_keys is treated as a string before splitting
            expected_keys = set(str(row.get('expected_kept_keys', '')).split(','))
            expected_keys.discard('') # Remove empty string if split results in one
            model_keys_raw = output.get('kept_context_keys', [])
            model_keys = set(model_keys_raw) if isinstance(model_keys_raw, list) else set()

            intersection = len(expected_keys.intersection(model_keys))
            union = len(expected_keys.union(model_keys))
            jaccard_score = intersection / union if union > 0 else 0
            key_scores.append(jaccard_score)

            results_data.append({
                "Question": row['new_question'],
                "Expected Action": expected_action,
                "Model Action": model_action,
                "Action Correct": "✅" if action_correct else "❌",
                "Expected Keys": ", ".join(sorted(expected_keys)),
                "Model Keys": ", ".join(sorted(model_keys)),
                "Key Score (Jaccard)": jaccard_score
            })

        action_accuracy = correct_actions / len(pruning_df) if len(pruning_df) > 0 else 0
        avg_key_score = sum(key_scores) / len(key_scores) if key_scores else 0

        # Create results DataFrame
        results_df = pd.DataFrame(results_data)

        # Use organized rendering
        st.subheader("📊 Test Results")
        render_kpi_metrics(results_df, test_type="pruning")

        st.divider()
        st.subheader("📊 Pruning Analysis Visualizations")

        viz_col1, viz_col2 = st.columns(2)

        with viz_col1:
            # Heatmap: Action vs. Count of Kept Keys
            st.markdown("**Heatmap: Action vs. Number of Kept Keys**")

            # Prepare data
            heatmap_data = []
            for _, row in results_df.iterrows():
                num_keys = len(row['Model Keys'].split(', ')) if row['Model Keys'] else 0
                heatmap_data.append({
                    'Action': row['Model Action'],
                    'Num_Keys': num_keys
                })

            heatmap_df = pd.DataFrame(heatmap_data)
            pivot = heatmap_df.groupby(['Action', 'Num_Keys']).size().reset_index(name='Count')
            pivot_matrix = pivot.pivot_table(index='Action', columns='Num_Keys', values='Count', fill_value=0)

            fig_heatmap = go.Figure(data=go.Heatmap(
                z=pivot_matrix.values,
                x=pivot_matrix.columns,
                y=pivot_matrix.index,
                colorscale='Blues',
                text=pivot_matrix.values,
                texttemplate='%{text}',
                textfont={"size": 14},
                hovertemplate='Action: %{y}<br>Keys: %{x}<br>Count: %{z}<extra></extra>',
                colorbar=dict(title="Frequency")
            ))

            fig_heatmap.update_layout(
                xaxis_title="Number of Keys Kept",
                yaxis_title="Action Type",
                height=300
            )

            st.plotly_chart(fig_heatmap, use_container_width=True, config=PLOTLY_CONFIG)

        with viz_col2:
            # Stacked bar: Specific kept keys by action
            st.markdown("**Key Distribution by Action (Stacked)**")

            # Parse all kept keys by action
            key_action_data = []
            for _, row in results_df.iterrows():
                action = row['Model Action']
                keys = [k.strip() for k in row['Model Keys'].split(',') if k.strip()]
                for key in keys:
                    key_action_data.append({'Action': action, 'Key': key})

            if key_action_data:
                ka_df = pd.DataFrame(key_action_data)
                key_counts = ka_df.groupby(['Action', 'Key']).size().reset_index(name='Count')

                fig_stacked = go.Figure()

                actions = key_counts['Action'].unique()
                keys = key_counts['Key'].unique()

                for key in keys:
                    key_data = key_counts[key_counts['Key'] == key]
                    fig_stacked.add_trace(go.Bar(
                        name=key,
                        x=key_data['Action'],
                        y=key_data['Count'],
                        text=key_data['Count'],
                        textposition='inside',
                        hovertemplate='<b>%{fullData.name}</b><br>Action: %{x}<br>Count: %{y}<extra></extra>'
                    ))

                fig_stacked.update_layout(
                    barmode='stack',
                    xaxis_title="Action Type",
                    yaxis_title="Key Count",
                    height=300,
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=1.02,
                        xanchor="right",
                        x=1
                    )
                )

                st.plotly_chart(fig_stacked, use_container_width=True, config=PLOTLY_CONFIG)
            else:
                st.info("No key data available for visualization.")

        st.divider()

        # Show detailed results in organized format
        with st.expander("📋 Detailed Row-by-Row Results", expanded=True):
            # Truncate long rationale columns for display
            display_df = results_df.copy()
            rationale_cols = [c for c in display_df.columns if c.endswith("_rationale") or c == "judge_rationale"]
            for col in rationale_cols:
                if col in display_df.columns:
                    display_df[col] = display_df[col].astype(str).str.slice(0, 150) + "..."
            st.dataframe(display_df, use_container_width=True)

            # Download option
            csv = results_df.to_csv(index=False).encode('utf-8')
            st.download_button(
                "⬇️ Download Pruning Results",
                data=csv,
                file_name="test4_pruning_results.csv",
                mime="text/csv",
                use_container_width=True
            )

        # --- Save results at the end of the test run ---
        save_results_df(results_df, "Test 4", len(pruning_df), is_pruning_test=True)
        # --- PATCH 7: Structured Summary Call for Test 4 ---
        report_text = f"Test 4 Pruning Results (N={len(pruning_df)}): Action Accuracy: {action_accuracy:.2%}, Avg Key Similarity: {avg_key_score:.3f}. Focus analysis on context key pruning efficiency."
        loop = asyncio.get_event_loop()
        loop.run_until_complete(display_final_summary_for_test("Test 4 Quantitative Context Pruning", report_text, PRUNER_INSTRUCTIONS))
        # ----------------------------------------------------


# --- PATCH 8: Test 5: Agent Self-Refinement (Code Execution) (tabs[5]) ---

if "agent_df" not in st.session_state:
    st.session_state.agent_df = pd.DataFrame(columns=["query", "expected_sequence"])

AGENT_SYSTEM_PROMPT = """
You are a self-refining Agent Builder using the Gemini Code Execution tool. Your task is to design a Python class 'AgentFramework'
that successfully processes a list of user queries, identifies the necessary tool calls, and returns
the exact sequence of tools used.

The queries are provided in JSON format, and your goal is to write a script that defines the AgentFramework,
runs the queries using an 'evaluate' function (which you must also define), and prints the accuracy of the predicted tool sequences
against the expected sequences.

You MUST derive the final accuracy metric using the format 'Accuracy: X.XXXX' in the stdout of your executable code block
for the next turn to process your performance.

Your response in each turn must ONLY contain your reasoning followed by the full executable Python code block containing the refined AgentFramework and evaluation logic.
"""

with tabs[5]:
    # Dashboard header
    render_test_flow_diagram(5, "Test 5: Unified Orchestrator (Three Modes)")

    # Collapsible configuration section
    with st.expander("⚙️ Test Configuration", expanded=False):
        st.markdown("""
        **Architecture:** Unified orchestrator with three execution modes

        **Execution Modes:**
        1. **🎯 Direct Inference:** Pattern matching (classification, prediction)
           - Each turn: Generate → Evaluate → Analyze Failures → Refine
           - Best for: Prediction tasks, classification, tool sequence prediction
           - Example: "Predict tool sequences for user queries"

        2. **📊 Computational Analysis:** Statistics, simulations, optimization
           - Uses code execution for computational tasks
           - Best for: Data analysis, statistical computations, optimization
           - Example: "Analyze performance metrics and compute statistics"

        3. **🔍 Research Tasks:** Multi-source information gathering
           - Each turn: Decompose → Execute in Parallel → Synthesize
           - Best for: Open-ended research, multi-source information gathering
           - Example: "Research George Morgan, Symbolica AI, and their fundraising"

        **Features:**
        - ✅ Auto-mode detection based on goal
        - ✅ Parallel task execution (asyncio.gather)
        - ✅ Smart caching with deduplication
        - ✅ Convergence detection (mode-specific)

        **Budget Modes:**
        - **Fixed Turns:** Run exactly N iterations (predictable, simpler)
        - **Cost/Token Limit:** Run until budget exhausted or converged (efficient, dynamic)

        **Stopping Conditions:**
        - Turn mode: Max turns reached OR no improvement in last 3 turns
        - Cost mode: Budget exhausted OR marginal value < 1%

        **Execution Model:** Gemini 2.5 Flash with Code Execution
        **Test Data:** Tool/Agent Sequence dataset (for inference mode)
        **Evaluation Metric:** Exact sequence match accuracy (inference mode)
        **Turn Tracking:** Per-turn metrics with improvement trajectory visualization
        """)

    st.divider()

    # ============================================================
    # DEMO LAUNCHER UI COMPONENT
    # ============================================================
    st.markdown("### 🎬 Quick Demonstration Scenarios")
    st.caption("Pre-configured scenarios showcasing Leaf Agent Scaffold versatility across domains")

    col1, col2 = st.columns(2)

    with col1:
        if st.button("🤖 Demo 1: PI Agent (Laundry Folding)", use_container_width=True, help="Autonomous robotics with vision-to-motor control and adaptive learning"):
            st.session_state['demo_scenario'] = 'pi_agent'
            st.session_state['demo_goal'] = PI_AGENT_GOAL_PROMPT
            st.session_state['demo_agents'] = ["web_researcher", "code_executor", "validator", "content_generator"]
            st.session_state['demo_memory_policy'] = FOLDING_POLICY_BLOCK
            st.session_state['demo_mode'] = 'research'
            st.session_state['demo_coordination'] = 'leaf_scaffold'
            st.session_state['auto_run_demo'] = True  # Flag to auto-run
            st.success("✅ PI Agent demo starting...")
            st.rerun()

    with col2:
        if st.button("🔒 Demo 2: Cybersecurity (Phishing Analysis)", use_container_width=True, help="Threat detection with reasoning, risk scoring, and policy enforcement"):
            st.session_state['demo_scenario'] = 'cybersecurity'
            st.session_state['demo_goal'] = CYBERSECURITY_GOAL_PROMPT
            st.session_state['demo_agents'] = ["web_researcher", "validator", "code_executor", "content_generator"]
            st.session_state['demo_memory_policy'] = THREAT_POLICY_BLOCK
            st.session_state['demo_mode'] = 'research'
            st.session_state['demo_coordination'] = 'leaf_scaffold'
            st.session_state['auto_run_demo'] = True  # Flag to auto-run
            st.success("✅ Cybersecurity demo starting...")
            st.rerun()

    # Display active scenario indicator
    if 'demo_scenario' in st.session_state:
        scenario_name = "PI Agent (Laundry Folding)" if st.session_state['demo_scenario'] == 'pi_agent' else "Cybersecurity (Phishing Analysis)"
        st.info(f"📌 Active Demo: **{scenario_name}**")

        # Add clear demo button
        if st.button("🔄 Clear Demo Configuration", help="Reset to manual configuration"):
            for key in ['demo_scenario', 'demo_goal', 'demo_agents', 'demo_memory_policy', 'demo_mode', 'demo_coordination']:
                if key in st.session_state:
                    del st.session_state[key]
            st.success("Demo configuration cleared!")
            st.rerun()

    st.divider()

    # Mode selection
    st.markdown("### Task Type")
    default_mode = st.session_state.get('demo_mode', 'auto')
    mode_index = ["auto", "inference", "research", "analysis"].index(default_mode) if default_mode in ["auto", "inference", "research", "analysis"] else 0
    mode_option = st.radio(
        "Select Mode",
        options=["auto", "inference", "research", "analysis"],
        index=mode_index,
        format_func=lambda x: {
            "auto": "🤖 Auto-detect",
            "inference": "🎯 Direct Inference",
            "research": "🔍 Research Tasks",
            "analysis": "📊 Computational Analysis"
        }.get(x, x),
        horizontal=True,
        help="""
        - **Auto-detect:** Automatically choose mode based on goal
        - **Direct Inference:** Prediction/classification (e.g., tool sequences)
        - **Research Tasks:** Open-ended multi-source questions
        - **Analysis:** Computational statistics and optimization
        """
    )

    # Mode-specific info (hide for auto mode)
    if mode_option == "inference":
        st.info("💡 **Inference Mode**: Optimizes prompts through self-evaluation and failure analysis")
        st.caption("Best for: Classification, prediction, pattern matching")

    elif mode_option == "analysis":
        st.info("💡 **Analysis Mode**: Generates Python code for computational tasks")
        st.caption("Best for: Statistics, data analysis, optimization, simulations")

    elif mode_option == "research":
        st.info("💡 **Research Mode**: Hybrid strategy - decomposes into subtasks, uses code generation for computational parts")
        st.caption("Best for: Multi-source research with computational analysis requirements")

    # Auto mode: no info box (mode not yet determined)

    st.divider()

    # Coordination Pattern selection
    st.markdown("### Coordination Pattern")
    default_coordination = st.session_state.get('demo_coordination', 'auto')
    coordination_index = ["auto", "solo", "subagent", "multi_agent", "leaf_scaffold"].index(default_coordination) if default_coordination in ["auto", "solo", "subagent", "multi_agent", "leaf_scaffold"] else 0
    coordination_option = st.radio(
        "Select Coordination Pattern",
        options=["auto", "solo", "subagent", "multi_agent", "leaf_scaffold"],
        index=coordination_index,
        format_func=lambda x: {
            "auto": "🤖 Auto-detect",
            "solo": "👤 Solo Agent",
            "subagent": "🏗️ Subagent Orchestration",
            "multi_agent": "👥 Multi-Agent Collaboration",
            "leaf_scaffold": "🌳 Leaf Agent Scaffold"
        }.get(x, x),
        horizontal=True,
        help="""
        - **Auto-detect:** Automatically choose pattern based on task complexity
        - **Solo Agent:** Single agent executes independently (fastest)
        - **Subagent Orchestration:** Hierarchical delegation with specialized subagents
        - **Multi-Agent Collaboration:** Peer agents propose, review, and build consensus
        - **Leaf Agent Scaffold:** Hierarchical multi-agent with supervisor and specialized leaf agents
        """
    )

    # Pattern-specific info (hide simple info for multi-agent and auto)
    if coordination_option == "solo":
        st.info("👤 **Solo Agent**: Single agent executes the task independently")
        st.caption("Best for: Simple, straightforward tasks")

    elif coordination_option == "subagent":
        st.info("🏗️ **Subagent Orchestration**: Hierarchical delegation (Decomposer → Generator → Evaluator → Analyzer → Synthesizer)")
        st.caption("Best for: Complex tasks requiring specialized expertise at each stage")

        # Optional: Show subagent workflow diagram
        with st.expander("🏗️ Subagent Workflow Details", expanded=False):
            st.markdown("""
            **Hierarchical Pipeline**:
            1. **Decomposer** → Breaks goal into subtasks
            2. **Generator** → Creates solutions for each subtask
            3. **Evaluator** → Tests and scores solutions
            4. **Analyzer** → Identifies failures and suggests improvements
            5. **Synthesizer** → Combines results into final solution

            Each subagent specializes in one stage of the pipeline.
            """)

    elif coordination_option == "multi_agent":
        st.divider()
        st.markdown("## 👥 Multi-Agent Collaboration")
        st.info("**How it works**: Peer agents independently propose solutions, cross-review each other's work, and build consensus through collaborative synthesis")
        st.caption("✨ Best for: Tasks requiring diverse perspectives, validation, or creative problem-solving")

        # Multi-agent configuration
        st.markdown("### 🎭 Configure Your Agent Team")

        # Get default roles based on mode
        default_roles = {
            "inference": ["Pattern Analyst", "Rule Designer", "Evaluator"],
            "analysis": ["Data Scientist", "Algorithm Designer", "Code Reviewer"],
            "research": ["Market Analyst", "Technical Expert", "Domain Specialist"]
        }

        current_mode = mode_option if mode_option != "auto" else "inference"
        default_role_list = default_roles.get(current_mode, ["Agent 1", "Agent 2", "Agent 3"])

        peer_roles_input = st.text_input(
            "Peer Agent Roles (comma-separated)",
            value=", ".join(default_role_list),
            help="Define the roles for peer agents. Each role brings a different perspective."
        )

        peer_agent_roles = [role.strip() for role in peer_roles_input.split(",") if role.strip()]

        if len(peer_agent_roles) < 2:
            st.warning("⚠️ Multi-agent mode requires at least 2 peer agents")
        else:
            st.success(f"✅ {len(peer_agent_roles)} peer agents configured: {', '.join(peer_agent_roles)}")

        # Show workflow preview
        with st.expander("📋 Multi-Agent Workflow Preview", expanded=True):
            st.markdown(f"""
            **Round 1: Independent Proposals** 💡
            - Each of the {len(peer_agent_roles)} agents proposes their solution independently
            - Agents: {', '.join(f'**{role}**' for role in peer_agent_roles)}

            **Round 2: Cross-Review** 🔍
            - Each agent reviews proposals from other agents
            - Provides constructive feedback and identifies concerns

            **Round 3: Consensus Building** 🤝
            - Synthesizer combines all proposals and reviews
            - Creates unified solution incorporating best ideas

            **Round 4: Joint Evaluation** ✅
            - All agents jointly evaluate the consensus
            - Assess quality, agreement, and completeness

            **View Results**: Check the **Agent Dashboard** (Tab 6) to see the full interaction timeline!
            """)

        # Quick demo button
        st.markdown("#### 🎯 Quick Demo")
        demo_col1, demo_col2 = st.columns([3, 1])
        with demo_col1:
            st.caption("Try a pre-configured multi-agent collaboration to see how it works")
        with demo_col2:
            if st.button("🎬 Load Demo", type="secondary", use_container_width=True):
                # Set demo values
                st.session_state['test5_prompt'] = "Design a robust tool sequence predictor with 90%+ accuracy"
                st.rerun()

        # Show message if demo was just loaded
        if st.session_state.get('test5_prompt') == "Design a robust tool sequence predictor with 90%+ accuracy":
            st.success("✅ Demo prompt loaded! Scroll down and click the '👥 Run Multi-Agent Collaboration' button to execute.")

    elif coordination_option == "leaf_scaffold":
        st.divider()
        st.markdown("## 🌳 Leaf Agent Scaffold")
        st.info("**Hierarchical multi-agent system** with supervisor orchestration and specialized leaf agents")
        st.caption("✨ Best for: Complex tasks requiring multiple specialized capabilities (research, computation, writing, validation)")

        # Show architecture diagram
        with st.expander("🏗️ Architecture Overview", expanded=True):
            st.markdown("""
            **Hierarchical Structure**:

            ```
            🧠 Supervisor Agent
                ├── 📋 Task Planning (decomposes complex task)
                ├── 🎯 Delegation (routes to specialists)
                └── 🔄 Synthesis (combines results)

            👥 Specialized Leaf Agents
                ├── 🔍 Web Researcher (information gathering)
                ├── 💻 Code Executor (computation & analysis)
                ├── ✍️ Content Generator (writing & formatting)
                └── ✅ Validator (quality assurance)
            ```

            **How it works**:
            1. **Supervisor** analyzes your goal and breaks it into specialized sub-tasks
            2. **Leaf Agents** execute their assigned sub-tasks independently
            3. **Supervisor** synthesizes all results into final answer

            **Check Agent Dashboard (Tab 6)** to see the full execution hierarchy!
            """)

        # Agent selection
        st.markdown("### 🎭 Select Leaf Agents")

        available_agents = {
            "web_researcher": "🔍 Web Researcher (information gathering from web sources)",
            "code_executor": "💻 Code Executor (Python computation & data analysis)",
            "content_generator": "✍️ Content Generator (writing & formatting)",
            "validator": "✅ Validator (quality assurance & validation)"
        }

        # Use demo agents if available, otherwise default
        default_agents = st.session_state.get('demo_agents', ["web_researcher", "validator", "content_generator"])
        selected_agent_types = st.multiselect(
            "Choose specialized agents for this task",
            options=list(available_agents.keys()),
            default=default_agents,
            format_func=lambda x: available_agents[x],
            help="Select agents based on task requirements. The supervisor will automatically delegate sub-tasks to appropriate agents. For research tasks, include 'validator' to prevent hallucinations."
        )

        # Store in session state for orchestrator
        st.session_state['selected_agent_types'] = selected_agent_types

        if len(selected_agent_types) < 1:
            st.warning("⚠️ Select at least one leaf agent")
        else:
            st.success(f"✅ {len(selected_agent_types)} specialized agents configured")

            # Show selected agents
            st.markdown("**Your Agent Team**:")
            for agent_type in selected_agent_types:
                st.write(f"  • {available_agents[agent_type]}")

        # Quick demo button
        st.markdown("#### 🎯 Quick Demo")
        demo_col1, demo_col2 = st.columns([3, 1])
        with demo_col1:
            st.caption("Try a pre-configured leaf agent scaffold to see hierarchical orchestration in action")
        with demo_col2:
            if st.button("🎬 Load Demo", key="leaf_demo", type="secondary", use_container_width=True):
                # Set demo values
                st.session_state['test5_prompt'] = "Research and analyze the latest trends in multi-agent AI systems, then write a comprehensive report"
                st.rerun()

        # Show message if demo was just loaded
        if st.session_state.get('test5_prompt') == "Research and analyze the latest trends in multi-agent AI systems, then write a comprehensive report":
            st.success("✅ Demo prompt loaded! Scroll down and click the '🌳 Run Leaf Agent Scaffold' button to execute.")

        peer_agent_roles = None

    else:
        peer_agent_roles = None
        selected_agent_types = None

    # Dataset-aware suggested prompts
    TEST5_SUGGESTED_PROMPTS = {
        "inference": [
            "Design an agent that predicts tool sequences from user queries with 90%+ accuracy through iterative refinement.",
            "Build a pattern-matching agent using keyword extraction and rule-based mapping for tool sequence prediction.",
            "Create a rule-based tool sequence predictor with learning capability that analyzes failures and adds new rules."
        ],
        "research": [
            "Research [Company/Person Name] for due diligence: background, financial health, market position, recent news, and key stakeholders.",
            "Competitive analysis for [Industry/Market]: identify top players, compare features/pricing, analyze strengths/weaknesses.",
            "Investment thesis research: fundamentals, market opportunity, competitive moat, team execution, and risk assessment."
        ],
        "analysis": [
            "Analyze test dataset performance metrics: sequence length distribution, common tool combinations, coverage analysis.",
            "Optimize prediction accuracy through data analysis: baseline accuracy, hard vs easy examples, feature importance, error analysis.",
            "Compute statistical summary of dataset: complexity scores, optimal training split, correlation analysis."
        ]
    }

    # Initialize prompt in session state
    if 'test5_prompt' not in st.session_state:
        st.session_state['test5_prompt'] = ""

    # Prompt generator function
    def generate_test5_prompt():
        """Generate a dataset-aware prompt based on current mode AND user input."""
        mode = mode_option if mode_option != "auto" else "inference"

        # Get current user input
        current_input = st.session_state.get('test5_prompt', '')

        # Select base prompt
        import random
        prompts = TEST5_SUGGESTED_PROMPTS.get(mode, TEST5_SUGGESTED_PROMPTS["inference"])
        base_prompt = random.choice(prompts) if isinstance(prompts, list) else prompts

        # Build context from dataset
        context = {
            'dataset_size': len(st.session_state.agent_df) if not st.session_state.agent_df.empty else 0,
            'columns': list(st.session_state.agent_df.columns) if not st.session_state.agent_df.empty else [],
            'mode': mode
        }

        if mode == "inference" and not st.session_state.agent_df.empty:
            # Extract actual tools from dataset
            all_tools = set()
            for seq in st.session_state.agent_df['expected_sequence']:
                if isinstance(seq, list):
                    all_tools.update(seq)
                elif isinstance(seq, str):
                    try:
                        tools = json.loads(seq.replace("'", '"'))
                        all_tools.update(tools)
                    except:
                        all_tools.update([t.strip() for t in seq.split(',') if t.strip()])

            # Sample queries
            sample_queries = st.session_state.agent_df['query'].head(3).tolist()

            context['tools'] = sorted(all_tools)[:15]
            context['sample_queries'] = sample_queries
            context['success_criteria'] = '85%+ exact sequence match accuracy'

            # Add dataset-specific approach
            context['suggested_approach'] = f"""Pattern matching + entity extraction:
1. Keyword dictionaries for common patterns
2. Regex for structured data (IDs, IPs)
3. Sequence logic for multi-step operations
4. Confidence scoring for ambiguous cases"""

            # Create dataset-aware base prompt
            base_prompt = f"""Design a tool sequence predictor for IT operations queries.

DATASET: {len(st.session_state.agent_df)} examples
TOOLS: {', '.join(sorted(all_tools)[:12])}...

SAMPLE QUERIES:
{chr(10).join([f'- "{q}"' for q in sample_queries])}

APPROACH:
1. Pattern matching (keywords → tools)
2. Entity extraction (IDs, IPs, device names)
3. Sequence logic (auth before query, check before order)
4. Error handling (unknown patterns → default)

TARGET: 85%+ exact match accuracy"""

        # Enhance with user input
        enhanced = enhance_prompt_with_user_input(base_prompt, current_input, context)

        return enhanced

    # Different inputs based on mode
    if mode_option in ["auto", "research"]:
        use_test_data = False

        # Prompt management section (NO test data preview for research/auto)
        st.markdown("### 🎯 Research Goal")

        # Use demo goal if available, otherwise default
        if 'demo_goal' in st.session_state:
            st.session_state['test5_prompt'] = st.session_state['demo_goal']
            # Clear demo_goal after using it to prevent re-applying on every rerun
            del st.session_state['demo_goal']
        elif not st.session_state['test5_prompt']:
            st.session_state['test5_prompt'] = "Help me research George Morgan, Symbolica AI, their position on AI Engineering, and their next fundraising round"

        col1, col2 = st.columns([3, 1])
        with col1:
            pass  # Placeholder for layout
        with col2:
            if st.button("🎲 Generate Prompt", width='content'):
                st.session_state['test5_prompt'] = generate_test5_prompt()
                st.rerun()

        goal = st.text_area(
            "Research Goal / Task Description",
            value=st.session_state['test5_prompt'],
            height=200,  # Increased height for demo prompts
            help="Describe what you want to research or accomplish",
            key="test5_goal_input"
        )

        # Update session state
        st.session_state['test5_prompt'] = goal

        if mode_option == "research":
            st.info("💡 **Research Mode:** Decomposes goal into parallel subtasks for multi-source information gathering.")
    else:
        # For inference and analysis modes: SHOW test data preview
        if st.session_state.agent_df.empty:
            st.warning("⚠️ Please generate a Tool/Agent Sequence dataset in the 'Preparation' tab first.")
            use_test_data = False
            goal = "Predict tool sequences"
        else:
            st.subheader("📊 Agent Test Data Preview")
            st.dataframe(st.session_state.agent_df.head(5), use_container_width=True)
            st.caption(f"Total: {len(st.session_state.agent_df)} examples")

            use_test_data = True

            # Prompt management section
            st.markdown("### 🎯 Task Description")

            # Use demo goal if available, otherwise generate
            if 'demo_goal' in st.session_state:
                st.session_state['test5_prompt'] = st.session_state['demo_goal']
                # Clear demo_goal after using it
                del st.session_state['demo_goal']
            elif not st.session_state['test5_prompt']:
                st.session_state['test5_prompt'] = generate_test5_prompt()

            col1, col2 = st.columns([3, 1])
            with col1:
                pass  # Placeholder for layout
            with col2:
                if st.button("🎲 Generate Prompt", width='content'):
                    st.session_state['test5_prompt'] = generate_test5_prompt()
                    st.rerun()

            goal = st.text_area(
                "Agent Goal / Task Description",
                value=st.session_state['test5_prompt'],
                height=200,
                help="Describe the task in detail. Be specific about inputs, outputs, and success criteria.",
                key="test5_goal_input"
            )

            # Update session state
            st.session_state['test5_prompt'] = goal

            # Quick Start Templates (ONLY for inference/analysis, NOT research)
            st.markdown("##### 💡 Quick Start Templates")
            mode_key = mode_option if mode_option != "auto" else "inference"
            prompts = TEST5_SUGGESTED_PROMPTS.get(mode_key, TEST5_SUGGESTED_PROMPTS["inference"])

            cols = st.columns(len(prompts))
            for i, prompt in enumerate(prompts):
                with cols[i]:
                    label = f"Template {i+1}"
                    if st.button(label, key=f"test5_pill_{i}", help=prompt, use_container_width=True):
                        st.session_state['test5_prompt'] = prompt
                        st.rerun()

            # Mode-specific tips (only for inference/analysis in this section)
            if mode_option == "inference":
                st.info("💡 **Inference Mode:** Each turn generates code, evaluates it, and refines based on failures. Best for prediction tasks.")
            elif mode_option == "analysis":
                st.info("💡 **Analysis Mode:** Uses code execution for computational analysis and optimization tasks.")

    st.divider()

    # Budget mode selection
    budget_mode = st.radio(
        "Budget Mode",
        options=["turns", "cost"],
        format_func=lambda x: "Fixed Turns" if x == "turns" else "Cost/Token Limit",
        horizontal=True,
        help="Fixed Turns: Run exactly N iterations. Cost/Token: Run until budget exhausted or converged."
    )

    col1, col2 = st.columns(2)

    if budget_mode == "turns":
        with col1:
            max_turns = st.number_input("Max Turns", min_value=1, max_value=20, value=3)
        with col2:
            st.info("Runs for exactly N turns (tracking cost for reporting)")
    else:  # cost mode
        with col1:
            max_cost = st.number_input("Budget (USD)", min_value=0.5, max_value=20.0, value=2.0, step=0.5)
        with col2:
            max_tokens = st.number_input("Token Limit", min_value=100_000, max_value=5_000_000, value=500_000, step=100_000)

    # Run button with pattern-specific messaging
    st.divider()

    if coordination_option == "multi_agent":
        st.markdown("### 🎬 Run Multi-Agent Collaboration")
        st.info("💡 **Tip**: After running, switch to **Tab 6 (Agent Dashboard)** to see the full agent interaction timeline!")
        button_label = "👥 Run Multi-Agent Collaboration"
    elif coordination_option == "subagent":
        st.markdown("### 🎬 Run Subagent Orchestration")
        button_label = "🏗️ Run Subagent Orchestration"
    elif coordination_option == "leaf_scaffold":
        st.markdown("### 🎬 Run Leaf Agent Scaffold")
        st.info("💡 **Tip**: After running, switch to **Tab 6 (Agent Dashboard)** to see the hierarchical execution flow!")
        button_label = "🌳 Run Leaf Agent Scaffold"
    else:
        st.markdown("### 🎬 Run Orchestrator")
        button_label = "🚀 Run Unified Orchestrator"

    # Check if demo should auto-run
    auto_run = st.session_state.get('auto_run_demo', False)
    if auto_run:
        # Clear the flag to prevent infinite loop
        st.session_state['auto_run_demo'] = False
        # Show demo is running
        demo_name = "PI Agent (Laundry Folding)" if st.session_state.get('demo_scenario') == 'pi_agent' else "Cybersecurity (Phishing Analysis)"
        st.info(f"🎬 **Auto-running demo:** {demo_name}")

    if st.button(button_label, type="primary", use_container_width=True) or auto_run:
        if not GEMINI_API_KEY:
            st.error("GEMINI_API_KEY required")
        else:
            # Reset tracker for fresh run
            tracker = st.session_state.get('execution_tracker')
            if tracker:
                tracker.reset()

            # Create budget based on mode
            if budget_mode == "turns":
                budget = Budget(mode="turns", max_turns=max_turns)
            else:
                budget = Budget(mode="cost", max_cost_usd=max_cost, max_tokens=max_tokens)

            # Prepare test data if needed
            test_data = None
            if use_test_data and not st.session_state.agent_df.empty:
                # CRITICAL: Parse expected_sequence from string to list with robust error handling
                test_data = st.session_state.agent_df.to_dict('records')

                parse_errors = []
                for idx, item in enumerate(test_data):
                    seq = item.get('expected_sequence', '')
                    original_seq = seq  # Keep for error reporting

                    try:
                        if isinstance(seq, str):
                            # Handle various formats: "tool1,tool2" or "['tool1','tool2']" or "tool1|tool2"
                            if seq.startswith('['):
                                # JSON-like format
                                try:
                                    parsed = json.loads(seq.replace("'", '"'))
                                    # Validate it's actually a list
                                    if isinstance(parsed, list):
                                        item['expected_sequence'] = parsed
                                    else:
                                        # Single value wrapped in brackets
                                        item['expected_sequence'] = [parsed]
                                except json.JSONDecodeError as e:
                                    # Fallback to split
                                    item['expected_sequence'] = [s.strip() for s in re.split(r'[,|;]', seq.strip('[]')) if s.strip()]
                                    parse_errors.append(f"Row {idx}: JSON parse failed, used split fallback: {str(e)[:50]}")
                            else:
                                # Comma or pipe separated
                                item['expected_sequence'] = [s.strip() for s in re.split(r'[,|;]', seq) if s.strip()]
                        elif isinstance(seq, list):
                            # Already a list - validate contents
                            item['expected_sequence'] = [str(x).strip() for x in seq]
                        else:
                            # Unexpected type
                            item['expected_sequence'] = []
                            parse_errors.append(f"Row {idx}: Unexpected type {type(seq).__name__}, defaulted to empty list")
                    except Exception as e:
                        # Catch-all for any parsing errors
                        item['expected_sequence'] = []
                        parse_errors.append(f"Row {idx}: Parse error '{str(e)[:50]}', defaulted to empty list")

                # Show parsing warnings if any
                if parse_errors:
                    st.warning(f"⚠️ {len(parse_errors)} data parsing issue(s):")
                    for err in parse_errors[:5]:  # Show first 5
                        st.text(f"  • {err}")
                    if len(parse_errors) > 5:
                        st.text(f"  ... and {len(parse_errors) - 5} more")

                st.write(f"📋 Loaded {len(test_data)} test cases")
                if test_data:
                    st.write(f"📝 Sample: {test_data[0]}")  # Verify format

            # Create orchestrator with auto-detection or explicit mode and coordination pattern
            orchestrator_kwargs = {
                "goal": goal,
                "test_data": test_data,
                "budget": budget
            }

            # Add mode if not auto
            if mode_option != "auto":
                orchestrator_kwargs["mode"] = mode_option

            # Add coordination pattern if not auto
            if coordination_option != "auto":
                orchestrator_kwargs["coordination_pattern"] = coordination_option

            # Add peer agent roles for multi-agent mode
            if coordination_option == "multi_agent" and peer_agent_roles:
                orchestrator_kwargs["peer_agent_roles"] = peer_agent_roles

            orchestrator = UnifiedOrchestrator(**orchestrator_kwargs)

            try:
                results = asyncio.run(orchestrator.run())

                # Display coordination pattern used
                st.info(f"🎯 Mode: **{orchestrator.mode.upper()}** | 🤝 Pattern: **{orchestrator.coordination_pattern.upper()}**")

                # Handle different result types based on coordination pattern and mode
                if orchestrator.coordination_pattern == "multi_agent":
                    # Multi-agent returns a dict with consensus
                    st.success("✅ Multi-agent collaboration completed!")

                    # Prominent dashboard link
                    st.info("📊 **View the full agent interaction timeline in Tab 6 (Agent Dashboard)!**")

                    # Show summary metrics
                    col1, col2, col3 = st.columns(3)

                    if isinstance(results, dict):
                        with col1:
                            st.metric("Final Score", f"{results.get('final_score', 0.0):.3f}")
                        with col2:
                            st.metric("Peer Agents", len(results.get('peer_roles', [])))
                        with col3:
                            st.metric("Consensus Rounds", len(results.get('consensus_history', [])))

                        # Show final consensus in expandable section
                        with st.expander("📋 View Final Consensus", expanded=False):
                            st.json(results.get('final_consensus', {}))

                        # Show consensus history
                        if results.get('consensus_history'):
                            with st.expander("📚 View Consensus History", expanded=False):
                                for idx, consensus_round in enumerate(results['consensus_history'], 1):
                                    st.markdown(f"### Turn {idx}")

                                    # Show proposals
                                    st.markdown("**Proposals:**")
                                    for proposal in consensus_round.get('proposals', []):
                                        role = proposal.get('role', 'Unknown')
                                        prop = proposal.get('proposal', {})
                                        st.markdown(f"- **{role}**: {prop.get('approach', 'N/A')}")

                                    # Show evaluation
                                    eval_data = consensus_round.get('evaluation', {})
                                    st.markdown(f"**Score**: {eval_data.get('score', 0.0):.3f} | **Agreement**: {eval_data.get('agreement', 0.0):.3f}")
                                    st.divider()

                elif orchestrator.coordination_pattern == "leaf_scaffold":
                    # Leaf scaffold returns a dict with hierarchical results
                    st.success("✅ Leaf agent scaffold execution completed!")

                    # Prominent dashboard link
                    st.info("📊 **View the hierarchical execution flow in Tab 6 (Agent Dashboard)!**")

                    # Show summary metrics
                    col1, col2, col3 = st.columns(3)

                    if isinstance(results, dict):
                        with col1:
                            st.metric("Leaf Agents", len(results.get('leaf_agents', [])))
                        with col2:
                            st.metric("Sub-Tasks", results.get('sub_tasks', 0))
                        with col3:
                            successful = results.get('metadata', {}).get('successful_tasks', 0)
                            st.metric("Successful Tasks", successful)

                        # Show final result
                        if results.get('final_result'):
                            st.markdown("### 📊 Final Synthesized Result")
                            st.markdown(results['final_result'])

                        # Show metadata
                        if results.get('metadata'):
                            with st.expander("📈 Execution Metadata", expanded=False):
                                st.json(results['metadata'])

                        # Show contributing agents
                        if results.get('leaf_agents'):
                            with st.expander("👥 Contributing Agents", expanded=False):
                                for agent_name in results['leaf_agents']:
                                    st.write(f"  • {agent_name}")

                elif orchestrator.coordination_pattern == "subagent":
                    # Subagent returns a dict with synthesized results
                    st.success("Subagent orchestration completed!")

                    if isinstance(results, dict):
                        st.metric("Final Score", f"{results.get('score', 0.0):.3f}")
                        st.metric("Best Turn", results.get('best_turn', 0))
                        st.metric("Total Turns", results.get('total_turns', 0))

                        if results.get('solution'):
                            st.subheader("Final Solution")
                            st.json(results['solution'])

                elif orchestrator.mode == "research":
                    # Research mode returns a dict with findings
                    st.success("Research completed!")
                    st.json(results)

                elif orchestrator.mode in ["inference", "analysis"]:
                    # Inference/analysis modes in solo pattern return (code, perf, history)
                    if isinstance(results, dict):
                        # New format from prompt optimization
                        best_code = results.get('best_prompt', '')
                        best_perf = results.get('best_accuracy', 0.0)
                        history = results.get('history', [])
                    else:
                        # Legacy tuple format
                        best_code, best_perf, history = results

                    st.success(f"Final Performance: {best_perf:.4f} (Turn {orchestrator.best_turn})")

                    if best_code:
                        st.subheader("Best Code Generated")
                        st.code(best_code, language='python')

                    # Turn-by-turn breakdown
                    st.subheader("Turn-by-Turn Progress")

                    turns_df = pd.DataFrame([
                        {
                            "Turn": m.turn,
                            "Tasks Attempted": m.tasks_attempted,
                            "Tasks Verified": m.tasks_verified,
                            "Best Accuracy": f"{m.best_accuracy:.4f}",
                            "Improvement": f"{m.improvement:+.4f}",
                            "Cost": f"${m.cost_spent:.3f}",
                            "Tokens": f"{m.tokens_used:,}"
                        }
                        for m in history
                    ])

                    st.dataframe(turns_df, width='content')

                    # Plot improvement trajectory
                    if history:
                        fig = go.Figure()

                        fig.add_trace(go.Scatter(
                            x=[m.turn for m in history],
                            y=[m.best_accuracy for m in history],
                            mode='lines+markers',
                            name='Accuracy',
                            line=dict(color='#1f77b4', width=3),
                            marker=dict(size=8)
                        ))

                        fig.update_layout(
                            title="Performance Improvement Trajectory",
                            xaxis_title="Turn",
                            yaxis_title="Accuracy",
                            height=400
                        )

                        st.plotly_chart(fig, width='content', config=PLOTLY_CONFIG)

                    # NEW: Agent Memory State Expander
                    if orchestrator.memory_manager:
                        with st.expander("🧠 Agent Memory State", expanded=False):
                            st.markdown("### Core Memory Blocks")

                            for block_name, block in orchestrator.memory_manager.core_blocks.items():
                                st.markdown(f"#### {block_name}")
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("Version", block.version)
                                with col2:
                                    st.metric("Last Modified Turn", block.last_modified_turn)
                                with col3:
                                    st.metric("Modifications", block.modification_count)

                                st.json({"content": block.content})
                                st.divider()

                            # Archival Memory Summary
                            st.markdown("### Archival Memory")
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Total Entries", len(orchestrator.memory_manager.archival_entries))
                            with col2:
                                st.metric("Total Retrievals", orchestrator.memory_manager.stats["total_retrievals"])
                            with col3:
                                avg_latency = orchestrator.memory_manager.stats["avg_retrieval_latency_ms"]
                                st.metric("Avg Retrieval Latency", f"{avg_latency:.2f}ms")

                            # Show recent archival entries
                            if orchestrator.memory_manager.archival_entries:
                                st.markdown("#### Recent Entries")
                                recent_entries = orchestrator.memory_manager.archival_entries[-5:]
                                for entry in reversed(recent_entries):
                                    st.text(f"[{entry.source_agent}] {entry.content[:100]}...")
                                    st.caption(f"Tags: {', '.join(entry.tags)} | {entry.timestamp}")

                    # NEW: Rethink Events on Improvement Trajectory
                    if orchestrator.self_correction_manager and orchestrator.self_correction_manager.rethink_history:
                        st.markdown("### 🔄 Self-Correction Events")

                        # Show rethink events as markers on the chart
                        rethink_turns = [r["turn"] for r in orchestrator.self_correction_manager.rethink_history]

                        if rethink_turns:
                            st.info(f"🔄 {len(rethink_turns)} rethink event(s) occurred at turn(s): {', '.join(map(str, rethink_turns))}")

                            # Table of rethink events
                            rethink_df = pd.DataFrame([
                                {
                                    "Turn": r["turn"],
                                    "Block Modified": r["block_name"],
                                    "Trigger": r["trigger"][:50] + "..." if len(r["trigger"]) > 50 else r["trigger"],
                                    "Timestamp": r["timestamp"]
                                }
                                for r in orchestrator.self_correction_manager.rethink_history
                            ])

                            st.dataframe(rethink_df, use_container_width=True)

                            # Analyze effectiveness
                            if history:
                                performance_scores = [m.best_accuracy for m in history]
                                effectiveness = orchestrator.self_correction_manager.analyze_correction_effectiveness(performance_scores)

                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("Total Rethinks", effectiveness["total_rethinks"])
                                with col2:
                                    st.metric("Successful Rethinks", effectiveness["successful_rethinks"])
                                with col3:
                                    success_rate = effectiveness["success_rate"] * 100
                                    st.metric("Success Rate", f"{success_rate:.1f}%")

                    # NEW: Turn-by-Turn Metrics with Memory & Security
                    if orchestrator.dashboard_logger:
                        st.markdown("### 📊 Detailed Turn Metrics")

                        # Load execution log
                        try:
                            run_data = DashboardLogger.load_run(orchestrator.dashboard_logger.run_id)
                            execution_log = run_data.get("execution_log", [])
                            security_audits = run_data.get("security_audits", [])

                            # Group by turn
                            turn_metrics = {}
                            for entry in execution_log:
                                turn = entry.get("turn", 0)
                                if turn not in turn_metrics:
                                    turn_metrics[turn] = {
                                        "memory_ops": 0,
                                        "security_checks": 0,
                                        "events": []
                                    }

                                if entry.get("event_type") == "MEMORY_WRITE":
                                    turn_metrics[turn]["memory_ops"] += 1

                                turn_metrics[turn]["events"].append(entry)

                            # Add security audits
                            for audit in security_audits:
                                turn = audit.get("turn", 0)
                                if turn in turn_metrics:
                                    turn_metrics[turn]["security_checks"] += 1

                            # Create enhanced metrics table
                            enhanced_metrics = []
                            for m in history:
                                turn_data = turn_metrics.get(m.turn, {})
                                enhanced_metrics.append({
                                    "Turn": m.turn,
                                    "Accuracy": f"{m.best_accuracy:.4f}",
                                    "Memory Ops": turn_data.get("memory_ops", 0),
                                    "Security Checks": turn_data.get("security_checks", 0),
                                    "Duration": f"{m.cost_spent:.3f}s",
                                    "Improvement": f"{m.improvement:+.4f}"
                                })

                            st.dataframe(pd.DataFrame(enhanced_metrics), use_container_width=True)

                        except Exception as e:
                            st.warning(f"Could not load detailed metrics: {e}")

                    # Summary
                    st.subheader("Execution Summary")
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("Total Turns", len(history))
                    with col2:
                        st.metric("Verified Tasks", len(orchestrator.cache.verified_tasks))
                    with col3:
                        st.metric("Total Cost", f"${budget.spent_cost:.2f}")
                    with col4:
                        st.metric("Best Turn", orchestrator.best_turn)

                    # Knowledge Index Summary
                    if orchestrator.index.entries:
                        st.subheader("Knowledge Index")
                        verified_count = sum(1 for e in orchestrator.index.entries if e["verdict"] == "verified")
                        partial_count = sum(1 for e in orchestrator.index.entries if e["verdict"] == "partial")
                        failed_count = sum(1 for e in orchestrator.index.entries if e["verdict"] == "failed")

                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Verified", verified_count, delta=None)
                        with col2:
                            st.metric("Partial", partial_count, delta=None)
                        with col3:
                            st.metric("Failed", failed_count, delta=None)

                    # LLM Analysis
                    mode_desc = f"Turn-based ({len(history)} turns)" if budget_mode == "turns" else f"Cost-based (${budget.spent_cost:.2f})"
                    report = f"""Orchestrator Test Report:
Task Type: {orchestrator.mode.upper()}
Mode: {mode_desc}
Final Best Accuracy: {best_perf:.4f} (achieved at turn {orchestrator.best_turn})
Total Turns: {len(history)}
Verified Tasks: {len(orchestrator.cache.verified_tasks)}
Total Cost: ${budget.spent_cost:.2f}
Total Tokens: {budget.spent_tokens:,}

Turn-by-Turn Summary:
{chr(10).join([f"Turn {m.turn}: {m.tasks_verified}/{m.tasks_attempted} verified, accuracy={m.best_accuracy:.4f}, improvement={m.improvement:+.4f}" for m in history])}
"""

                    summary_result = asyncio.run(get_structured_summary_and_refinement(report, best_code))

                    st.subheader("🎯 LLM Analysis & Suggested Improvements")
                    st.markdown(f"**Summary:** {summary_result.findings_summary}")
                    st.markdown("**Key Suggestions:**")
                    st.json(summary_result.key_suggestions)

                    if summary_result.suggested_improvement_code:
                        st.subheader("✨ Refined Code/Prompt for Next Iteration")
                        st.code(summary_result.suggested_improvement_code, language='python')
                    if summary_result.suggested_improvement_prompt_reasoning:
                        st.subheader("💡 Reasoning for Refinement")
                        st.markdown(summary_result.suggested_improvement_prompt_reasoning)

            except Exception as e:
                st.error(f"Orchestrator failed: {e}")
                st.exception(e)


# ---------- Tab 6: Agent Dashboard ----------
with tabs[6]:
    st.header("🎯 Agent Execution Dashboard")
    st.caption("Real-time monitoring and visualization of all test executions")

    # Enhanced dashboard with historical data loading and mock data
    def load_historical_data():
        """Load historical execution data from dashboard logs."""
        try:
            from utils.dashboard_logger import DashboardLogger

            # Load index
            index_data = DashboardLogger.load_index()
            historical_runs = []

            for run_info in index_data.get('runs', []):
                try:
                    run_data = DashboardLogger.load_run(run_info['run_id'])
                    historical_runs.append({
                        'run_info': run_info,
                        'data': run_data
                    })
                except Exception as e:
                    st.warning(f"Could not load run {run_info['run_id']}: {e}")

            return historical_runs
        except Exception as e:
            st.warning(f"Could not load historical data: {e}")
            return []

    def generate_mock_data():
        """Generate mock execution data for demonstration with RELATIVE timestamps."""
        mock_events = []
        base_time = 0  # Start at 0 for relative timing

        # Mock Test 1: Classification Test
        for i in range(3):
            mock_events.append({
                'test_name': 'Classification Test',
                'agent_id': f'classifier_{i+1}',
                'event_type': 'start',
                'timestamp': base_time + i * 60,  # Start at 0s, 60s, 120s
                'data': {
                    'model': ['gpt-4o-mini', 'claude-3-5-sonnet', 'gemini-2.5-flash'][i],
                    'batch_size': 50,
                    'task': f'Classification Batch {i+1}'
                }
            })
            mock_events.append({
                'test_name': 'Classification Test',
                'agent_id': f'classifier_{i+1}',
                'event_type': 'complete',
                'timestamp': base_time + i * 60 + 45,  # Duration: 45s each
                'data': {
                    'accuracy': [0.87, 0.91, 0.89][i],
                    'f1_score': [0.85, 0.90, 0.88][i],
                    'cost_usd': [0.12, 0.18, 0.08][i],
                    'tokens': [2400, 3200, 1800][i]
                }
            })

        # Mock Test 5: Orchestrator
        orchestrator_start = 200  # Start at 200s
        mock_events.append({
            'test_name': 'Test 5',
            'agent_id': 'orchestrator',
            'event_type': 'start',
            'timestamp': orchestrator_start,
            'data': {
                'task': 'Research: TechCorp Fundraising',
                'mode': 'leaf_scaffold',
                'target_person': 'John Smith, CEO'
            }
        })

        # Sub-agents for orchestrator (parallel execution)
        sub_agents = [
            ('web_researcher', 'LinkedIn Profile Analysis', 25, 0.92),
            ('knowledge_retriever', 'Company Background Research', 35, 0.88),
            ('content_generator', 'Executive Summary Generation', 20, 0.95),
            ('validator', 'Fact Verification', 15, 0.85)
        ]

        for i, (agent_type, task, duration, score) in enumerate(sub_agents):
            start_time = orchestrator_start + 5 + i * 10  # Staggered start
            mock_events.extend([
                {
                    'test_name': 'Test 5',
                    'agent_id': f'{agent_type}_{i+1}',
                    'event_type': 'start',
                    'timestamp': start_time,
                    'data': {
                        'agent_type': agent_type,
                        'task': task,
                        'parent': 'orchestrator'
                    }
                },
                {
                    'test_name': 'Test 5',
                    'agent_id': f'{agent_type}_{i+1}',
                    'event_type': 'complete',
                    'timestamp': start_time + duration,
                    'data': {
                        'score': score,
                        'output_size': f'{1.2 + i * 0.3:.1f} KB',
                        'confidence': score * 0.9
                    }
                }
            ])

        mock_events.append({
            'test_name': 'Test 5',
            'agent_id': 'orchestrator',
            'event_type': 'complete',
            'timestamp': orchestrator_start + 80,  # Total duration: 80s
            'data': {
                'final_score': 0.89,
                'total_cost': 0.45,
                'research_quality': 'High',
                'hallucination_risk': 0.12
            }
        })

        return mock_events

    # Get data sources
    tracker = st.session_state.get('execution_tracker')
    historical_data = load_historical_data()

    # Data source selector
    col1, col2, col3 = st.columns([2, 1, 1])

    with col1:
        data_source = st.selectbox(
            "📊 Data Source",
            options=['current_session', 'historical', 'mock_demo'],
            format_func=lambda x: {
                'current_session': '🔄 Current Session Data',
                'historical': '📚 Historical Runs',
                'mock_demo': '🎭 Demo Data (Mock)'
            }[x],
            index=2 if not tracker or not tracker.events else 0
        )

    with col2:
        if st.button("🔄 Refresh", width='content'):
            st.rerun()

    with col3:
        auto_refresh = st.checkbox("Auto-refresh", value=False)

    # Load appropriate data based on selection
    if data_source == 'current_session' and tracker and tracker.events:
        events_data = tracker.events
        st.success(f"✅ Loaded {len(events_data)} events from current session")
    elif data_source == 'historical' and historical_data:
        # Combine all historical events
        events_data = []
        for run in historical_data:
            for event in run['data'].get('execution_log', []):
                events_data.append(event)
        st.success(f"✅ Loaded {len(events_data)} events from {len(historical_data)} historical runs")
    elif data_source == 'mock_demo':
        events_data = generate_mock_data()
        st.info(f"🎭 Generated {len(events_data)} mock events for demonstration")
    else:
        events_data = []
        if data_source == 'current_session':
            st.info("No execution data in current session. Run any test to see live data.")
        elif data_source == 'historical':
            st.info("No historical data found. Historical runs will appear here after tests complete.")
        else:
            st.info("Select a data source to view the dashboard.")

    if events_data:
        # Enhanced Gantt chart renderer for mock/historical/current data
        def render_enhanced_gantt_from_events(events, title="Agent Execution Timeline"):
            """Render enhanced Gantt chart from event data with vibrant, high-contrast colors."""
            # Vibrant, high-contrast color scheme for maximum visual distinction
            AGENT_COLORS_VIBRANT = {
                'orchestrator': '#6366F1',      # Vibrant Indigo
                'classifier': '#10B981',        # Emerald Green
                'web_researcher': '#0EA5E9',    # Sky Blue
                'code_executor': '#F59E0B',     # Amber Orange
                'knowledge_retriever': '#8B5CF6', # Purple
                'content_generator': '#EC4899',  # Hot Pink
                'validator': '#14B8A6',         # Teal
                'editor': '#EF4444',            # Red
                'main_agent': '#22C55E',        # Bright Green
                'sub_agent': '#64748B'          # Slate Gray
            }

            # Build agent timeline
            agents = {}
            min_time = float('inf')

            for event in events:
                agent_id = event.get('agent_id', 'unknown')
                timestamp = event.get('timestamp', 0)

                # Track minimum timestamp for normalization
                min_time = min(min_time, timestamp)

                if agent_id not in agents:
                    agents[agent_id] = {
                        'id': agent_id,
                        'name': event.get('data', {}).get('task', agent_id),
                        'type': event.get('data', {}).get('agent_type', 'task'),
                        'start': timestamp,
                        'end': None,
                        'status': 'running',
                        'data': event.get('data', {})
                    }

                if event.get('event_type') == 'complete':
                    agents[agent_id]['end'] = timestamp
                    agents[agent_id]['status'] = 'complete'
                    agents[agent_id]['data'].update(event.get('data', {}))

            if not agents:
                st.info("No agent data to visualize.")
                return

            # Normalize timestamps to start at 0
            if min_time == float('inf'):
                min_time = 0

            # Create Gantt data with normalized timestamps
            gantt_data = []
            for agent in sorted(agents.values(), key=lambda x: x['start']):
                normalized_start = agent['start'] - min_time
                end_time = agent.get('end') or (agent['start'] + 30)
                normalized_end = end_time - min_time
                duration = normalized_end - normalized_start

                # Determine color (vibrant, high-contrast scheme)
                agent_type = agent.get('type', 'task')
                color = AGENT_COLORS_VIBRANT.get(agent_type, '#94A3B8')

                if agent['status'] == 'error':
                    color = '#DC2626'  # Bright Red for errors
                elif agent['status'] == 'running':
                    color = color + 'CC'  # Slightly transparent for running

                gantt_data.append({
                    'Task': agent['name'],
                    'Start': normalized_start,
                    'Finish': normalized_end,
                    'Duration': duration,
                    'Type': agent_type,
                    'Status': agent['status'],
                    'Color': color
                })

            # Create Plotly figure with enhanced styling
            fig = go.Figure()

            for row in gantt_data:
                fig.add_trace(go.Bar(
                    y=[row['Task']],
                    x=[row['Duration']],
                    base=row['Start'],
                    orientation='h',
                    name=row['Task'],
                    marker=dict(
                        color=row['Color'],
                        line=dict(color='rgba(255,255,255,0.3)', width=2),  # White border for contrast
                        pattern=dict(
                            shape="" if row['Status'] == 'complete' else "/" if row['Status'] == 'running' else "x"
                        )
                    ),
                    text=f"{row['Duration']:.1f}s",
                    textposition='inside',
                    textfont=dict(color='white', size=12, family='Arial Black', weight='bold'),
                    hovertemplate=(
                        f"<b style='font-size:14px'>{row['Task']}</b><br>" +
                        f"<b>Type:</b> {row['Type']}<br>" +
                        f"<b>Start:</b> {row['Start']:.1f}s<br>" +
                        f"<b>Duration:</b> {row['Duration']:.1f}s<br>" +
                        f"<b>Status:</b> {row['Status']}<br>" +
                        "<extra></extra>"
                    ),
                    showlegend=False
                ))

            fig.update_layout(
                title=dict(
                    text=title,
                    font=dict(size=20, weight='bold', color='#1F2937')
                ),
                xaxis=dict(
                    title="Time (seconds)",
                    titlefont=dict(size=14, weight='bold'),
                    gridcolor='#D1D5DB',
                    showgrid=True,
                    zeroline=True,
                    zerolinecolor='#6B7280',
                    zerolinewidth=2
                ),
                yaxis=dict(
                    title="Agent / Task",
                    titlefont=dict(size=14, weight='bold'),
                    gridcolor='#E5E7EB',
                    tickfont=dict(size=11, weight='bold')
                ),
                height=max(450, len(gantt_data) * 50),
                showlegend=False,
                hovermode='closest',
                plot_bgcolor='#FFFFFF',
                paper_bgcolor='#F9FAFB',
                margin=dict(l=220, r=60, t=90, b=70),
                font=dict(family='Arial, sans-serif')
            )

            st.plotly_chart(fig, use_container_width=True, config=PLOTLY_CONFIG)

            # Color Legend
            st.markdown("### 🎨 Agent Type Color Legend")
            legend_cols = st.columns(5)

            legend_items = [
                ("Orchestrator", AGENT_COLORS_VIBRANT['orchestrator']),
                ("Web Researcher", AGENT_COLORS_VIBRANT['web_researcher']),
                ("Code Executor", AGENT_COLORS_VIBRANT['code_executor']),
                ("Knowledge Retriever", AGENT_COLORS_VIBRANT['knowledge_retriever']),
                ("Content Generator", AGENT_COLORS_VIBRANT['content_generator']),
                ("Validator", AGENT_COLORS_VIBRANT['validator']),
                ("Classifier", AGENT_COLORS_VIBRANT['classifier']),
                ("Main Agent", AGENT_COLORS_VIBRANT['main_agent']),
                ("Editor", AGENT_COLORS_VIBRANT['editor']),
                ("Sub Agent", AGENT_COLORS_VIBRANT['sub_agent'])
            ]

            for idx, (name, color) in enumerate(legend_items[:5]):
                with legend_cols[idx]:
                    st.markdown(f"""
                    <div style="display: flex; align-items: center; margin-bottom: 8px;">
                        <div style="width: 20px; height: 20px; background-color: {color};
                                    border: 2px solid white; border-radius: 4px; margin-right: 8px;
                                    box-shadow: 0 2px 4px rgba(0,0,0,0.2);"></div>
                        <span style="font-size: 11px; font-weight: 600;">{name}</span>
                    </div>
                    """, unsafe_allow_html=True)

            if len(legend_items) > 5:
                legend_cols2 = st.columns(5)
                for idx, (name, color) in enumerate(legend_items[5:]):
                    with legend_cols2[idx]:
                        st.markdown(f"""
                        <div style="display: flex; align-items: center; margin-bottom: 8px;">
                            <div style="width: 20px; height: 20px; background-color: {color};
                                        border: 2px solid white; border-radius: 4px; margin-right: 8px;
                                        box-shadow: 0 2px 4px rgba(0,0,0,0.2);"></div>
                            <span style="font-size: 11px; font-weight: 600;">{name}</span>
                        </div>
                        """, unsafe_allow_html=True)

            st.divider()

            # Summary metrics
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Total Agents", len(agents))
            with col2:
                completed = len([a for a in agents.values() if a['status'] == 'complete'])
                st.metric("Completed", completed)
            with col3:
                total_time = max([a.get('end', a['start']) - min_time for a in agents.values()]) if agents else 0
                st.metric("Total Time", f"{total_time:.1f}s")
            with col4:
                avg_duration = sum([(a.get('end', a['start']) - a['start']) for a in agents.values()]) / len(agents) if agents else 0
                st.metric("Avg Duration", f"{avg_duration:.1f}s")

        # Render the enhanced Gantt chart
        st.subheader("📊 Execution Timeline")
        render_enhanced_gantt_from_events(events_data, title=f"{data_source.replace('_', ' ').title()} - Agent Execution Timeline")

        st.divider()

        # Test selector with available tests
        test_name_map = {
            "classification": "Classification Test",
            "pruning": "Pruning Test",
            "test5": "Test 5",
            "smoke_test": "Smoke Test Run"
        }

        col1, col2, col3 = st.columns([2, 1, 1])

        with col1:
            # Build options dynamically based on available data
            if data_source == 'mock_demo':
                test_options = ["classification", "test5"]
            else:
                # Extract from events
                available_tests = set()
                for event in events_data:
                    test_name = event.get('test_name', '')
                    if 'Classification' in test_name:
                        available_tests.add('classification')
                    elif 'Test 5' in test_name or 'Orchestrator' in test_name:
                        available_tests.add('test5')
                    elif 'Pruning' in test_name:
                        available_tests.add('pruning')

                test_options = sorted(list(available_tests)) if available_tests else ["test5"]

            # Use old selector for backward compatibility
            test_selector = st.selectbox(
                "Select Test to Visualize",
                options=test_options,
                format_func=lambda x: {
                    "classification": "Classification Tests (1-3)",
                    "pruning": "Context Pruning (Test 4)",
                    "test5": "Agent Self-Refinement (Test 5)",
                    "smoke_test": "🔍 Smoke Test Run"
                }.get(x, x),
                index=test_options.index(st.session_state.get('selected_test_in_dashboard_selector', test_options[0])) if st.session_state.get('selected_test_in_dashboard_selector') in test_options else 0,
                key='dashboard_test_selector'
            )

            # Map to actual test name
            selected_test = test_name_map.get(test_selector, "Classification Test")

            # Store selection for persistence
            st.session_state['selected_test_in_dashboard_selector'] = test_selector

        with col2:
            if st.button("🔄 Refresh", key='dashboard_refresh_btn_2', width='content'):
                st.rerun()

        with col3:
            auto_refresh = st.checkbox("Auto-refresh", key='dashboard_auto_refresh_2', value=False)

        st.divider()

        # Add historical run selector for Test 5
        if test_selector == "test5":
            st.markdown("### 📂 Load Past Run (Historical Replay)")

            all_runs = DashboardLogger.list_all_runs()
            test5_runs = [r for r in all_runs if r.get("test_type") == "orchestrator"]

            if test5_runs:
                run_options = ["Live Execution"] + [
                    f"{r['run_id']} - {r.get('model', 'unknown')} - {r.get('timestamp', '')[:19]} ({r.get('summary_metrics', {}).get('final_score', 0.0):.2%} accuracy)"
                    for r in test5_runs
                ]

                selected_run = st.selectbox(
                    "Select Run to View",
                    options=run_options,
                    help="View historical test runs without re-execution"
                )

                # Load historical data if selected
                if selected_run != "Live Execution":
                    run_id = selected_run.split(" - ")[0]
                    try:
                        historical_data = DashboardLogger.load_run(run_id)
                        st.session_state['historical_run_data'] = historical_data
                        st.session_state['view_mode'] = 'historical'
                        st.success(f"✅ Loaded historical run: {run_id}")
                    except Exception as e:
                        st.error(f"Failed to load run: {e}")
                        st.session_state['view_mode'] = 'live'
                else:
                    st.session_state['view_mode'] = 'live'
            else:
                st.info("No historical Test 5 runs available yet. Run Test 5 to create logs.")
                st.session_state['view_mode'] = 'live'

            st.divider()

        # Create enhanced dashboard tabs
        dashboard_tabs = st.tabs([
            "📊 Overview",
            "📈 Gantt Timeline",
            "📋 Task Cards",
            "📜 Event Log",
            "📥 Export",
            "🧠 Memory Inspector",
            "🔒 Security Audit Log",
            "📜 Rethink History",
            "🔍 Interactive Tools"
        ])

        # --- TAB 1: OVERVIEW ---
        with dashboard_tabs[0]:
            st.subheader("Execution Overview")

            test_events = tracker.get_test_events(selected_test)

            if not test_events:
                st.info(f"No execution data for {selected_test}. Run the test first.")
            else:
                # KPIs
                col1, col2, col3, col4 = st.columns(4)

                with col1:
                    total_agents = len(set([e.agent_id for e in test_events]))
                    st.metric("Total Agents", total_agents)

                with col2:
                    completed = len([e for e in test_events if e.status == "complete"])
                    st.metric("Completed", completed)

                with col3:
                    running = len(tracker.active_agents)
                    st.metric("Running", running)

                with col4:
                    errors = len([e for e in test_events if e.status == "error"])
                    st.metric("Errors", errors)

                st.divider()

                # Timeline summary
                st.subheader("Timeline Summary")

                start_time = min([e.timestamp for e in test_events])
                end_time = max([e.timestamp for e in test_events])
                total_duration = end_time - start_time

                col1, col2 = st.columns(2)

                with col1:
                    st.metric("Total Duration", f"{total_duration:.1f}s")

                with col2:
                    durations = [e.duration for e in test_events if e.duration]
                    avg_agent_duration = sum(durations) / max(len(durations), 1) if durations else 0
                    st.metric("Avg Agent Duration", f"{avg_agent_duration:.2f}s")

        # --- TAB 2: GANTT TIMELINE ---
        with dashboard_tabs[1]:
            st.subheader("Execution Timeline (Gantt Chart)")

            test_events = tracker.get_test_events(selected_test)

            if not test_events:
                st.info("No events recorded for this test.")
            else:
                # Build Gantt data
                gantt_df = generate_gantt_data(selected_test, tracker)

                if gantt_df.empty:
                    st.info("No timeline data available.")
                else:
                    # Color scale based on status
                    color_map = {
                        'complete': '#10B981',
                        'running': '#F59E0B',
                        'error': '#EF4444',
                        'pending': '#94A3B8'
                    }

                    colors = [color_map.get(status, '#94A3B8') for status in gantt_df['Status']]

                    fig = go.Figure()

                    for idx, row in gantt_df.iterrows():
                        # Add hover text with metadata
                        hover_text = f"<b>{row['Task']}</b><br>"
                        hover_text += f"Start: {row['Start']:.1f}s<br>"
                        hover_text += f"Duration: {row['Duration']:.2f}s<br>"
                        hover_text += f"Status: {row['Status']}<br>"

                        if 'Metadata' in row and row['Metadata']:
                            metadata = row['Metadata']
                            if isinstance(metadata, dict):
                                for key, value in list(metadata.items())[:3]:  # Show first 3 metadata items
                                    if key != 'code':  # Skip code in hover
                                        hover_text += f"{key}: {value}<br>"

                        fig.add_trace(go.Bar(
                            y=[row['Task']],
                            x=[row['Duration']],
                            base=row['Start'],
                            orientation='h',
                            name=row['Task'],
                            marker=dict(color=colors[idx]),
                            text=f"{row['Progress']:.0f}%",
                            textposition='inside',
                            hovertemplate=hover_text + "<extra></extra>",
                            showlegend=False
                        ))

                    fig.update_layout(
                        title=f"{selected_test}: Execution Timeline",
                        xaxis_title="Time (seconds)",
                        yaxis_title="Agent / Task",
                        height=max(400, len(gantt_df) * 40),
                        showlegend=False,
                        hovermode='closest',
                        plot_bgcolor='#F7F7FB',
                        paper_bgcolor='white'
                    )

                    st.plotly_chart(fig, width='content', config=PLOTLY_CONFIG)

                    # Summary metrics below chart
                    col1, col2, col3, col4 = st.columns(4)

                    with col1:
                        st.metric("Total Tasks", len(gantt_df))

                    with col2:
                        completed = len(gantt_df[gantt_df['Status'] == 'complete'])
                        st.metric("Completed", completed)

                    with col3:
                        total_duration = gantt_df['Start'].max() + gantt_df.loc[gantt_df['Start'].idxmax(), 'Duration']
                        st.metric("Total Time", f"{total_duration:.1f}s")

                    with col4:
                        avg_progress = gantt_df['Progress'].mean()
                        st.metric("Avg Progress", f"{avg_progress:.0f}%")

        # --- TAB 3: TASK CARDS ---
        with dashboard_tabs[2]:
            render_task_cards(selected_test, tracker)

        # --- TAB 4: EVENT LOG ---
        with dashboard_tabs[3]:
            st.subheader("Event Log")

            # Check if we have historical Test 5 data with enhanced logging
            view_mode = st.session_state.get('view_mode', 'live')

            if view_mode == 'historical' and 'historical_run_data' in st.session_state and test_selector == "test5":
                # Enhanced Event Log for Test 5 with structured entries
                run_data = st.session_state['historical_run_data']
                execution_log = run_data.get('execution_log', [])

                if not execution_log:
                    st.info("No execution log data available for this run.")
                else:
                    st.markdown("### Structured Execution Log")

                    # Enhanced filter controls
                    col1, col2, col3, col4 = st.columns(4)

                    with col1:
                        all_event_types = set(entry.get('event_type', 'UNKNOWN') for entry in execution_log)
                        event_type_filter = st.multiselect(
                            "Event Type",
                            options=sorted(all_event_types),
                            default=list(all_event_types),
                            help="Filter by event type: TOOL_RULE_ENFORCED, MEMORY_WRITE, SECURITY_AUDIT, RETHINK_TRIGGERED"
                        )

                    with col2:
                        all_severities = set(entry.get('severity', 'INFO') for entry in execution_log)
                        severity_filter = st.multiselect(
                            "Severity",
                            options=sorted(all_severities),
                            default=list(all_severities),
                            help="Filter by severity: INFO, WARNING, ERROR, SECURITY_ALERT"
                        )

                    with col3:
                        all_agents = set(entry.get('agent', 'Unknown') for entry in execution_log)
                        agent_filter = st.multiselect(
                            "Agent",
                            options=sorted(all_agents),
                            default=list(all_agents)
                        )

                    with col4:
                        # Turn range filter
                        max_turn = max((entry.get('turn', 0) for entry in execution_log), default=0)
                        turn_range = st.slider(
                            "Turn Range",
                            min_value=0,
                            max_value=max_turn,
                            value=(0, max_turn),
                            help="Filter events by turn number"
                        )

                    # Apply filters
                    filtered_log = [
                        entry for entry in execution_log
                        if entry.get('event_type') in event_type_filter
                        and entry.get('severity') in severity_filter
                        and entry.get('agent') in agent_filter
                        and turn_range[0] <= entry.get('turn', 0) <= turn_range[1]
                    ]

                    # Display with visual indicators
                    if filtered_log:
                        st.caption(f"Showing {len(filtered_log)} of {len(execution_log)} events")

                        # Event type icons and colors
                        event_icons = {
                            'TOOL_RULE_ENFORCED': '⚙️',
                            'MEMORY_WRITE': '💾',
                            'SECURITY_AUDIT': '🔒',
                            'RETHINK_TRIGGERED': '🔄',
                            'CODE_GENERATED': '📝',
                            'VALIDATION': '✅'
                        }

                        severity_colors = {
                            'INFO': '🔵',
                            'WARNING': '🟡',
                            'ERROR': '🔴',
                            'SECURITY_ALERT': '🔴'
                        }

                        # Create enhanced table
                        log_data = []
                        for entry in filtered_log:
                            event_type = entry.get('event_type', 'UNKNOWN')
                            severity = entry.get('severity', 'INFO')

                            icon = event_icons.get(event_type, '📋')
                            color = severity_colors.get(severity, '⚪')

                            log_data.append({
                                'Turn': entry.get('turn', 0),
                                'Event': f"{icon} {event_type}",
                                'Severity': f"{color} {severity}",
                                'Agent': entry.get('agent', 'Unknown'),
                                'Message': entry.get('message', '')[:80] + "..." if len(entry.get('message', '')) > 80 else entry.get('message', ''),
                                'Timestamp': entry.get('timestamp', '')[:19]
                            })

                        df_log = pd.DataFrame(log_data)
                        st.dataframe(df_log, use_container_width=True, height=500)

                        # Event type distribution
                        st.divider()
                        st.markdown("### Event Distribution")

                        col1, col2 = st.columns(2)

                        with col1:
                            # Event type counts
                            from collections import Counter
                            event_counts = Counter([entry.get('event_type', 'UNKNOWN') for entry in filtered_log])

                            st.markdown("**Event Types:**")
                            for event_type, count in event_counts.most_common():
                                icon = event_icons.get(event_type, '📋')
                                st.text(f"{icon} {event_type}: {count}")

                        with col2:
                            # Severity counts
                            severity_counts = Counter([entry.get('severity', 'INFO') for entry in filtered_log])

                            st.markdown("**Severity Levels:**")
                            for severity, count in severity_counts.most_common():
                                color = severity_colors.get(severity, '⚪')
                                st.text(f"{color} {severity}: {count}")
                    else:
                        st.info("No events match the selected filters.")
            else:
                # Standard Event Log for other tests
                test_events = tracker.get_test_events(selected_test)

                if not test_events:
                    st.info("No events to display.")
                else:
                    # Filter controls
                    col1, col2 = st.columns([1, 1])

                    with col1:
                        event_type_filter = st.multiselect(
                            "Event Type",
                            options=["start", "progress", "complete", "error"],
                            default=["start", "complete", "error"]
                        )

                    with col2:
                        agent_type_filter = st.multiselect(
                            "Agent Type",
                            options=["orchestrator", "main_agent", "sub_agent", "batch"],
                            default=["orchestrator", "main_agent", "sub_agent", "batch"]
                        )

                    # Filter events
                    filtered_events = [
                        e for e in test_events
                        if e.event_type in event_type_filter and e.agent_type in agent_type_filter
                    ]

                    # Display as table
                    if filtered_events:
                        df_events = pd.DataFrame([
                            {
                                'Timestamp': f"{e.timestamp:.2f}s",
                                'Event': e.event_type,
                                'Agent': e.agent_name,
                                'Type': e.agent_type,
                                'Status': e.status,
                                'Progress': f"{e.progress:.0f}%",
                                'Duration': f"{e.duration:.2f}s" if e.duration else "—",
                                'Metadata': str(e.metadata)[:50] + "..." if e.metadata else ""
                            }
                            for e in filtered_events
                        ])

                        st.dataframe(df_events, width='content', height=400)
                    else:
                        st.info("No events match the filters.")

        # --- TAB 5: EXPORT ---
        with dashboard_tabs[4]:
            st.subheader("Export Execution Data")

            test_events = tracker.get_test_events(selected_test)

            if not test_events:
                st.info("No data to export.")
            else:
                # Export options
                col1, col2 = st.columns(2)

                with col1:
                    st.markdown("**Export Timeline Data**")

                    df_export = tracker.export_timeline(selected_test)

                    csv_bytes = df_export.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        "📥 Download CSV",
                        data=csv_bytes,
                        file_name=f"{selected_test.lower().replace(' ', '_')}_timeline.csv",
                        mime="text/csv",
                        width='content'
                    )

                    json_str = df_export.to_json(orient='records', indent=2)
                    st.download_button(
                        "📥 Download JSON",
                        data=json_str,
                        file_name=f"{selected_test.lower().replace(' ', '_')}_timeline.json",
                        mime="application/json",
                        width='content'
                    )

                with col2:
                    st.markdown("**Export Gantt Chart**")

                    # Generate Gantt data
                    gantt_df = generate_gantt_data(selected_test, tracker)

                    if not gantt_df.empty:
                        csv_gantt = gantt_df.to_csv(index=False).encode('utf-8')
                        st.download_button(
                            "📥 Download Gantt CSV",
                            data=csv_gantt,
                            file_name=f"{selected_test.lower().replace(' ', '_')}_gantt.csv",
                            mime="text/csv",
                            width='content'
                        )

                # Preview
                st.markdown("**Data Preview**")
                st.dataframe(df_export.head(20), width='content')

        # --- TAB 6: MEMORY INSPECTOR ---
        with dashboard_tabs[5]:
            st.subheader("🧠 Memory Inspector")

            # Get historical data if in historical mode
            view_mode = st.session_state.get('view_mode', 'live')

            if view_mode == 'historical' and 'historical_run_data' in st.session_state:
                run_data = st.session_state['historical_run_data']
                memory_snapshots = run_data.get('memory_snapshots', [])

                if not memory_snapshots:
                    st.info("No memory data available for this run.")
                else:
                    # Get latest snapshot
                    latest_snapshot = memory_snapshots[-1]

                    st.markdown("### Current Memory Blocks")
                    core_blocks = latest_snapshot.get('core_blocks', {})

                    for block_name, block_data in core_blocks.items():
                        with st.expander(f"📝 {block_name}", expanded=False):
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Version", block_data.get('version', 1))
                            with col2:
                                st.metric("Last Modified Turn", block_data.get('last_modified_turn', 0))
                            with col3:
                                st.metric("Modifications", block_data.get('modification_count', 0))

                            st.code(block_data.get('content', ''), language='text')

                    st.divider()

                    st.markdown("### Archival Memory Index")
                    archival_entries = latest_snapshot.get('archival_entries', [])

                    if archival_entries:
                        # Pagination
                        page_size = 50
                        total_pages = (len(archival_entries) + page_size - 1) // page_size

                        page = st.number_input("Page", min_value=1, max_value=total_pages, value=1)
                        start_idx = (page - 1) * page_size
                        end_idx = min(start_idx + page_size, len(archival_entries))

                        # Filters
                        col1, col2 = st.columns(2)
                        with col1:
                            all_tags = set()
                            for entry in archival_entries:
                                all_tags.update(entry.get('tags', []))

                            tag_filter = st.multiselect("Filter by Tags", options=sorted(all_tags))

                        with col2:
                            all_agents = set(entry.get('source_agent', 'Unknown') for entry in archival_entries)
                            agent_filter = st.multiselect("Filter by Source Agent", options=sorted(all_agents))

                        # Apply filters
                        filtered_entries = archival_entries
                        if tag_filter:
                            filtered_entries = [e for e in filtered_entries if any(tag in e.get('tags', []) for tag in tag_filter)]
                        if agent_filter:
                            filtered_entries = [e for e in filtered_entries if e.get('source_agent') in agent_filter]

                        # Display entries
                        st.caption(f"Showing {start_idx + 1}-{end_idx} of {len(filtered_entries)} entries")

                        for entry in filtered_entries[start_idx:end_idx]:
                            with st.expander(f"[{entry.get('source_agent', 'Unknown')}] {entry.get('content', '')[:100]}...", expanded=False):
                                st.text(entry.get('content', ''))
                                st.caption(f"Tags: {', '.join(entry.get('tags', []))} | {entry.get('timestamp', '')}")
                    else:
                        st.info("No archival memory entries.")

                    st.divider()

                    st.markdown("### Memory Statistics")
                    stats = latest_snapshot.get('statistics', {})

                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("Total Entries", len(archival_entries))
                    with col2:
                        st.metric("Total Retrievals", stats.get('total_retrievals', 0))
                    with col3:
                        st.metric("Cache Hits", stats.get('cache_hits', 0))
                    with col4:
                        avg_latency = stats.get('avg_retrieval_latency_ms', 0.0)
                        st.metric("Avg Retrieval Latency", f"{avg_latency:.2f}ms")
            else:
                st.info("Memory Inspector is only available for Test 5 historical runs. Run Test 5 and then load a past run to view memory data.")

        # --- TAB 7: SECURITY AUDIT LOG ---
        with dashboard_tabs[6]:
            st.subheader("🔒 Security Audit Log")

            view_mode = st.session_state.get('view_mode', 'live')

            if view_mode == 'historical' and 'historical_run_data' in st.session_state:
                run_data = st.session_state['historical_run_data']
                security_audits = run_data.get('security_audits', [])

                if not security_audits:
                    st.info("No security audit data available for this run.")
                else:
                    st.markdown("### Audit Events Table")

                    # Filters
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        all_operations = set(audit.get('operation_type', 'Unknown') for audit in security_audits)
                        operation_filter = st.multiselect("Operation Type", options=sorted(all_operations), default=list(all_operations))

                    with col2:
                        all_risk_levels = set(audit.get('risk_level', 'UNKNOWN') for audit in security_audits)
                        risk_filter = st.multiselect("Risk Level", options=sorted(all_risk_levels), default=list(all_risk_levels))

                    with col3:
                        all_statuses = set(audit.get('status', 'Unknown') for audit in security_audits)
                        status_filter = st.multiselect("Status", options=sorted(all_statuses), default=list(all_statuses))

                    # Apply filters
                    filtered_audits = [
                        audit for audit in security_audits
                        if audit.get('operation_type') in operation_filter
                        and audit.get('risk_level') in risk_filter
                        and audit.get('status') in status_filter
                    ]

                    # Display table
                    audit_data = []
                    for audit in filtered_audits:
                        status_icon = {
                            'Pass': '🟢',
                            'Blocked': '🔴',
                            'Warning': '🟡'
                        }.get(audit.get('status', ''), '⚪')

                        audit_data.append({
                            'Turn': audit.get('turn', 0),
                            'Agent': audit.get('agent', 'Unknown'),
                            'Operation': audit.get('operation_type', 'Unknown'),
                            'Risk': audit.get('risk_level', 'UNKNOWN'),
                            'Status': f"{status_icon} {audit.get('status', 'Unknown')}",
                            'Details': str(audit.get('details', {}))[:50] + "..."
                        })

                    if audit_data:
                        st.dataframe(pd.DataFrame(audit_data), use_container_width=True, height=400)

                        # Security Metrics Dashboard
                        st.divider()
                        st.markdown("### Security Metrics")

                        col1, col2, col3, col4 = st.columns(4)

                        with col1:
                            st.metric("Total Audits", len(security_audits))

                        with col2:
                            blocked = len([a for a in security_audits if a.get('status') == 'Blocked'])
                            st.metric("Blocked Operations", blocked)

                        with col3:
                            warnings = len([a for a in security_audits if a.get('status') == 'Warning'])
                            st.metric("Warnings", warnings)

                        with col4:
                            passed = len([a for a in security_audits if a.get('status') == 'Pass'])
                            st.metric("Passed", passed)

                        # Pie chart: Distribution of operation types
                        operation_counts = {}
                        for audit in security_audits:
                            op_type = audit.get('operation_type', 'Unknown')
                            operation_counts[op_type] = operation_counts.get(op_type, 0) + 1

                        if operation_counts:
                            fig = go.Figure(data=[go.Pie(
                                labels=list(operation_counts.keys()),
                                values=list(operation_counts.values()),
                                hole=0.3
                            )])

                            fig.update_layout(
                                title="Distribution of Operation Types",
                                height=300
                            )

                            st.plotly_chart(fig, use_container_width=True, config=PLOTLY_CONFIG)
                    else:
                        st.info("No audits match the selected filters.")
            else:
                st.info("Security Audit Log is only available for Test 5 historical runs. Run Test 5 and then load a past run to view security data.")

        # --- TAB 8: RETHINK HISTORY ---
        with dashboard_tabs[7]:
            st.subheader("📜 Rethink History")

            view_mode = st.session_state.get('view_mode', 'live')

            if view_mode == 'historical' and 'historical_run_data' in st.session_state:
                run_data = st.session_state['historical_run_data']
                rethink_history = run_data.get('rethink_history', [])

                if not rethink_history:
                    st.info("No rethink events recorded for this run.")
                else:
                    st.markdown("### Policy Modification Log")

                    # Display rethink events
                    for idx, event in enumerate(rethink_history, 1):
                        with st.expander(f"🔄 Rethink #{idx} - Turn {event.get('turn', 0)} - {event.get('block_name', 'Unknown')}", expanded=False):
                            col1, col2 = st.columns(2)

                            with col1:
                                st.markdown("**Trigger:**")
                                st.text(event.get('trigger', 'N/A'))

                                st.markdown("**Change Summary:**")
                                st.text(event.get('change_summary', 'N/A'))

                            with col2:
                                st.markdown("**Revert Point ID:**")
                                st.code(event.get('revert_point_id', 'N/A'))

                                st.markdown("**Timestamp:**")
                                st.text(event.get('timestamp', 'N/A'))

                            st.divider()

                            # Side-by-side diff
                            st.markdown("**Content Diff:**")
                            col_before, col_after = st.columns(2)

                            with col_before:
                                st.markdown("*Before:*")
                                st.code(event.get('old_content', ''), language='text')

                            with col_after:
                                st.markdown("*After:*")
                                st.code(event.get('new_content', ''), language='text')

                    st.divider()

                    # Self-Correction Analytics
                    st.markdown("### Self-Correction Analytics")

                    # Most frequently modified blocks
                    from collections import Counter
                    block_counts = Counter([event.get('block_name', 'Unknown') for event in rethink_history])
                    most_common = block_counts.most_common(3)

                    if most_common:
                        st.markdown("**Most Frequently Modified Blocks:**")
                        for block_name, count in most_common:
                            st.text(f"  • {block_name}: {count} modification(s)")

                    # Average turns between rethinks
                    if len(rethink_history) > 1:
                        turns = [event.get('turn', 0) for event in rethink_history]
                        avg_gap = sum(turns[i+1] - turns[i] for i in range(len(turns)-1)) / (len(turns) - 1)
                        st.metric("Average Turns Between Rethinks", f"{avg_gap:.1f}")
            else:
                st.info("Rethink History is only available for Test 5 historical runs. Run Test 5 and then load a past run to view rethink data.")

        # --- TAB 9: INTERACTIVE TOOLS ---
        with dashboard_tabs[8]:
            st.subheader("🔍 Interactive Tools")

            view_mode = st.session_state.get('view_mode', 'live')

            if view_mode == 'historical' and 'historical_run_data' in st.session_state:
                run_data = st.session_state['historical_run_data']
                memory_snapshots = run_data.get('memory_snapshots', [])

                if not memory_snapshots:
                    st.info("No memory data available for interactive tools.")
                else:
                    latest_snapshot = memory_snapshots[-1]
                    archival_entries = latest_snapshot.get('archival_entries', [])

                    # Memory Search Simulator
                    st.markdown("### 🔍 Memory Search Simulator")
                    st.caption("Demonstrate RAG mechanism interactively")

                    col1, col2 = st.columns([3, 1])

                    with col1:
                        search_query = st.text_input("Search Query", placeholder="Enter keywords to search archival memory...")

                    with col2:
                        # Tag filters
                        all_tags = set()
                        for entry in archival_entries:
                            all_tags.update(entry.get('tags', []))

                        tag_filters = st.multiselect("Filter by Tags", options=sorted(all_tags))

                    if st.button("🔍 Search Archival Memory", type="primary"):
                        if not search_query:
                            st.warning("Please enter a search query.")
                        else:
                            # Simple keyword search
                            results = []
                            query_lower = search_query.lower()

                            for entry in archival_entries:
                                # Tag filter
                                if tag_filters and not any(tag in entry.get('tags', []) for tag in tag_filters):
                                    continue

                                # Keyword matching
                                content = entry.get('content', '').lower()
                                if query_lower in content:
                                    # Calculate simple relevance score (keyword frequency)
                                    relevance = content.count(query_lower) / max(len(content.split()), 1)
                                    results.append((entry, relevance))

                            # Sort by relevance
                            results.sort(key=lambda x: x[1], reverse=True)
                            top_results = results[:5]

                            if top_results:
                                st.success(f"Found {len(results)} matching entries. Showing top 5:")

                                for idx, (entry, relevance) in enumerate(top_results, 1):
                                    with st.expander(f"Result #{idx} (Relevance: {relevance:.4f})", expanded=idx==1):
                                        st.text(entry.get('content', ''))
                                        st.caption(f"Source: {entry.get('source_agent', 'Unknown')} | Tags: {', '.join(entry.get('tags', []))}")
                            else:
                                st.info("No matching entries found.")

                    st.divider()

                    # Tool Rule Tester
                    st.markdown("### ⚙️ Tool Rule Tester")
                    st.caption("Test tool usage rules from policy blocks")

                    core_blocks = latest_snapshot.get('core_blocks', {})
                    tool_guidelines = core_blocks.get('Tool Guidelines', {})

                    if tool_guidelines:
                        st.markdown("**Current Tool Guidelines:**")
                        st.code(tool_guidelines.get('content', 'No guidelines available'), language='text')

                        st.markdown("**Test a Task:**")
                        test_task = st.text_area("Sample Task Description", placeholder="Enter a task description to test against tool rules...")

                        if st.button("🧪 Test Rule Enforcement", type="secondary"):
                            if not test_task:
                                st.warning("Please enter a task description.")
                            else:
                                # Simple rule matching (can be enhanced)
                                guidelines_text = tool_guidelines.get('content', '').lower()
                                task_lower = test_task.lower()

                                # Check for common rule keywords
                                triggered_rules = []

                                if 'verify' in guidelines_text and 'verify' not in task_lower:
                                    triggered_rules.append("⚠️ Guideline suggests verification, but task doesn't mention it")

                                if 'test' in guidelines_text and 'test' not in task_lower:
                                    triggered_rules.append("⚠️ Guideline suggests testing, but task doesn't mention it")

                                if 'cautious' in guidelines_text or 'careful' in guidelines_text:
                                    triggered_rules.append("ℹ️ Guidelines emphasize caution - ensure proper validation")

                                if triggered_rules:
                                    st.warning("**Rule Enforcement Suggestions:**")
                                    for rule in triggered_rules:
                                        st.text(f"  • {rule}")
                                else:
                                    st.success("✅ Task appears to align with tool guidelines")
                    else:
                        st.info("No Tool Guidelines available in this run.")

            # --- LIVE SMOKE TEST UTILITY (Always Available) ---
            st.divider()
            st.subheader("🔍 Live Smoke Test Utility")
            st.caption("Rapidly validate core planning, policy RAG, and execution across key scenarios.")

            smoke_scenario = st.selectbox(
                "Select Scenario for Live Smoke Test",
                options=list(SMOKE_TEST_SCENARIOS.keys()),
                index=0,
                key='smoke_test_select_final'
            )

            st.markdown(f"**Goal:** {SMOKE_TEST_SCENARIOS[smoke_scenario]['goal'][:150]}...")

            # Display memory policy if applicable
            policy = SMOKE_TEST_SCENARIOS[smoke_scenario]['policy']
            if policy:
                with st.expander(f"Policy Loaded (RAG Source): {policy.splitlines()[0]}", expanded=False):
                    st.code(policy, language='text')

            if st.button("▶️ Run Live Smoke Test (1-Turn Execution)", type="primary", use_container_width=True):
                if not GEMINI_API_KEY:
                    st.error("GEMINI_API_KEY is required to run the smoke test.")
                else:
                    # Run test synchronously and update dashboard selection
                    with st.spinner(f"Running **{smoke_scenario}**... (1 turn execution)"):
                        test_result = asyncio.run(run_live_smoke_test(smoke_scenario))

                    # Immediate Display and Logging
                    st.markdown("---")
                    if test_result['success']:
                        st.success("✅ Smoke Test PASSED: Core Orchestration Path Verified")
                        st.markdown("### Final Answer")
                        st.markdown(test_result['final_answer'])
                        st.markdown("### Specialized Output")
                        st.code(test_result['code_output'], language='python')

                        st.info(f"📊 Log available in **Event Log** and **Gantt Timeline** tabs. Select 'Smoke Test Run' from the test selector above.")
                    else:
                        st.error("❌ Smoke Test FAILED: Orchestrator Encountered an Error")
                        st.text(f"Error: {test_result['error']}")
                        st.warning("Check API Keys and ensure GEMINI_API_KEY is set correctly.")

                    # Clean up temporary policy setting
                    if 'demo_memory_policy' in st.session_state:
                        del st.session_state['demo_memory_policy']
                    if 'demo_scenario' in st.session_state:
                        del st.session_state['demo_scenario']

                    # Force dashboard selector to point to the smoke test to show log immediately
                    st.session_state['selected_test_in_dashboard'] = 'Smoke Test Run'
                    st.session_state['selected_test_in_dashboard_selector'] = 'smoke_test'
                    st.rerun()

        # Auto-refresh
        if auto_refresh:
            time.sleep(2)
            st.rerun()


# ---------- Footer: download + run cmd ----------
# Conditionally display the main DataFrame state and download button for Tests 1-3
if show_main_df_previews:
    st.divider()
    st.subheader("Current DataFrame State")
    # Truncate long rationale columns for display in footer preview
    display_df = st.session_state.df.copy()
    rationale_cols = [c for c in display_df.columns if c.endswith("_rationale") or c == "judge_rationale"]
    for col in rationale_cols:
        if col in display_df.columns:
            display_df[col] = display_df[col].astype(str).str.slice(0, 150) + "..."
    st.dataframe(display_df, use_container_width=True)

    col1, col2 = st.columns([1, 3]) # Give more space to the button
    with col1:
        csv_bytes = st.session_state.df.to_csv(index=False).encode("utf-8")
        st.download_button("⬇️ Download Results as CSV", data=csv_bytes, file_name="classification_results.csv", mime="text/csv", use_container_width=True)
    with col2:
        pass # Keep layout clean

st.divider()
st.code("streamlit run streamlit_app.py", language="bash")